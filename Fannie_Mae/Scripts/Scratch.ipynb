{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>Q2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  Year Quarter\n",
       "0      1  2008      Q2\n",
       "1      2  2008      Q2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core Packages\n",
    "import pandas as pd\n",
    "\n",
    "acq, per = pd.DataFrame({'Index': [1, 2]}), pd.DataFrame({'Index': [1, 2]})\n",
    "x = ['Acquisition_2007Q1', 'Acquisition_2008Q2']\n",
    "y = ['Perf_2007Q1', 'Perf_2008Q2']\n",
    "\n",
    "for i in range(len(x)):\n",
    "    acq['Year'], per['Year'] = x[i][12:16], x[i][12:16]\n",
    "    acq['Quarter'], per['Quarter'] = x[i][16:18], x[i][16:18]\n",
    "    \n",
    "acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Grabs the entire Federal Deposit Insurance Corporation (FDIC) Statistics on\n",
    "Depository Institutions (SDI) data set.\n",
    "\n",
    "Note that this is a large data set! There are roughly 85 zip files each of\n",
    "which is between 40 and 84 MB.\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "base_url = 'https://www7.fdic.gov/sdi/Resource/AllReps/All_Reports_'\n",
    "\n",
    "# use pandas to construct a list of quarterly dates\n",
    "present = '20071231'\n",
    "datetimes = pd.date_range('20070331', end=present, freq='Q')\n",
    "dates = datetimes.format(formatter=lambda t: t.strftime('%Y%m%d'))\n",
    "\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    # ...construct the url...\n",
    "    tmp_url = base_url + date + '.zip'\n",
    "\n",
    "    # ...make the connection and grab the zipped files...\n",
    "    tmp_buffer = requests.get(tmp_url)\n",
    "\n",
    "    # ...save them to disk...\n",
    "    with open('All_Reports_' + date + '.zip', 'wb') as tmp_zip_file:\n",
    "        tmp_zip_file.write(tmp_buffer.content)\n",
    "\n",
    "    print('Done with files for ' + date + '!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Grabs the entire Federal Deposit Insurance Corporation (FDIC) institutions data\n",
    "set which catalogues the history of mergers and acquisitions for all FDIC\n",
    "regulated instutitions and turns it into a Pandas DataFrame and picles the\n",
    "object for future use.\n",
    "\n",
    "\"\"\"\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# download the data\n",
    "#base_url = 'https://www7.fdic.gov/IDASP/'\n",
    "filename = 'institutions2/INSTITUTIONS2.CSV'\n",
    "#tmp_buffer = requests.get(base_url + filename)\n",
    "#\n",
    "#with open(filename, 'wb') as tmp_zip_file:\n",
    "#    tmp_zip_file.write(tmp_buffer.content)\n",
    "#\n",
    "## convert to pandas DataFrame\n",
    "#tmp_buffer = zipfile.ZipFile(filename)\n",
    "#tmp_file = tmp_buffer.namelist()[1]\n",
    "\n",
    "used_cols = ['CERT', 'CHANGEC1']\n",
    "dtypes = {}\n",
    "tmp_dataframe = pd.read_csv(filename,\n",
    "                            usecols=used_cols,\n",
    "                            )\n",
    "#tmp_dataframe = pd.read_csv(tmp_buffer.open(tmp_file),\n",
    "#                            usecols=used_cols,\n",
    "#                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script imports the subset of the FDIC SDI data used in the analysis,\n",
    "converts the data to a Pandas data frame and writes the object to disk.\n",
    "\n",
    "There are on the order of 50 corrupted observations in the various zip files.\n",
    "Not clear why there are 90 entries in those rows instead of 89\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# use pandas to construct a list of quarterly dates\n",
    "present = '20071231'\n",
    "datetimes = pd.date_range('20070331', end=present, freq='Q')\n",
    "\n",
    "# get a list of zip files over which to iterate\n",
    "zip_files = glob.glob('*.zip')\n",
    "\n",
    "# only want to return a subset of cols (save on memory usage!)\n",
    "used_columns = ['cert', 'repdte', 'asset', 'lnlsnet', 'liab', 'dep', 'eqtot',\n",
    "                'numemp',\n",
    "                ]\n",
    "used_dtypes = {'cert': int, 'repdte': datetime, 'asset': float,\n",
    "               'lnlsnet': float, 'liab': float, 'eqtot': float, 'dep': float,\n",
    "               'numemp': float}\n",
    "\n",
    "# create a container for the individual dataframes\n",
    "dataframes = []\n",
    "\n",
    "for zip_file in zip_files[0:4]:\n",
    "\n",
    "    tmp_buffer = zipfile.ZipFile(zip_file)\n",
    "    \n",
    "    # want to work with the assets and liabilities file\n",
    "    tmp_file = tmp_buffer.namelist()[5]\n",
    "    \n",
    "    tmp_dataframe = pd.read_csv(tmp_buffer.open(tmp_file),\n",
    "                                index_col=['cert', 'repdte'],\n",
    "                                error_bad_lines=False,  # skips the mangled obs\n",
    "                                usecols=used_columns,\n",
    "                                #dtype=used_dtypes,\n",
    "                                parse_dates=True,\n",
    "                                )\n",
    "    \n",
    "    dataframes.append(tmp_dataframe)\n",
    "\n",
    "    print('Done with ' + zip_file + '!')\n",
    "\n",
    "# concatenate the quarterly dataframes into a single data frame\n",
    "combined_dataframe = pd.concat(dataframes)\n",
    "\n",
    "# convert units from thousands to billions of USD\n",
    "combined_dataframe[['asset', 'lnlsnet', 'liab', 'dep', 'eqtot']] /= 1e6\n",
    "\n",
    "# convert units from nummber of people to thousands of people\n",
    "combined_dataframe['numemp'] /= 1e3\n",
    "\n",
    "# convert to panel (major_axis: cert, minor_axis: repdte)\n",
    "combined_panel = combined_dataframe.to_panel()\n",
    "\n",
    "# pickle the object for later use!\n",
    "combined_panel.to_pickle('FDIC_SDI_panel_nominal.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickled data\n",
    "FDIC_SDI_panel = combined_dataframe\n",
    "\n",
    "# compute the by quarter totals for each measure\n",
    "totals = FDIC_SDI_panel.sum()\n",
    "\n",
    "# compute the base quarter totals for each measure\n",
    "base_qtr='2007-03-31'\n",
    "totals_base_qtr = totals.copy()\n",
    "totals_base_qtr[:] = totals[base_qtr]\n",
    "totals_base_qtr.fillna(method='bfill', inplace=True)\n",
    "\n",
    "def janicki_prescott_norm(item):\n",
    "    \"\"\"\n",
    "    In order to make sure results are comparable across years, I follow \n",
    "    Janicki and Prescott (2006) and deflate and re-scale each measure of bank \n",
    "    size by dividing by banking sector totals relative to some base quarter. \n",
    "    Specifically, let :math:`S_{i,t}^{raw}` denote the raw size of bank :math:`i`\n",
    "    in year :math:`t` based on one of the six size measures detailed above. The \n",
    "    normalized size of bank :math:`i` relative to the base quarter is defined as\n",
    "    follows:\n",
    "             \n",
    "    .. math::\n",
    "    \n",
    "        S_{i,t}^{norm} = \\frac{S_{i,t}^{raw}}{\\sum_{j}S_{j,t}^{raw}}\\sum_{j}S_{i,base}^{raw}\n",
    "    \n",
    "    where :math:\\sum_{j}S_{j,t}^{raw}` is the banking sector total of some size \n",
    "    measure in year :math:`t` (i.e., total banking sector assets in year :math:`t`), \n",
    "    and :math:`\\sum_{j}S_{j,base}^{raw}` is the banking sector total of the same\n",
    "    size measure in the base quarter.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (FDIC_SDI_panel[item] / totals[item]) * totals_base_qtr[item]\n",
    "\n",
    "# apply the Janicki and Prescott (2006) normalized size measure \n",
    "for item in FDIC_SDI_panel.items:\n",
    "    FDIC_SDI_panel[item] = janicki_prescott_norm(item)\n",
    "    \n",
    "# pickle the object for later use!\n",
    "FDIC_SDI_panel.to_pickle('FDIC_SDI_normed_panel.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income Groups\n",
    "income_stats = df['Median Household Income'].describe()\n",
    "\n",
    "df['Income Category'] = np.nan\n",
    "df['Income Category'][  df['Median Household Income'] <  income_stats['25%']] = 'Lowest Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= income_stats['25%'] ) & \\\n",
    "                      ( df['Median Household Income'] <  income_stats['50%'] )] = 'Low-Middle Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= income_stats['50%'] ) & \\\n",
    "                      ( df['Median Household Income'] <  income_stats['75%'] )] = 'High-Middle Income'\n",
    "df['Income Category'][  df['Median Household Income'] >= income_stats['75%']] = 'Highest Income'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income Groups\n",
    "\n",
    "df['Income Category'] = np.nan\n",
    "df['Income Category'][  df['Median Household Income'] <  df['Median Household Income'].quantile(0.2) ] = 'Lowest Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= df['Median Household Income'].quantile(0.2) ) & \\\n",
    "                      ( df['Median Household Income'] <  df['Median Household Income'].quantile(0.4) )] = 'Low-Middle Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= df['Median Household Income'].quantile(0.4) ) & \\\n",
    "                      ( df['Median Household Income'] <  df['Median Household Income'].quantile(0.6) )] = 'Middle Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= df['Median Household Income'].quantile(0.6) ) & \\\n",
    "                      ( df['Median Household Income'] <  df['Median Household Income'].quantile(0.8) )] = 'High-Middle Income'\n",
    "df['Income Category'][  df['Median Household Income'] >= df['Median Household Income'].quantile(0.8) ] = 'Highest Income'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank Variables\n",
    "df2 = df\n",
    "\n",
    "NoBL = df.groupby(['Bank', 'Original Date', 'Zip Code']).size().reset_index(name='Number of Bank Loans')\n",
    "TBL = df.groupby(['Bank', 'Original Date']).size().reset_index(name='Total Bank Loans')\n",
    "PBL = pd.merge(NoBL, TBL, on=['Bank', 'Original Date'], how=\"left\")\n",
    "PBL['Proportion of Bank Loans'] = (PBL['Number of Bank Loans'] / PBL['Total Bank Loans']) * 100\n",
    "df2 = pd.merge(df2, PBL, on=['Bank', 'Original Date', 'Zip Code'], how=\"left\")\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of banks\n",
    "Banks = ['AMTRUST BANK', 'BANK OF AMERICA, N.A.', 'CITIMORTGAGE, INC.', \n",
    "         'FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB', \n",
    "         'FIRST TENNESSEE BANK NATIONAL ASSOCIATION', 'FLAGSTAR CAPITAL MARKETS CORPORATION', \n",
    "         'GMAC MORTGAGE, LLC', 'JPMORGAN CHASE BANK, NATIONAL ASSOCIATION', 'OTHER', \n",
    "         'PNC BANK, N.A.', 'SUNTRUST MORTGAGE INC.', 'CHASE HOME FINANCE', 'SMALL LOAN BANKS']\n",
    "\n",
    "# Function to subset banking datasets\n",
    "def Bank_Subsets(bank_strs, df_X = X_train, df_y = y_train):\n",
    "    # Initiate Bank dictionaries\n",
    "    X = {}\n",
    "    y = {}\n",
    "\n",
    "    # Bank Subset\n",
    "    for bank_str in bank_strs:\n",
    "        X[bank_str] = onehotencoding( df_X[df_X['Bank']==bank_str] \\\n",
    "            .drop(labels='Bank', axis=1) )\n",
    "        y[bank_str] = y_train[X_train['Bank']==bank_str]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Run Function\n",
    "Banks_X, Banks_y = Bank_Subsets(Banks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Banks = ['AMTRUST BANK', 'BANK OF AMERICA, N.A.', 'CITIMORTGAGE, INC.', \n",
    "         'FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB', \n",
    "         'FIRST TENNESSEE BANK NATIONAL ASSOCIATION', 'FLAGSTAR CAPITAL MARKETS CORPORATION', \n",
    "         'GMAC MORTGAGE, LLC', 'JPMORGAN CHASE BANK, NATIONAL ASSOCIATION', 'OTHER', \n",
    "         'PNC BANK, N.A.', 'SUNTRUST MORTGAGE INC.', 'CHASE HOME FINANCE', 'SMALL LOAN BANKS']\n",
    "\n",
    "# Variables to drop\n",
    "dropvars = ['Year', 'Quarter', 'Original Date', 'Credit Score', \n",
    "            'Zip Code', 'Mortgage Insurance Type']  # 'Property State', \n",
    "\n",
    "# All Data\n",
    "All_X = onehotencoding( df.drop(labels=dropvars, axis=1) )\n",
    "All_y = All_X['Foreclosed']\n",
    "All_X = All_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "dropvars.append('Bank')\n",
    "\n",
    "# Bank Datasets\n",
    "Amtrust_X = onehotencoding( df[df['Bank']=='AMTRUST BANK'] \\\n",
    "    .drop(labels='Bank', axis=1) )\n",
    "Amtrust_y = df[df['Bank']=='AMTRUST BANK']\n",
    "Amtrust_X = Amtrust_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "BoA_X = onehotencoding( df[df['Bank']=='BANK OF AMERICA, N.A.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "BoA_y = BoA_X['Foreclosed']\n",
    "BoA_X = BoA_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Citi_X = onehotencoding( df[df['Bank']=='CITIMORTGAGE, INC.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Citi_y = Citi_X['Foreclosed']\n",
    "Citi_X = Citi_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "IndyMac_X = onehotencoding( df[df['Bank']=='FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "IndyMac_y = IndyMac_X['Foreclosed']\n",
    "IndyMac_X = IndyMac_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Tenn_X = onehotencoding( df[df['Bank']=='FIRST TENNESSEE BANK NATIONAL ASSOCIATION'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Tenn_y = Tenn_X['Foreclosed']\n",
    "Tenn_X = Tenn_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "FlagStar_X = onehotencoding( df[df['Bank']=='FLAGSTAR CAPITAL MARKETS CORPORATION'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "FlagStar_y = FlagStar_X['Foreclosed']\n",
    "FlagStar_X = FlagStar_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "GMac_X = onehotencoding( df[df['Bank']=='GMAC MORTGAGE, LLC'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "GMac_y = GMac_X['Foreclosed']\n",
    "GMac_X = GMac_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "JPMorgan_X = onehotencoding( df[df['Bank']=='JPMORGAN CHASE BANK, NATIONAL ASSOCIATION'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "JPMorgan_y = JPMorgan_X['Foreclosed']\n",
    "JPMorgan_X = JPMorgan_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Misc_X = onehotencoding( df[df['Bank']=='OTHER'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Misc_y = Misc_X['Foreclosed']\n",
    "Misc_X = Misc_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "PNC_X = onehotencoding( df[df['Bank']=='PNC BANK, N.A.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "PNC_y = PNC_X['Foreclosed']\n",
    "PNC_X = PNC_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "SunTrust_X = onehotencoding( df[df['Bank']=='SUNTRUST MORTGAGE INC.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "SunTrust_y = SunTrust_X['Foreclosed']\n",
    "SunTrust_X = SunTrust_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Chase_X = onehotencoding( df[df['Bank']=='CHASE HOME FINANCE'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Chase_y = Chase_X['Foreclosed']\n",
    "Chase_X = Chase_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Small_X = onehotencoding( df[df['Bank']=='SMALL LOAN BANKS'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Small_y = Small_X['Foreclosed']\n",
    "Small_X = Small_X.drop(labels='Foreclosed', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(All_X, All_y, test_size = 0.3, \n",
    "                                                    stratify = All_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(target_prob):\n",
    "    # Determine threshold\n",
    "    threshold = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "    \n",
    "    acc = []\n",
    "    prec = []\n",
    "    f1 = []\n",
    "    best_avg = {'Iteration': -1, 'Threshold': 0.5, 'Best Avg Score': 0.0}\n",
    "    best_acc = {'Iteration': -1, 'Threshold': 0.5, 'Best Accuracy Score': 0.0}\n",
    "    best_prec = {'Iteration': -1, 'Threshold': 0.5, 'Best Precision Score': 0.0}\n",
    "    best_f1 = {'Iteration': -1, 'Threshold': 0.5, 'Best F1 Score': 0.0}\n",
    "    for i in range(len(threshold)):\n",
    "        y_pred = target_prob.map(lambda x: 1 if x >= threshold[i] else 0)\n",
    "        \n",
    "        # Accuracy\n",
    "        acc.append(accuracy_score(y_test, y_pred).round(2))\n",
    "        # Precision\n",
    "        prec.append(precision_score(y_test, y_pred).round(2))\n",
    "        # F1 \n",
    "        f1.append(f1_score(y_test, y_pred).round(2))\n",
    "        # Avg\n",
    "        avg.append( 3 * ( (acc[i]*prec[i]*f1[i]) / (acc[i]+prec[i]+f1[i]) ) )\n",
    "        \n",
    "        # Save best accuracy\n",
    "        if (best_acc['Best Accuracy Score'] < acc[i]):\n",
    "            best_acc = {'Iteration': i, 'Threshold': threshold[i], 'Best Accuracy Score': acc[i]}\n",
    "        # Save best precision\n",
    "        if (best_prec['Best Precision Score'] < prec[i]):\n",
    "            best_prec = {'Iteration': i, 'Threshold': threshold[i], 'Best Precision Score': prec[i]}      \n",
    "        # Save best f1\n",
    "        if (best_f1['Best F1 Score'] < f1[i]):\n",
    "            best_f1 = {'Iteration': i, 'Threshold': threshold[i], 'Best F1 Score': f1[i]}       \n",
    "        # Save best avg\n",
    "        if (best_avg['Best Avg Score'] < avg[i]):\n",
    "            best_avg = {'Iteration': i, 'Threshold': threshold[i], 'Best Avg Score': avg[i]}   \n",
    "        \n",
    "    print(best_acc)\n",
    "    print(best_prec)\n",
    "    print(best_f1)\n",
    "    print(best_avg)\n",
    "    df_plot = pd.DataFrame({'Threshold': threshold, 'Accuracy': acc, 'Precision': prec, \n",
    "                            'F1 Score': f1, 'Average Score', avg})\n",
    "    \n",
    "    # Plot\n",
    "    return( plt.plot(df_plot['Threshold'], df_plot.iloc[:,1:5]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bal_bagging(X y, smote=False, random_state=2020, n_estimators=500, max_features=0.75,\n",
    "                replacement=False, sampling_strategy='auto', pca=False, n_components=25):\n",
    "\n",
    "    # SMOTE\n",
    "    if smote:\n",
    "        sm = SVMSMOTE(random_state=2020, out_step=0.75, n_jobs=-1)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # define models\n",
    "    model_bal = BalancedRandomForestClassifier(n_estimators=n_estimators, random_state=random_state, \n",
    "                                               max_features=max_features, replacement=replacement, \n",
    "                                               sampling_strategy=sampling_strategy, n_jobs=-1)\n",
    "    model_wgt = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state, \n",
    "                                       max_features=max_features, \n",
    "                                       class_weight={0:0.1, 1:0.9}, n_jobs=-1)\n",
    "    model_gbm = GradientBoostingClassifier(n_estimators=n_estimators, random_state=random_state, n_jobs=-1)\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\n",
    "    \n",
    "    if pca:\n",
    "        dimredu = pca(n_components=n_components, random_state=random_state).fit(X_train)\n",
    "        X_train = dimredu.transform(X_train)\n",
    "        X_test = dimredu.transform(X_test)\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Model fit\n",
    "    model_bal.fit(X_train, y_train)\n",
    "    y_proba_bal = pd.DataFrame(model_bal.predict_proba(X_test), columns=[\"Did not Foreclose\", \"Foreclosed\"])\n",
    "    y_proba_wgt = pd.DataFrame(model_wgt.predict_proba(X_test), columns=[\"Did not Foreclose\", \"Foreclosed\"])\n",
    "    y_proba_gbm = pd.DataFrame(model_gbm.predict_proba(X_test), columns=[\"Did not Foreclose\", \"Foreclosed\"])\n",
    "    y_proba_knn = pd.DataFrame(model_knn.predict_proba(X_test), columns=[\"Did not Foreclose\", \"Foreclosed\"])\n",
    "\n",
    "    # Compare results via thresholds\n",
    "    threshold_bal = threshold(y_proba_bal['Foreclosed'])\n",
    "    threshold_reg = threshold(y_proba_reg['Foreclosed'])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 990, 0: 10})\n",
      "[[  2   0]\n",
      " [  5 243]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.ensemble import EasyEnsembleClassifier # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    weights=[0.01, 0.99], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=5, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=0)\n",
    "model = EasyEnsembleClassifier(n_estimators=10, random_state=42, replacement=False)\n",
    "model.fit(X_train, y_train) # doctest: +ELLIPSIS\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Accuracy: 0.98\n",
      "Precision: 1.0\n",
      "F1: 0.99\n",
      "\n",
      "iteration 1\n",
      "Accuracy: 0.98\n",
      "Precision: 1.0\n",
      "F1: 0.99\n",
      "\n",
      "iteration 2\n",
      "Accuracy: 0.97\n",
      "Precision: 1.0\n",
      "F1: 0.98\n",
      "\n",
      "iteration 3\n",
      "Accuracy: 0.91\n",
      "Precision: 1.0\n",
      "F1: 0.95\n",
      "\n",
      "iteration 4\n",
      "Accuracy: 0.89\n",
      "Precision: 1.0\n",
      "F1: 0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output Statistics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Table of predictions versus actuals\n",
    "y_proba = pd.DataFrame(model.predict_proba(X_test), columns=[\"Did not Foreclose\", \"Foreclosed\"])\n",
    "threshold = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "\n",
    "acc = []\n",
    "prec = []\n",
    "f1 = []\n",
    "best_f1 = {'Iteration': -1, 'Threshold': 0.5, 'Best F1 Score': 0.0}\n",
    "best_prec = {'Iteration': -1, 'Threshold': 0.5, 'Best Precision Score': 0.0}\n",
    "for i in range(len(threshold)):\n",
    "    y_pred = y_proba[\"Foreclosed\"].map(lambda x: 1 if x >= threshold[i] else 0)\n",
    "    \n",
    "    # Overall Scores\n",
    "    print('iteration', i, 'threshold', threshold[i])\n",
    "    print('Accuracy:', end=' ')\n",
    "    acc.append(accuracy_score(y_test, y_pred).round(2))\n",
    "    print(acc[i])\n",
    "    print('Precision:', end=' ')\n",
    "    prec.append(precision_score(y_test, y_pred).round(2))\n",
    "    print(prec[i])\n",
    "    print('F1:', end=' ')\n",
    "    f1.append(f1_score(y_test, y_pred).round(2))\n",
    "    print(f1[i])\n",
    "    print('')\n",
    "    \n",
    "    # Save best f1\n",
    "    if (best_f1['Best F1 Score'] < f1[i]):\n",
    "        best_f1 = {'Iteration': i, 'Threshold': threshold[i], 'Best F1 Score': f1[i]}\n",
    "\n",
    "    # Save best precision\n",
    "    if (best_prec['Best Precision Score'] < prec[i]):\n",
    "        best_prec = {'Iteration': i, 'Threshold': threshold[i], 'Best Precision Score': prec[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Iteration': 4, 'Best F1 Score': 0.94}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use best threshold\n",
    "y_pred = y_proba[\"Foreclosed\"].map(lambda x: 1 if x >= best_f1['Threshold'] else 0)\n",
    "\n",
    "# Table of predictions versus actuals\n",
    "target_values(y_pred, prediction=True)\n",
    "    \n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion matrix:\")\n",
    "PredictTable = pd.crosstab(y_test, np.array(y_pred))\n",
    "PredictTable.columns = ['Predicted False', 'Predicted True']\n",
    "PredictTable.index = ['Actual False', 'Actual True']\n",
    "print(PredictTable)\n",
    "# F1 Score\n",
    "print(\"\\nFinal F1 Score:\")\n",
    "print(best_f1['Best F1 Score'])\n",
    "# Precision Table\n",
    "print('\\nFinal Precision Percentages:')\n",
    "PrecisionTable = ( (PredictTable/(PredictTable.sum(0)))*100 ).round(1)\n",
    "print(PrecisionTable.iloc[:,1])\n",
    "# Recall Table\n",
    "print('\\nFinal Recall Percentages:')\n",
    "RecallTable = ( (PredictTable.div(PredictTable.sum(axis=1), axis=0))*100 ).round(1)\n",
    "print(RecallTable.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "X, y = load_iris(return_X_y=True)\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                          LinearSVC(random_state=42)))\n",
    "]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression()\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42\n",
    ")\n",
    "t = clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
