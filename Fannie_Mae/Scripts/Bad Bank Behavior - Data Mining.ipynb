{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Bad Bank Behavior<br>Analyzing Bank Mortgage during the 2007 Housing Bubble</center>  \n",
    "\n",
    "<center>Michael Siebel</center>\n",
    "<center>August 2020</center>\n",
    "\n",
    "<br>\n",
    "    \n",
    "## Table of Contents\n",
    "- [Goals](#Goals)<br>\n",
    "- [Load Packages](#Load-Packages)<br>\n",
    "- [Set Up Functions](#Set-Up-Functions)<br>\n",
    "- [Implement Data Cleanings](#Implement-Data-Cleanings)<br>\n",
    "- [Analysis Functions](#Analysis-Functions)<br>\n",
    "- [Imbalanced Prediction](#Imbalanced-Prediction)\n",
    "- [Downsampling Prediction](#Downsampling-Prediction)<br>\n",
    "- [Upsampling Prediction](#Upsampling-Prediction)<br>\n",
    "- [Conclusion](#Conclusion)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals  \n",
    "<br>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# Convert Time Features\n",
    "from datetime import datetime as dt\n",
    "# Data Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='chesterish', grid=False)\n",
    "# Splitting Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Machine Learning Packages\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Save Runtime\n",
    "import time\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Output Statistics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Functions  \n",
    "<br>\n",
    "\n",
    "In this post, I set up all data wrangling and data analysis as a series of functions, which will enable me to reuse on data from subsequent years (future projects) and various analysis techniques (this project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fannie Mae data comes in two forms:  \n",
    "### 1) acquistion data and 2) performance data  \n",
    "<br>\n",
    "1) The acquisition data includes one observation for each loan with each feature representing knowledge Fannie Mae has when acquiring the loan (e.g., balance, primary lender, credit score, etc.).  \n",
    "\n",
    "2) The performance data includes observations for each month each loan is held and information on the payment of the loan.  \n",
    "<br>\n",
    "I use the acquisition data as predictors for a dichotomous categorization of whether the homeowner defaulted on their loan, a target variable I create using the performance data and merging onto the acquisition data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(acq, per):\n",
    "    df_acq = pd.read_csv(acq, sep='|', header=None)\n",
    "    df_per = pd.read_csv(per, sep='|', header=None)\n",
    "    \n",
    "    # Specify the name of the columns\n",
    "    df_acq.columns = ['Loan ID','Origination Channel','Bank','Original Interest Rate',\n",
    "                      'Original Mortgage Amount','Original Loan Term','Original Date','First Payment',\n",
    "                      'Original Loan-to-Value (LTV)','Original Combined Loan-to-Value (CLTV)',\n",
    "                      'Number of Borrowers','Original Debt to Income Ratio','Credit Score',\n",
    "                      'First Time Home Buyer','Loan Purpose','Property Type','Number of Units',\n",
    "                      'Occupancy Type','Property State','Zip Code','Mortgage Insurance %',\n",
    "                      'Product Type','Co-Borrower Credit Score','Mortgage Insurance Type',\n",
    "                      'Relocation Mortgage Indicator']\n",
    "    \n",
    "    df_per.columns = ['Loan ID','MonthRep','Servicer','CurrInterestRate','CAUPB','LoanAge','MonthsToMaturity',\n",
    "              'AdMonthsToMaturity','MaturityDate','MSA','CLDS','ModFlag','ZeroBalCode','ZeroBalDate',\n",
    "              'LastInstallDate','Foreclosure Date','DispositionDate','FCCCost','PPRC','AssetRecCost','MHRC',\n",
    "              'ATFHP','NetSaleProceeds','CreditEnhProceeds','RPMWP','OFP','NIBUPB','PFUPB','RMWPF',\n",
    "              'FPWA','ServicingIndicator']\n",
    "    \n",
    "    return df_acq, df_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance data is much larger as it is transaction based, while the acquistion data has the loan owner as its unit of analysis.  \n",
    "<br>\n",
    "I retain only the most recent performance transaction relating to foreclosure, then drop all other variables except  Loan ID (the primary key) and merge performance data onto acquisition data.  \n",
    "<br>\n",
    "I recode performance data into dichotomous categorization of whether loan was foreclosed upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge foreclosures from performance data to acquistion data\n",
    "def merge_df(df_acq, df_per):\n",
    "    \n",
    "    # Columns to maintain from Performance Data\n",
    "    per_ColKeep = ['Loan ID','Foreclosure Date']\n",
    "    df_per = df_per[per_ColKeep]\n",
    "    df_per = df_per.drop_duplicates(subset='Loan ID', keep='last')\n",
    "    df = pd.merge(df_acq, df_per, on='Loan ID', how='inner')\n",
    "    \n",
    "    # Set Foreclosed to binary\n",
    "    df.loc[df['Foreclosure Date'].isnull(), 'Foreclosed'] = 0\n",
    "    df.loc[df['Foreclosure Date'].notnull(),'Foreclosed'] = 1\n",
    "    df = df.drop(columns='Foreclosure Date')\n",
    "    df['Foreclosed'] = df['Foreclosed'].astype(int)\n",
    "    \n",
    "    # Drop mergeID column\n",
    "    df = df.drop(df.columns[[0]], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remove features with really high missingness or no data variation, and then impute on features with low missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_treat(df):\n",
    "    # Find features with 10% missing or more\n",
    "    # condition = ( df.isnull().sum(axis=0)/df.shape[0]*100 ) \n",
    "    # df_HighMissing = condition > 10 \n",
    "    \n",
    "    # Save features that contain missing data\n",
    "    # df_HighMissing = df_HighMissing.index[df_HighMissing.values == True]\n",
    "    \n",
    "    # remove high missing features\n",
    "    # df = df.drop(labels=df_HighMissing, axis=1)\n",
    "    \n",
    "    # remove Product Type, it only had one value    \n",
    "    # df = df.drop(labels=['Product Type'], axis=1)\n",
    "    \n",
    "    # remove Original Loan-to-Value (LTV) and use Original Combine Loan-to-Value (CLTV)\n",
    "    df = df.drop(labels=['Original Loan-to-Value (LTV)'], axis=1)\n",
    "        \n",
    "    # remove First Payment as this is of no value\n",
    "    df = df.drop(labels=['First Payment'], axis=1)\n",
    "        \n",
    "    # impute on the mean for low missing features that are continuous   \n",
    "    # df_cont = df.select_dtypes(include=['float64', 'int64'])\n",
    "    # df[df_cont.columns] = df_cont.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    \n",
    "    # impute on the mode for low missing features that are categorical   \n",
    "    # df_cat = df.select_dtypes(include=['object'])\n",
    "    # df[df_cat.columns] = df_cat.apply(lambda x: x.fillna(x.mode()),axis=0)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing date features to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing date features to ordinal variables using the function toordinal()\n",
    "def change_date(df):\n",
    "    \n",
    "    # Origination Date\n",
    "    df['Original Date'] = df['Original Date'].apply(lambda x: dt.strptime(x, '%m/%Y').toordinal())\n",
    "    \n",
    "    # Date of First Payment\n",
    "    # df['FirstPayment'] = df['FirstPayment'].apply(lambda x: dt.strptime(x, '%m/%Y').toordinal())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical variables to dummy variables\n",
    "def onehotencoding(df):\n",
    "    columns = df.columns[df.isnull().any()]\n",
    "    nan_cols = df[columns]\n",
    "\n",
    "    df = df.drop(nan_cols.columns, axis=1)\n",
    "\n",
    "    df_cat = df.select_dtypes(include=['object'])\n",
    "    onehot = pd.get_dummies(df_cat)\n",
    "    \n",
    "    df_cont = df.drop(df_cat.columns, axis=1)\n",
    "\n",
    "    df = pd.concat([df_cont,onehot,nan_cols], axis=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentage and frequency of target variable\n",
    "def target_values(df_depvar, data=False, prediction=False):\n",
    "        \n",
    "    # save target frequencies\n",
    "    target_frequency = df_depvar.value_counts()\n",
    "        \n",
    "    # save target percentage\n",
    "    target_percentage = round((df_depvar.value_counts()/df_depvar.count())*100).astype(int)\n",
    "        \n",
    "    # graphing target variable\n",
    "    jtplot.style(ticks=True, grid=False)\n",
    "    plt.figure(figsize=(14,4))\n",
    "    target_percentage.plot.barh(stacked=True, color='#ca2c92').invert_yaxis()\n",
    "    if data:\n",
    "        plt.suptitle('Bar Chart of Target Variable', fontsize=18)\n",
    "    elif prediction:\n",
    "        plt.suptitle('Bar Chart of Predictions', fontsize=18)\n",
    "    else:\n",
    "        plt.suptitle('Percent of Mortage Defaults', fontsize=18)\n",
    "    plt.ylabel('Foreclosed')\n",
    "    plt.xlabel('Percentage')\n",
    "    plt.xlim([0,100])\n",
    "    # plt.yticks([0, 1], ['Did not Foreclose', 'Foreclosed'])\n",
    "    plt.show()\n",
    "    \n",
    "    # display frequency of foreclosures\n",
    "    print('Frequency of Foreclosures\\n', target_frequency, '\\n', sep='')\n",
    "    \n",
    "    # display percentage of foreclosures\n",
    "    print('Percentage of Foreclosures\\n', target_percentage, '\\n', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Data Cleanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Collect file names\n",
    "fld = 'C:/Users/siebe/Documents/03_GWU/10 Capstone/Data/'\n",
    "x = []\n",
    "for file in os.listdir(fld):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.startswith(\"Acquisition_2007Q\"): \n",
    "        x.append(fld + filename)\n",
    "        \n",
    "y = []\n",
    "for file in os.listdir(fld):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.startswith(\"Performance_2007Q\"): \n",
    "        y.append(fld + filename)\n",
    "\n",
    "# Load data\n",
    "df_acq = pd.DataFrame()\n",
    "df_per = pd.DataFrame()\n",
    "for i in range(len(x)):\n",
    "    acq, per = load_data(x[i], y[i])\n",
    "    df_acq = pd.concat([df_acq, acq],ignore_index=True)\n",
    "    df_per = pd.concat([df_per, per],ignore_index=True)\n",
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "df_acq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates\n",
    "df_acq['Original Date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Borrower Credit Score\n",
    "df_acq['Harmonized Credit Score'] = ( df_acq['Co-Borrower Credit Score'][df_acq['Co-Borrower Credit Score'].notnull()] * 0.25 ) \\\n",
    "                                  + ( df_acq['Credit Score'][df_acq['Co-Borrower Credit Score'].notnull()] * 0.75 ) \n",
    "df_acq['Harmonized Credit Score'][df_acq['Co-Borrower Credit Score'].isnull()] = df_acq['Credit Score'][df_acq['Co-Borrower Credit Score'].isnull()]    \n",
    "\n",
    "print(df_acq[['Harmonized Credit Score', 'Credit Score', 'Co-Borrower Credit Score']].head(10))\n",
    "df_acq = df_acq.drop(labels=['Co-Borrower Credit Score'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortgage Insurance %\n",
    "df_acq['Mortgage Insurance %'] = np.where(df_acq['Mortgage Insurance %'].isnull(), \\\n",
    "                                             0, df_acq['Mortgage Insurance %'])\n",
    "df_acq['Mortgage Insurance Type'][df_acq['Mortgage Insurance %']==0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortgage Insurance Type\n",
    "df_acq['Mortgage Insurance Type'] = np.where(df_acq['Mortgage Insurance Type'].isnull(), \\\n",
    "                                             0, 1)\n",
    "df_acq['Mortgage Insurance Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Number of Borrowers\n",
    "# Single Borrower binary\n",
    "# More than one borrower is 0\n",
    "df_acq['Number of Borrowers'] = df_acq['Number of Borrowers'].where(df_acq['Number of Borrowers'] == 1, 0)\n",
    "df_acq = df_acq.rename(columns={'Number of Borrowers': 'Single Borrower'})\n",
    "df_acq['Single Borrower'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import zipcode median household income\n",
    "income = pd.read_excel(\"C:/Users/siebe/Documents/03_GWU/10 Capstone/Data/est07all.xls\",\n",
    "                       sheet_name = 'est07ALL', header = 2)\n",
    "income = income[['Name', 'Median Household Income']]\n",
    "income = income.rename(columns={'Name': 'County'})\n",
    "\n",
    "# Import county zipcode crosswalk\n",
    "crosswalk = pd.read_csv(\"C:/Users/siebe/Documents/03_GWU/10 Capstone/Data/ZIP-COUNTY-FIPS_2017-06.csv\",\n",
    "                        header = 0)\n",
    "crosswalk = crosswalk[['ZIP', 'COUNTYNAME']]\n",
    "crosswalk = crosswalk.rename(columns={'ZIP': 'Zip Code', 'COUNTYNAME': 'County'})\n",
    "crosswalk['Zip Code'] = crosswalk['Zip Code'].astype(str)\n",
    "crosswalk['Zip Code'] = crosswalk['Zip Code'].str.slice(start=0, stop=-2)\n",
    "crosswalk['Zip Code'] = crosswalk['Zip Code'].astype(int)\n",
    "\n",
    "# Merge\n",
    "income_zipcode = pd.merge(income, crosswalk, on='County', how='outer')\n",
    "income_zipcode.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge zipcode crosswalk with \n",
    "income_zipcode = income_zipcode[['Median Household Income', 'Zip Code']]\n",
    "income_zipcode = income_zipcode.groupby('Zip Code').agg({'Median Household Income': 'mean'})\n",
    "df_acq = pd.merge(df_acq, income_zipcode, on='Zip Code', how='left')\n",
    "\n",
    "df_acq['Median Household Income'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target Variable with Merge\n",
    "df = merge_df(df_acq, df_per)\n",
    "\n",
    "print('\\nThe number of features is:\\n', df.shape[1], sep='')\n",
    "print('\\nThe number of observations is:\\n', df.shape[0], sep='')\n",
    "target_values(df['Foreclosed'], data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "## Drop U.S. Terroritories due to missing data\n",
    "df = df[df['Property State'] != 'PR']\n",
    "df = df[df['Property State'] != 'GU']\n",
    "df = df[df['Property State'] != 'VI']\n",
    "    \n",
    "## Drop high missing and impute on others\n",
    "df = missing_treat(df)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing\n",
    "(df.isna().sum() / df.shape[0] * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreclosure Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Foreclosure_Data(date1 = \"\", date2 = \"\", subset = \"\", df = df):\n",
    "    # Subset by Date\n",
    "    if (date1 == \"\" and date2 == \"\"):\n",
    "        df_sub = df\n",
    "    elif (date1 != \"\" and date2 != \"\"):\n",
    "        date = pd.date_range(date1, date2)\n",
    "        month_yr = np.array([])\n",
    "        for i in range(len(date)): \n",
    "            month_yr = np.append( month_yr, str( str(date.month[i]) + '/' + str(date.year[i]) ) )\n",
    "        month_yr = np.unique(month_yr)\n",
    "        month_yr = np.char.zfill(month_yr, 7)\n",
    "        df_sub = df[df['Original Date'].isin(month_yr)]\n",
    "    elif (date2 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date1]\n",
    "    elif (date1 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date2]\n",
    "    \n",
    "    # Subset by other variable\n",
    "    if (date1 == \"\" and date2 == \"\" and subset != \"\"):\n",
    "        df_sub = df[eval(subset)]\n",
    "        \n",
    "    # Foreclosures represented\n",
    "    Foreclosed = ['Not Forclosed', 'Forclosed']\n",
    "    Target = df_sub.groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "    # Original Mortgage Amount\n",
    "    ORM = df_sub.groupby(['Foreclosed']).agg({'Original Mortgage Amount': 'mean'}).round(2)\n",
    "    # Credit Score\n",
    "    CS = df_sub.groupby(['Foreclosed']).agg({'Harmonized Credit Score': 'mean'}).astype(int)\n",
    "    # Original Debt to Income Ratio\n",
    "    # DTI = df_sub.groupby(['Foreclosed']).agg({'Original Debt to Income Ratio': 'mean'}).round(1)\n",
    "    # First Time Home Buyer\n",
    "    FT = df_sub[df_sub['First Time Home Buyer']=='Y'].groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "    # Original Interest Rate\n",
    "    IR = df_sub.groupby(['Foreclosed']).agg({'Original Interest Rate': 'mean'}).round(2)\n",
    "    # Original Loan Term\n",
    "    LT = df_sub.groupby(['Foreclosed']).agg({'Original Loan Term': 'mean'}).astype(int)\n",
    "    # Original Combined Loan-to-Value (CLTV)\n",
    "    CLTV = df_sub.groupby(['Foreclosed']).agg({'Original Combined Loan-to-Value (CLTV)': 'mean'}).round(1)\n",
    "    # Single Borrower Ratio\n",
    "    SBR = df_sub.groupby(['Foreclosed']).agg({'Single Borrower': 'mean'}).round(2)\n",
    "    # Mortgage Insurance Type\n",
    "    MIT = df_sub.groupby(['Foreclosed']).agg({'Mortgage Insurance Type': 'mean'}).round(2)\n",
    "    # Mortgage Insurance %\n",
    "    MIP = df_sub.groupby(['Foreclosed']).agg({'Mortgage Insurance %': 'mean'}).round(2)    \n",
    "    # Median Household Income\n",
    "    MHI = df_sub.groupby(['Foreclosed']).agg({'Median Household Income': 'mean'}).round(2)\n",
    "    \n",
    "    # Create Dataset\n",
    "    df_new = pd.DataFrame({ 'Foreclosed': Foreclosed, \n",
    "                            'Foreclosed (%)': ((Target['Total'] / df_sub.shape[0]) * 100).round(1),\n",
    "                            'Foreclosed (N)': df_sub.groupby(['Foreclosed']).size(),\n",
    "                            'Mortgage Amount ($)': ORM['Original Mortgage Amount'].tolist(),\n",
    "                            'Harmonized Credit Score': CS['Harmonized Credit Score'].tolist(),\n",
    "                            # 'Debt to Income Ratio': DTI['Original Debt to Income Ratio'].tolist(),\n",
    "                            'First Time Home Buyer (%)': ((FT['Total'] / Target['Total']) * 100).round(1).tolist(),\n",
    "                            'Interest Rate': IR['Original Interest Rate'].tolist(),\n",
    "                            'Loan Term': LT['Original Loan Term'].tolist(),\n",
    "                            'Combined Loan-to-Value (CLTV)': CLTV['Original Combined Loan-to-Value (CLTV)'].tolist(),\n",
    "                            'Single Borrower Ratio': SBR['Single Borrower'].tolist(),\n",
    "                            'Mortgage Insurance Ratio': MIT['Mortgage Insurance Type'].tolist(),\n",
    "                            'Mortgage Insurance %': MIP['Mortgage Insurance %'].tolist(),\n",
    "                            'Estimated Household Income ($)': MHI['Median Household Income'].tolist()\n",
    "                         })\n",
    "    \n",
    "    df_new = df_new.set_index('Foreclosed')\n",
    "    return df_new\n",
    "\n",
    "Foreclosed = Foreclosure_Data(subset = \"df['Property State']=='FL'\")\n",
    "Foreclosed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bank_Data(date1 = \"\", date2 = \"\", subset =\"\", df = df):\n",
    "    # Subset by Date\n",
    "    if (date1 == \"\" and date2 == \"\"):\n",
    "        df_sub = df\n",
    "    elif (date1 != \"\" and date2 != \"\"):\n",
    "        date = pd.date_range(date1, date2)\n",
    "        month_yr = np.array([])\n",
    "        for i in range(len(date)): \n",
    "            month_yr = np.append( month_yr, str( str(date.month[i]) + '/' + str(date.year[i]) ) )\n",
    "        month_yr = np.unique(month_yr)\n",
    "        month_yr = np.char.zfill(month_yr, 7)\n",
    "        df_sub = df[df['Original Date'].isin(month_yr)]\n",
    "    elif (date2 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date1]\n",
    "    elif (date1 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date2]\n",
    "    \n",
    "    # Subset by other variable\n",
    "    if (date1 == \"\" and date2 == \"\" and subset != \"\"):\n",
    "        df_sub = df[eval(subset)]\n",
    "        \n",
    "    # Banks represented\n",
    "    Banks = df_sub.groupby(['Bank']).size().reset_index(name='Total')\n",
    "    # Foreclosures\n",
    "    Target = df_sub.groupby(['Bank']).agg({'Foreclosed': 'mean'})\n",
    "    # Original Mortgage Amount\n",
    "    ORM = df_sub.groupby(['Bank']).agg({'Original Mortgage Amount': 'mean'}).round(2)\n",
    "    # Credit Score\n",
    "    CS = df_sub.groupby(['Bank']).agg({'Credit Score': 'mean'}).astype(int)\n",
    "    # Original Debt to Income Ratio\n",
    "    DTI = df_sub.groupby(['Bank']).agg({'Original Debt to Income Ratio': 'mean'}).round(1)\n",
    "    # First Time Home Buyer\n",
    "    FT = df_sub[df_sub['First Time Home Buyer']=='Y'].groupby(['Bank']).size().reset_index(name='Total')\n",
    "    # Original Interest Rate\n",
    "    IR = df_sub.groupby(['Bank']).agg({'Original Interest Rate': 'mean'}).round(2)\n",
    "    # Original Loan Term\n",
    "    LT = df_sub.groupby(['Bank']).agg({'Original Loan Term': 'mean'}).astype(int)\n",
    "    # Original Combined Loan-to-Value (CLTV)\n",
    "    CLTV = df_sub.groupby(['Bank']).agg({'Original Combined Loan-to-Value (CLTV)': 'mean'}).round(1)\n",
    "    # Single Borrower Ratio\n",
    "    SBR = df_sub.groupby(['Bank']).agg({'Single Borrower': 'mean'}).round(2)\n",
    "    # Mortgage Insurance Type\n",
    "    MIT = df_sub.groupby(['Bank']).agg({'Mortgage Insurance Type': 'mean'}).round(2)\n",
    "    # Mortgage Insurance %\n",
    "    MIP = df_sub.groupby(['Bank']).agg({'Mortgage Insurance %': 'mean'}).round(2)     \n",
    "    # Median Household Income\n",
    "    MHI = df_sub.groupby(['Bank']).agg({'Median Household Income': 'mean'}).round(2)\n",
    "    \n",
    "    # Create Dataset\n",
    "    df_new = pd.DataFrame({ 'Bank': Banks['Bank'], \n",
    "                            'Bank (%)': ((Banks['Total'] / df_sub.shape[0]) * 100).round(1),\n",
    "                            'Bank (N)': Banks['Total'],\n",
    "                            'Foreclosed (%)': ((Target['Foreclosed'] * 100).round(1)).tolist(), \n",
    "                            'Mortgage Amount ($)': ORM['Original Mortgage Amount'].tolist(),\n",
    "                            'Credit Score': CS['Credit Score'].tolist(),\n",
    "                            'Debt to Income Ratio': DTI['Original Debt to Income Ratio'].tolist(),\n",
    "                            'First Time Home Buyer (%)': ((FT['Total'] / Banks['Total']) * 100).round(1).tolist(),\n",
    "                            'Interest Rate': IR['Original Interest Rate'].tolist(),\n",
    "                            'Loan Term': LT['Original Loan Term'].tolist(),\n",
    "                            'Combined Loan-to-Value (CLTV)': CLTV['Original Combined Loan-to-Value (CLTV)'].tolist(),\n",
    "                            'Single Borrower Ratio': SBR['Single Borrower'].tolist(),\n",
    "                            'Mortgage Insurance Ratio': MIT['Mortgage Insurance Type'].tolist(),\n",
    "                            'Mortgage Insurance %': MIP['Mortgage Insurance %'].tolist(),                           \n",
    "                            'Median Household Income ($)': MHI['Median Household Income'].tolist()\n",
    "                        })\n",
    "    \n",
    "    df_new = df_new.set_index(\"Bank\")\n",
    "    return df_new\n",
    "\n",
    "Banks = Bank_Data()\n",
    "Banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banks represented\n",
    "Banks[['Bank (%)', 'Bank (N)', 'Foreclosed (%)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_Banks(col, df = Banks, func = max, subset = True):\n",
    "    print(col, func.__name__, \"value\")\n",
    "    if (subset): cols = col\n",
    "    else: cols = df.columns\n",
    "    values = pd.DataFrame(df[cols][df[col] == func(df[col])])\n",
    "    return values\n",
    "\n",
    "search_Banks('Foreclosed (%)', func = max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [1,2,3]\n",
    "for mnth in range(1, 13):\n",
    "\n",
    "    yr = '2007'\n",
    "    mnth = np.char.zfill(str(mnth), 2)\n",
    "    print(str(mnth) + '/' + str(yr))\n",
    "    Banks_mnth = Bank_Data(date1 = str(mnth) + '/' + str(yr))\n",
    "    tbl = search_Banks('Foreclosed (%)', func = max, df = Banks_mnth[Banks_mnth['Bank (N)'] > 100])\n",
    "    print(display(tbl[['Foreclosed (%)']]))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for yr in range(2004, 2007):\n",
    "    print('Year', yr)\n",
    "    Banks_yr = Bank_Data(date1 = '01/' + str(yr), date2 = '12/' + str(yr))\n",
    "    tbl = search_Banks('Foreclosed (%)', func = max, subset = False, df = Banks_yr[Banks_yr['Bank (N)'] > 100])\n",
    "    print(display(tbl[['Foreclosed (%)', 'Bank (%)', 'Bank (N)']]))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep:\n",
    "AMTRUST BANK\n",
    "BANK OF AMERICA, N.A.\n",
    "CITIMORTGAGE, INC.\n",
    "FIRST TENNESSEE BANK NATIONAL ASSOCIATION\n",
    "FLAGSTAR CAPITAL MARKETS CORPORATION\t\n",
    "GMAC MORTGAGE, LLC\n",
    "JPMORGAN CHASE BANK, NATIONAL ASSOCIATION\n",
    "OTHER\t\n",
    "PNC BANK, N.A.\n",
    "SUNTRUST MORTGAGE INC.\n",
    "\n",
    "Small:\n",
    "BISHOPS GATE RESIDENTIAL MORTGAGE TRUST\n",
    "FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB\n",
    "FREEDOM MORTGAGE CORP.\n",
    "GMAC MORTGAGE, LLC (USAA FEDERAL SAVINGS BANK)\n",
    "HSBC BANK USA, NATIONAL ASSOCIATION\n",
    "PHH MORTGAGE CORPORATION (USAA FEDERAL SAVINGS BANK)\n",
    "THIRD FEDERAL SAVINGS AND LOAN\n",
    "WELLS FARGO BANK, N.A.\n",
    "\n",
    "Collapse:\n",
    "CHASE HOME FINANCE\t\n",
    "CHASE HOME FINANCE (CIE 1)\t\n",
    "CHASE HOME FINANCE, LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small loan banks\n",
    "Small_Loan = ['BISHOPS GATE RESIDENTIAL MORTGAGE TRUST', \n",
    "              'FREEDOM MORTGAGE CORP.', 'GMAC MORTGAGE, LLC (USAA FEDERAL SAVINGS BANK)', \n",
    "              'HSBC BANK USA, NATIONAL ASSOCIATION', 'PHH MORTGAGE CORPORATION (USAA FEDERAL SAVINGS BANK)', \n",
    "              'THIRD FEDERAL SAVINGS AND LOAN', 'WELLS FARGO BANK, N.A.']\n",
    "df = df.replace({'Bank': Small_Loan}, 'SMALL LOAN BANKS')\n",
    "\n",
    "# Collapse similar banks\n",
    "Chase = [\"CHASE HOME FINANCE (CIE 1)\", \"CHASE HOME FINANCE, LLC\"]\n",
    "df = df.replace({'Bank': Chase}, 'CHASE HOME FINANCE')\n",
    "\n",
    "df['Bank'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Banks = ['AMTRUST BANK', 'BANK OF AMERICA, N.A.', 'CITIMORTGAGE, INC.', \n",
    "         'FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB', \n",
    "         'FIRST TENNESSEE BANK NATIONAL ASSOCIATION', 'FLAGSTAR CAPITAL MARKETS CORPORATION', \n",
    "         'GMAC MORTGAGE, LLC', 'JPMORGAN CHASE BANK, NATIONAL ASSOCIATION', 'OTHER', \n",
    "         'PNC BANK, N.A.', 'SUNTRUST MORTGAGE INC.', 'CHASE HOME FINANCE', 'SMALL LOAN BANKS']\n",
    "\n",
    "# Variables to drop\n",
    "dropvars = ['Bank', 'Original Date', 'Credit Score', 'Zip Code', 'Mortgage Insurance Type']\n",
    "\n",
    "# Bank Datasets\n",
    "Amtrust_X = onehotencoding( df[df['Bank']=='AMTRUST BANK'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Amtrust_y = Amtrust_X['Foreclosed']\n",
    "Amtrust_X = Amtrust_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "BoA_X = onehotencoding( df[df['Bank']=='BANK OF AMERICA, N.A.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "BoA_y = BoA_X['Foreclosed']\n",
    "BoA_X = BoA_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Citi_X = onehotencoding( df[df['Bank']=='CITIMORTGAGE, INC.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Citi_y = Citi_X['Foreclosed']\n",
    "Citi_X = Citi_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "IndyMac_X = onehotencoding( df[df['Bank']=='FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "IndyMac_y = IndyMac_X['Foreclosed']\n",
    "IndyMac_X = IndyMac_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Tenn_X = onehotencoding( df[df['Bank']=='FIRST TENNESSEE BANK NATIONAL ASSOCIATION'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Tenn_y = Tenn_X['Foreclosed']\n",
    "Tenn_X = Tenn_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "FlagStar_X = onehotencoding( df[df['Bank']=='FLAGSTAR CAPITAL MARKETS CORPORATION'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "FlagStar_y = FlagStar_X['Foreclosed']\n",
    "FlagStar_X = FlagStar_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "GMac_X = onehotencoding( df[df['Bank']=='GMAC MORTGAGE, LLC'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "GMac_y = GMac_X['Foreclosed']\n",
    "GMac_X = GMac_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "JPMorgan_X = onehotencoding( df[df['Bank']=='JPMORGAN CHASE BANK, NATIONAL ASSOCIATION'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "JPMorgan_y = JPMorgan_X['Foreclosed']\n",
    "JPMorgan_X = JPMorgan_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Misc_X = onehotencoding( df[df['Bank']=='OTHER'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Misc_y = Misc_X['Foreclosed']\n",
    "Misc_X = Misc_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "PNC_X = onehotencoding( df[df['Bank']=='PNC BANK, N.A.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "PNC_y = PNC_X['Foreclosed']\n",
    "PNC_X = PNC_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "SunTrust_X = onehotencoding( df[df['Bank']=='SUNTRUST MORTGAGE INC.'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "SunTrust_y = SunTrust_X['Foreclosed']\n",
    "SunTrust_X = SunTrust_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Chase_X = onehotencoding( df[df['Bank']=='CHASE HOME FINANCE'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Chase_y = Chase_X['Foreclosed']\n",
    "Chase_X = Chase_X.drop(labels='Foreclosed', axis=1) \n",
    "\n",
    "Small_X = onehotencoding( df[df['Bank']=='SMALL LOAN BANKS'] \\\n",
    "    .drop(labels=dropvars, axis=1) )\n",
    "Small_y = Small_X['Foreclosed']\n",
    "Small_X = Small_X.drop(labels='Foreclosed', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(PNC_X, PNC_y,\n",
    "                                                    stratify = PNC_y, random_state=0)\n",
    "eec = EasyEnsembleClassifier(random_state=42)\n",
    "eec.fit(X_train, y_train) # doctest: +ELLIPSIS\n",
    "\n",
    "y_pred = eec.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Grabs the entire Federal Deposit Insurance Corporation (FDIC) Statistics on\n",
    "Depository Institutions (SDI) data set.\n",
    "\n",
    "Note that this is a large data set! There are roughly 85 zip files each of\n",
    "which is between 40 and 84 MB.\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "base_url = 'https://www7.fdic.gov/sdi/Resource/AllReps/All_Reports_'\n",
    "\n",
    "# use pandas to construct a list of quarterly dates\n",
    "present = '20071231'\n",
    "datetimes = pd.date_range('20070331', end=present, freq='Q')\n",
    "dates = datetimes.format(formatter=lambda t: t.strftime('%Y%m%d'))\n",
    "\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    # ...construct the url...\n",
    "    tmp_url = base_url + date + '.zip'\n",
    "\n",
    "    # ...make the connection and grab the zipped files...\n",
    "    tmp_buffer = requests.get(tmp_url)\n",
    "\n",
    "    # ...save them to disk...\n",
    "    with open('All_Reports_' + date + '.zip', 'wb') as tmp_zip_file:\n",
    "        tmp_zip_file.write(tmp_buffer.content)\n",
    "\n",
    "    print('Done with files for ' + date + '!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Grabs the entire Federal Deposit Insurance Corporation (FDIC) institutions data\n",
    "set which catalogues the history of mergers and acquisitions for all FDIC\n",
    "regulated instutitions and turns it into a Pandas DataFrame and picles the\n",
    "object for future use.\n",
    "\n",
    "\"\"\"\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# download the data\n",
    "#base_url = 'https://www7.fdic.gov/IDASP/'\n",
    "filename = 'institutions2/INSTITUTIONS2.CSV'\n",
    "#tmp_buffer = requests.get(base_url + filename)\n",
    "#\n",
    "#with open(filename, 'wb') as tmp_zip_file:\n",
    "#    tmp_zip_file.write(tmp_buffer.content)\n",
    "#\n",
    "## convert to pandas DataFrame\n",
    "#tmp_buffer = zipfile.ZipFile(filename)\n",
    "#tmp_file = tmp_buffer.namelist()[1]\n",
    "\n",
    "used_cols = ['CERT', 'CHANGEC1']\n",
    "dtypes = {}\n",
    "tmp_dataframe = pd.read_csv(filename,\n",
    "                            usecols=used_cols,\n",
    "                            )\n",
    "#tmp_dataframe = pd.read_csv(tmp_buffer.open(tmp_file),\n",
    "#                            usecols=used_cols,\n",
    "#                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script imports the subset of the FDIC SDI data used in the analysis,\n",
    "converts the data to a Pandas data frame and writes the object to disk.\n",
    "\n",
    "There are on the order of 50 corrupted observations in the various zip files.\n",
    "Not clear why there are 90 entries in those rows instead of 89\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# use pandas to construct a list of quarterly dates\n",
    "present = '20071231'\n",
    "datetimes = pd.date_range('20070331', end=present, freq='Q')\n",
    "\n",
    "# get a list of zip files over which to iterate\n",
    "zip_files = glob.glob('*.zip')\n",
    "\n",
    "# only want to return a subset of cols (save on memory usage!)\n",
    "used_columns = ['cert', 'repdte', 'asset', 'lnlsnet', 'liab', 'dep', 'eqtot',\n",
    "                'numemp',\n",
    "                ]\n",
    "used_dtypes = {'cert': int, 'repdte': datetime, 'asset': float,\n",
    "               'lnlsnet': float, 'liab': float, 'eqtot': float, 'dep': float,\n",
    "               'numemp': float}\n",
    "\n",
    "# create a container for the individual dataframes\n",
    "dataframes = []\n",
    "\n",
    "for zip_file in zip_files[0:4]:\n",
    "\n",
    "    tmp_buffer = zipfile.ZipFile(zip_file)\n",
    "    \n",
    "    # want to work with the assets and liabilities file\n",
    "    tmp_file = tmp_buffer.namelist()[5]\n",
    "    \n",
    "    tmp_dataframe = pd.read_csv(tmp_buffer.open(tmp_file),\n",
    "                                index_col=['cert', 'repdte'],\n",
    "                                error_bad_lines=False,  # skips the mangled obs\n",
    "                                usecols=used_columns,\n",
    "                                #dtype=used_dtypes,\n",
    "                                parse_dates=True,\n",
    "                                )\n",
    "    \n",
    "    dataframes.append(tmp_dataframe)\n",
    "\n",
    "    print('Done with ' + zip_file + '!')\n",
    "\n",
    "# concatenate the quarterly dataframes into a single data frame\n",
    "combined_dataframe = pd.concat(dataframes)\n",
    "\n",
    "# convert units from thousands to billions of USD\n",
    "combined_dataframe[['asset', 'lnlsnet', 'liab', 'dep', 'eqtot']] /= 1e6\n",
    "\n",
    "# convert units from nummber of people to thousands of people\n",
    "combined_dataframe['numemp'] /= 1e3\n",
    "\n",
    "# convert to panel (major_axis: cert, minor_axis: repdte)\n",
    "combined_panel = combined_dataframe.to_panel()\n",
    "\n",
    "# pickle the object for later use!\n",
    "combined_panel.to_pickle('FDIC_SDI_panel_nominal.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataframe.reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickled data\n",
    "FDIC_SDI_panel = combined_dataframe\n",
    "\n",
    "# compute the by quarter totals for each measure\n",
    "totals = FDIC_SDI_panel.sum()\n",
    "\n",
    "# compute the base quarter totals for each measure\n",
    "base_qtr='2007-03-31'\n",
    "totals_base_qtr = totals.copy()\n",
    "totals_base_qtr[:] = totals[base_qtr]\n",
    "totals_base_qtr.fillna(method='bfill', inplace=True)\n",
    "\n",
    "def janicki_prescott_norm(item):\n",
    "    \"\"\"\n",
    "    In order to make sure results are comparable across years, I follow \n",
    "    Janicki and Prescott (2006) and deflate and re-scale each measure of bank \n",
    "    size by dividing by banking sector totals relative to some base quarter. \n",
    "    Specifically, let :math:`S_{i,t}^{raw}` denote the raw size of bank :math:`i`\n",
    "    in year :math:`t` based on one of the six size measures detailed above. The \n",
    "    normalized size of bank :math:`i` relative to the base quarter is defined as\n",
    "    follows:\n",
    "             \n",
    "    .. math::\n",
    "    \n",
    "        S_{i,t}^{norm} = \\frac{S_{i,t}^{raw}}{\\sum_{j}S_{j,t}^{raw}}\\sum_{j}S_{i,base}^{raw}\n",
    "    \n",
    "    where :math:\\sum_{j}S_{j,t}^{raw}` is the banking sector total of some size \n",
    "    measure in year :math:`t` (i.e., total banking sector assets in year :math:`t`), \n",
    "    and :math:`\\sum_{j}S_{j,base}^{raw}` is the banking sector total of the same\n",
    "    size measure in the base quarter.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (FDIC_SDI_panel[item] / totals[item]) * totals_base_qtr[item]\n",
    "\n",
    "# apply the Janicki and Prescott (2006) normalized size measure \n",
    "for item in FDIC_SDI_panel.items:\n",
    "    FDIC_SDI_panel[item] = janicki_prescott_norm(item)\n",
    "    \n",
    "# pickle the object for later use!\n",
    "FDIC_SDI_panel.to_pickle('FDIC_SDI_normed_panel.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income Groups\n",
    "income_stats = df['Median Household Income'].describe()\n",
    "\n",
    "df['Income Category'] = np.nan\n",
    "df['Income Category'][  df['Median Household Income'] <  income_stats['25%']] = 'Lowest Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= income_stats['25%'] ) & \\\n",
    "                      ( df['Median Household Income'] <  income_stats['50%'] )] = 'Low-Middle Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= income_stats['50%'] ) & \\\n",
    "                      ( df['Median Household Income'] <  income_stats['75%'] )] = 'High-Middle Income'\n",
    "df['Income Category'][  df['Median Household Income'] >= income_stats['75%']] = 'Highest Income'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income Groups\n",
    "\n",
    "df['Income Category'] = np.nan\n",
    "df['Income Category'][  df['Median Household Income'] <  df['Median Household Income'].quantile(0.2) ] = 'Lowest Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= df['Median Household Income'].quantile(0.2) ) & \\\n",
    "                      ( df['Median Household Income'] <  df['Median Household Income'].quantile(0.4) )] = 'Low-Middle Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= df['Median Household Income'].quantile(0.4) ) & \\\n",
    "                      ( df['Median Household Income'] <  df['Median Household Income'].quantile(0.6) )] = 'Middle Income'\n",
    "df['Income Category'][( df['Median Household Income'] >= df['Median Household Income'].quantile(0.6) ) & \\\n",
    "                      ( df['Median Household Income'] <  df['Median Household Income'].quantile(0.8) )] = 'High-Middle Income'\n",
    "df['Income Category'][  df['Median Household Income'] >= df['Median Household Income'].quantile(0.8) ] = 'Highest Income'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank Variables\n",
    "df2 = df\n",
    "\n",
    "NoBL = df.groupby(['Bank', 'Original Date', 'Zip Code']).size().reset_index(name='Number of Bank Loans')\n",
    "TBL = df.groupby(['Bank', 'Original Date']).size().reset_index(name='Total Bank Loans')\n",
    "PBL = pd.merge(NoBL, TBL, on=['Bank', 'Original Date'], how=\"left\")\n",
    "PBL['Proportion of Bank Loans'] = (PBL['Number of Bank Loans'] / PBL['Total Bank Loans']) * 100\n",
    "df2 = pd.merge(df2, PBL, on=['Bank', 'Original Date', 'Zip Code'], how=\"left\")\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Proportion of Bank Loans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
