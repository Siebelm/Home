{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Bad Bank Behavior<br>Analyzing Bank Mortgage during the 2007 Housing Bubble</center>  \n",
    "\n",
    "<center>Michael Siebel</center>\n",
    "<center>August 2020</center>\n",
    "<br>\n",
    "\n",
    "## <center>Functions Script</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose  \n",
    "<br>\n",
    "\n",
    "> Load necessary packages and custom functions to be used in project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import zipfile\n",
    "# Convert Time Features\n",
    "from datetime import datetime as dt\n",
    "# Data Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='chesterish', grid=False)\n",
    "# Imputing Data\n",
    "from sklearn.impute import KNNImputer\n",
    "# Splitting Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Machine Learning Packages\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier, BaggingClassifier \n",
    "from imblearn.ensemble import BalancedBaggingClassifier, RUSBoostClassifier, EasyEnsembleClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Save Runtime\n",
    "import time\n",
    "# Model Selection and Hyperparameter Tuning\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Output Statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Functions  \n",
    "<br>\n",
    "\n",
    "> I set up all data wrangling and data analysis as a series of functions, which will enable me to reuse on data from subsequent years (future projects) and various analysis techniques (this project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty data frames and arrays for function parameters called in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate function parameters\n",
    "df = pd.DataFrame()\n",
    "df_acq = pd.DataFrame()\n",
    "Banks = pd.DataFrame()\n",
    "X_train = pd.DataFrame()\n",
    "bank_str = ''\n",
    "y_train = np.array([])\n",
    "y_test = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "#### Fannie Mae data comes in two forms:  \n",
    "#### 1) acquistion data and 2) performance data  \n",
    "\n",
    "> 1) The acquisition data includes one observation for each loan with each feature representing knowledge Fannie Mae has when acquiring the loan (e.g., balance, primary lender, credit score, etc.).  \n",
    "\n",
    "> 2) The performance data includes observations for each month each loan is held and information on the payment of the loan.  \n",
    "\n",
    "I use the acquisition data as predictors for a dichotomous categorization of whether the homeowner defaulted on their loan, a target variable I create using the performance data and merging onto the acquisition data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(acq, per=None):\n",
    "    # Load acquistion data\n",
    "    df_acq = pd.read_csv(acq, sep='|', header=None)\n",
    "    # Specify the name of the columns\n",
    "    df_acq.columns = ['Loan ID','Origination Channel','Bank','Original Interest Rate',\n",
    "                      'Original Mortgage Amount','Original Loan Term','Original Date','First Payment',\n",
    "                      'Original Loan-to-Value (LTV)','Original Combined Loan-to-Value (CLTV)',\n",
    "                      'Number of Borrowers','Original Debt to Income Ratio','Credit Score',\n",
    "                      'First Time Home Buyer','Loan Purpose','Property Type','Number of Units',\n",
    "                      'Occupancy Type','Property State','Zip Code','Mortgage Insurance %',\n",
    "                      'Product Type','Co-Borrower Credit Score','Mortgage Insurance Type',\n",
    "                      'Relocation Mortgage Indicator']\n",
    "    \n",
    "    if per is not None:\n",
    "        df_per = pd.read_csv(per, sep='|', header=None)\n",
    "        df_per.columns = ['Loan ID','MonthRep','Servicer','Curr Interest Rate','CAUPB','Loan Age',\n",
    "                          'Months To Maturity','Add Months To Maturity','Maturity Date','MSA','CLDS',\n",
    "                          'Mod Flag','Zero Bal Code','Zero Bal Date','Last Install Date','Foreclosure Date',\n",
    "                          'Disposition Date','FCCCost','PPRC','Asset Rec Cost','MHRC','ATFHP',\n",
    "                          'Net Sale Proceeds','Credit Enh Proceeds','RPMWP','OFP','NIBUPB','PFUPB','RMWPF',\n",
    "                          'FPWA','Servicing Indicator']\n",
    "    else: df_per = None\n",
    "        \n",
    "    return df_acq, df_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target Variable\n",
    "<br>\n",
    "\n",
    "> Performance data is much larger as it is transaction based, while the acquistion data has the loan owner as its unit of analysis.  \n",
    "\n",
    "> I retain only the most recent performance transaction relating to foreclosure, then drop all other variables except  Loan ID (the primary key) and merge performance data onto acquisition data.  \n",
    "\n",
    "> I recode performance data into dichotomous categorization of whether loan was foreclosed upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge foreclosures from performance data to acquistion data\n",
    "def merge_df(df_acq, df_per):\n",
    "    \n",
    "    # Columns to maintain from Performance Data\n",
    "    per_ColKeep = ['Loan ID','Foreclosure Date']\n",
    "    df_per = df_per[per_ColKeep]\n",
    "    df_per = df_per.drop_duplicates(subset='Loan ID', keep='last')\n",
    "    df = pd.merge(df_acq, df_per, on='Loan ID', how='inner')\n",
    "    \n",
    "    # Set Foreclosed to binary\n",
    "    df.loc[df['Foreclosure Date'].isnull(), 'Foreclosed'] = 0\n",
    "    df.loc[df['Foreclosure Date'].notnull(),'Foreclosed'] = 1\n",
    "    df = df.drop(columns='Foreclosure Date')\n",
    "    df['Foreclosed'] = df['Foreclosed'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL of External Data\n",
    "<br>\n",
    "\n",
    "> Federal Reserve Economic Data (FRED) includes macro economic data that is related to the housing market.  This data is merged on the date variable (mm/yyyy); it includes monthly data and quarterly data, the latter used carryforward hard coding (each quarter represented the beginning of the quarter) to cover each month.  Some FRED sets included four Census region subsets; these were merged on the date variable (mm/yyyy) and the property state variable, the latter was mapped to the four Census regions of Northeast, Midwest, South, and West.  Values were converted to quarterly and yearly deltas (e.g., the change in housing vacancies from 2006 Q4 to 2007 Q1 or 2006 Q1 to 2007 Q1).\n",
    "\n",
    "> Federal Deposit Insurance Corporation (FDIC) data includes information on FDIC-backed banks, such as their number of employees, assets, debts, etc.  I used regular expressions to map FDIC data to the Bank variable; this included summing various instances of the same bank (from a different branch or functional area).  This data is merged on the bank variable and the date variable (mm/yyyy); it includes quarterly data, which used carrybackward hard coding (each quarter represented the end of the quarter) to cover each month.  Values were converted to quarterly and yearly deltas (e.g., the change in Bank of America liabilities from 2006 Q4 to 2007 Q1 or 2006 Q1 to 2007 Q1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge FRED on Monthly Data\n",
    "\n",
    "Carryforward hard coding if data is quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fred_on_month(df_mnth, merge_df = df, varname = '', quarter=False, pct_change=1):\n",
    "    # Split date var\n",
    "    df_mnth['Month'] = df_mnth['DATE'].apply(str).apply(lambda x: x.split('/')[0].strip()).apply(str)\n",
    "    df_mnth['Year'] = df_mnth['DATE'].apply(str).apply(lambda x: x.split('/')[1].strip()).apply(str)\n",
    "    df_mnth = df_mnth.drop(labels='DATE', axis=1)\n",
    "        \n",
    "    # Retrieve name of main column\n",
    "    var = df_mnth.columns[0]\n",
    "    # Period change\n",
    "    df_mnth[var] = df_mnth[var].pct_change(pct_change)\n",
    "    # Ensure correct dtype\n",
    "    df_mnth[var] = df_mnth[var].astype(float)\n",
    "    \n",
    "    # If quarterly data,\n",
    "    # Carry first month of quarter forward\n",
    "    if quarter:\n",
    "        for i in range(df_mnth.shape[0]):\n",
    "            if df_mnth['Month'][i]=='01':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '01'}, '02')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '01'}, '03')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "            elif df_mnth['Month'][i]=='04':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '04'}, '05')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '04'}, '06')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "            elif df_mnth['Month'][i]=='07':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '07'}, '08')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '07'}, '09')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "            elif df_mnth['Month'][i]=='10':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '10'}, '11')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '10'}, '12')\n",
    "                df_mnth = df_mnth.append(new_row)     \n",
    "    \n",
    "    # Create merge var\n",
    "    df_mnth['Original Date'] = (df_mnth['Month'].map(str) + '/' + df_mnth['Year']).apply(str)\n",
    "    df_mnth = df_mnth.rename(columns={var: varname})\n",
    "\n",
    "    # Merge\n",
    "    merge_df = pd.merge(merge_df, df_mnth, on='Original Date', how='inner')\n",
    "    merge_df['Sort'] = (merge_df['Year'].map(str) + merge_df['Month']).apply(str)\n",
    "    merge_df = merge_df.sort_values(by=['Sort'])\n",
    "    merge_df = merge_df.drop(labels=['Year', 'Month', 'Sort'], axis=1)\n",
    "    \n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State to Region Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_region(df_new, var, state_drop=False):\n",
    "    # Region-State crosswalk\n",
    "    Northeast = ['ME','VT','NH','MA','NY','RI','CT','PA','NJ']\n",
    "    South = ['DE','MD','DC','WV','VA','KY','NC','TN','SC','GA','FL','AL','MS','AR','LA','OK','TX']\n",
    "    Midwest = ['ND','SD','NE','KS','MN','IA','MO','WI','IL','MI','IN','OH']\n",
    "    West = ['WA','OR','ID','MT','WY','CA','NV','UT','AZ','CO','NM','AK','HI']\n",
    "    \n",
    "    # Replace States with Census regions\n",
    "    df_new['Region'] = df_new[var]\n",
    "    df_new['Region'] = df_new['Region'].replace(Northeast, 'Northeast')\n",
    "    df_new['Region'] = df_new['Region'].replace(South, 'South')\n",
    "    df_new['Region'] = df_new['Region'].replace(Midwest, 'Midwest')\n",
    "    df_new['Region'] = df_new['Region'].replace(West, 'West')\n",
    "    \n",
    "    # Drop State var\n",
    "    if state_drop:\n",
    "        df_new = df_new.drop(labels=var, axis=1)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge regional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_merge(NE, SO, MW, WE, varname, df_orig = df, quarter=False, pct_change=1):\n",
    "    # Northeast\n",
    "    Northeast = df_orig[df_orig['Region']=='Northeast']\n",
    "    Northeast = Northeast[['Loan ID', 'Region', 'Original Date']]\n",
    "    Northeast = merge_fred_on_month(df_mnth = NE, merge_df = Northeast, \n",
    "                                    varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    Northeast = Northeast.rename(columns={Northeast.columns[3]: varname})\n",
    "    \n",
    "    # South\n",
    "    South = df_orig[df_orig['Region']=='South']\n",
    "    South = South[['Loan ID', 'Region', 'Original Date']]\n",
    "    South = merge_fred_on_month(df_mnth = SO, merge_df = South, \n",
    "                                    varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    South = South.rename(columns={South.columns[3]: varname})\n",
    "    \n",
    "    # Midwest\n",
    "    Midwest = df_orig[df_orig['Region']=='Midwest']\n",
    "    Midwest = Midwest[['Loan ID', 'Region', 'Original Date']]\n",
    "    Midwest = merge_fred_on_month(df_mnth = MW, merge_df = Midwest, \n",
    "                                  varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    Midwest = Midwest.rename(columns={Midwest.columns[3]: varname})\n",
    "    \n",
    "    # West\n",
    "    West = df_orig[df_orig['Region']=='West']\n",
    "    West = West[['Loan ID', 'Region', 'Original Date']]\n",
    "    West = merge_fred_on_month(df_mnth = WE, merge_df = West, \n",
    "                               varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    West = West.rename(columns={West.columns[3]: varname})\n",
    "    \n",
    "    # Stack\n",
    "    df_region = pd.concat([Northeast, South, Midwest, West])\n",
    "    df_region = df_region[['Loan ID', varname]]\n",
    "    \n",
    "    # Merge\n",
    "    df_new = pd.merge(df_orig, df_region, on='Loan ID', how='inner')\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRED Data merge wrapper (full US) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fred_merge(fred_df, df_orig = df, quarter=True, varname = ''):\n",
    "    # Define units within year\n",
    "    if quarter:\n",
    "        pct_qtr = 1\n",
    "        pct_year = 4\n",
    "    else:\n",
    "        pct_qtr = 4\n",
    "        pct_year = 12    \n",
    "    # Define variable name, if not set\n",
    "    if varname=='':\n",
    "        varname = str(fred_df)\n",
    "    \n",
    "    # convert datetime\n",
    "    fred_df['DATE'] = pd.to_datetime(fred_df['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    \n",
    "    # merge FRED data and convert to percent change\n",
    "    df_new = merge_fred_on_month(fred_df, df_orig, varname, quarter=quarter, pct_change=pct_qtr)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Qtr)')})\n",
    "    df_new = merge_fred_on_month(fred_df, df_new, varname, quarter=quarter, pct_change=pct_year)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Yr)')})\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRED merge wrapper (region) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fred_merge_region(NE, SO, MW, WE, df_orig = df, varname = '', quarter=True):\n",
    "    # Define units within year\n",
    "    if quarter:\n",
    "        pct_qtr = 1\n",
    "        pct_year = 4\n",
    "    else:\n",
    "        pct_qtr = 4\n",
    "        pct_year = 12    \n",
    "    # Define variable name, if not set\n",
    "    if varname=='':\n",
    "        varname = str(fred_df)\n",
    "    \n",
    "    # convert datetime\n",
    "    NE['DATE'] = pd.to_datetime(NE['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    SO['DATE'] = pd.to_datetime(SO['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    MW['DATE'] = pd.to_datetime(MW['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    WE['DATE'] = pd.to_datetime(WE['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "\n",
    "    # merge FRED data and convert to percent change\n",
    "    df_new = region_merge(NE=NE, SO=SO, MW=MW, WE=WE, df_orig = df_orig, \n",
    "                          varname=varname, quarter=quarter, pct_change=pct_qtr)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Qtr)')})\n",
    "    df_new = region_merge(NE=NE, SO=SO, MW=MW, WE=WE, df_orig = df_new, \n",
    "                          varname=varname, quarter=quarter, pct_change=pct_year)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Yr)')})\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert FDIC Data to Monthly Data\n",
    "\n",
    "Carrybackwards hard coding to convert quarterly data to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdic_on_month(df_mnth):\n",
    "    # Retrieve names of columns\n",
    "    cols = df_mnth.columns\n",
    "    \n",
    "    # convert datetime\n",
    "    df_mnth['repdte'] = pd.to_datetime(df_mnth['repdte']).dt.strftime('%m/%Y').apply(str)\n",
    "    \n",
    "    # Split date var\n",
    "    df_mnth['Month'] = df_mnth['repdte'].apply(str).apply(lambda x: x.split('/')[0].strip()).apply(str)\n",
    "    df_mnth['Year'] = df_mnth['repdte'].apply(str).apply(lambda x: x.split('/')[1].strip()).apply(str)\n",
    "    \n",
    "    # Carry first month of quarter forward\n",
    "    for i in range(df_mnth.shape[0]):\n",
    "        if df_mnth['Month'][i]=='03':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '03'}, '01')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '03'}, '02')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "        elif df_mnth['Month'][i]=='06':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '06'}, '04')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '06'}, '05')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "        elif df_mnth['Month'][i]=='09':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '09'}, '07')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '09'}, '08')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "        elif df_mnth['Month'][i]=='12':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '12'}, '10')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '12'}, '11')\n",
    "            df_mnth = df_mnth.append(new_row)          \n",
    "        \n",
    "    # Create merge var\n",
    "    df_mnth['Original Date'] = (df_mnth['Month'].map(str) + '/' + df_mnth['Year']).apply(str)\n",
    "    df_mnth['Sort'] = (df_mnth['Year'].map(str) + df_mnth['Month']).apply(str)\n",
    "    df_mnth = df_mnth.sort_values(by=['Sort'])\n",
    "    df_mnth = df_mnth.drop(labels=['Month', 'Year', 'Sort', 'repdte'], axis=1)\n",
    "    \n",
    "    return df_mnth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of features with really high missingness or no data variation, and then mean/mode hard coding on features with low missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_treat(df):\n",
    "    # Find features with 10% missing or more\n",
    "    condition = ( df.isnull().sum(axis=0)/df.shape[0]*100 ) \n",
    "    df_HighMissing = condition > 10 \n",
    "    \n",
    "    # Save features that contain missing data\n",
    "    df_HighMissing = df_HighMissing.index[df_HighMissing.values == True]\n",
    "    \n",
    "    # remove high missing features\n",
    "    df = df.drop(labels=df_HighMissing, axis=1)\n",
    "        \n",
    "    # impute on the mean for low missing features that are continuous   \n",
    "    df_cont = df.select_dtypes(include=['float64', 'int64'])\n",
    "    df[df_cont.columns] = df_cont.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    \n",
    "    # impute on the mode for low missing features that are categorical   \n",
    "    df_cat = df.select_dtypes(include=['object'])\n",
    "    df[df_cat.columns] = df_cat.apply(lambda x: x.fillna(x.mode()),axis=0)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_imputations(df, X_cols, n_neighbors=2):\n",
    "    KNN_impute = KNNImputer(n_neighbors=n_neighbors, weights=\"uniform\")\n",
    "    df = KNN_impute.fit_transform(df)\n",
    "    df = pd.DataFrame(df, columns=X_cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing date features to numeric, if one decides to use time as a ordinal feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_date(df, var_str):\n",
    "    \n",
    "    # Convert to ordinal\n",
    "    df[var_str] = df[var_str].apply(lambda x: dt.strptime(x, '%m/%Y').toordinal())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding\n",
    "\n",
    "Converts categorical variables to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(df):\n",
    "    columns = df.columns[df.isnull().any()]\n",
    "    nan_cols = df[columns]\n",
    "\n",
    "    df = df.drop(nan_cols.columns, axis=1)\n",
    "\n",
    "    df_cat = df.select_dtypes(include=['object'])\n",
    "    onehot = pd.get_dummies(df_cat)\n",
    "    \n",
    "    df_cont = df.drop(df_cat.columns, axis=1)\n",
    "\n",
    "    df = pd.concat([df_cont,onehot,nan_cols], axis=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "\n",
    "Run permutation importance and score based on ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_importance(X_train, y_train, bank_str, sample='bal', max_features=0.5):\n",
    "    # Transform X\n",
    "    ## define datasets \n",
    "    y = y_train  \n",
    "    X = X_train\n",
    "    readd = X.loc[:, str('Bank_' + bank_str)]\n",
    "    X = X.filter(regex=r'^(?!Bank_).*$')\n",
    "    X.loc[:, str('Bank_' + bank_str)] = readd\n",
    "    \n",
    "    ## Add interaction terms\n",
    "    X = Bank_Interactions(X, bank_str = bank_str)\n",
    "    \n",
    "    ## Standardize Vars\n",
    "    X_cols = X.columns\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    # Permutation importance for feature evaluation\n",
    "    if sample=='bal':\n",
    "        clf = BalancedRandomForestClassifier(n_estimators=50, random_state=2020, max_features=max_features, \n",
    "                                             replacement=False, n_jobs=-1)\n",
    "    elif sample=='wgt':\n",
    "        clf = RandomForestClassifier(n_estimators=50, random_state=2020, max_features=max_features, \n",
    "                                     class_weight={1: 0.1, 0: 0.9}, n_jobs=-1)     \n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=50, random_state=2020, max_features=max_features, \n",
    "                                     n_jobs=-1)\n",
    "    \n",
    "    clf = clf.fit(X, y)\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, scoring='roc_auc_score',\n",
    "                                    random_state=2020)\n",
    "    importances = pd.Series(result.importances_mean, index=X_cols)\n",
    "    \n",
    "    return(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foreclosure Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Foreclosure_Data(date1 = \"\", date2 = \"\", subset = \"\", df = df):\n",
    "    # Subset by Date\n",
    "    if (date1 == \"\" and date2 == \"\"):\n",
    "        df_sub = df\n",
    "    elif (date1 != \"\" and date2 != \"\"):\n",
    "        date = pd.date_range(date1, date2)\n",
    "        month_yr = np.array([])\n",
    "        for i in range(len(date)): \n",
    "            month_yr = np.append( month_yr, str( str(date.month[i]) + '/' + str(date.year[i]) ) )\n",
    "        month_yr = np.unique(month_yr)\n",
    "        month_yr = np.char.zfill(month_yr, 7)\n",
    "        df_sub = df[df['Original Date'].isin(month_yr)]\n",
    "    elif (date2 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date1]\n",
    "    elif (date1 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date2]\n",
    "    \n",
    "    # Subset by other variable\n",
    "    if (date1 == \"\" and date2 == \"\" and subset != \"\"):\n",
    "        df_sub = df[eval(subset)]\n",
    "        \n",
    "    # Foreclosures represented\n",
    "    Foreclosed = ['Not Forclosed', 'Forclosed']\n",
    "    Target = df_sub.groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "    # Original Mortgage Amount\n",
    "    ORM = df_sub.groupby(['Foreclosed']).agg({'Original Mortgage Amount': 'mean'}).round(2)\n",
    "    # Credit Score\n",
    "    CS = df_sub.groupby(['Foreclosed']).agg({'Harmonized Credit Score': 'mean'}).astype(int)\n",
    "    # Original Debt to Income Ratio\n",
    "    DTI = df_sub.groupby(['Foreclosed']).agg({'Original Debt to Income Ratio': 'mean'}).round(1)\n",
    "    # First Time Home Buyer\n",
    "    FT = df_sub[df_sub['First Time Home Buyer']=='Y'].groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "    # Refinance\n",
    "    LP = df_sub[df_sub['Loan Purpose']!=0].groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "    # Original Interest Rate\n",
    "    IR = df_sub.groupby(['Foreclosed']).agg({'Original Interest Rate': 'mean'}).round(2)\n",
    "    # Original Loan Term\n",
    "    LT = df_sub.groupby(['Foreclosed']).agg({'Original Loan Term': 'mean'}).astype(int)\n",
    "    # Original Combined Loan-to-Value (CLTV)\n",
    "    CLTV = df_sub.groupby(['Foreclosed']).agg({'Original Combined Loan-to-Value (CLTV)': 'mean'}).round(1)\n",
    "    # Single Borrower Ratio\n",
    "    SBR = df_sub.groupby(['Foreclosed']).agg({'Single Borrower': 'mean'}).round(2)\n",
    "    # Mortgage Insurance Type\n",
    "    MIT = df_sub.groupby(['Foreclosed']).agg({'Mortgage Insurance Type': 'mean'}).round(2)\n",
    "    # Mortgage Insurance %\n",
    "    MIP = df_sub.groupby(['Foreclosed']).agg({'Mortgage Insurance %': 'mean'}).round(2)    \n",
    "    # Median Household Income\n",
    "    MHI = df_sub.groupby(['Foreclosed']).agg({'Median Household Income': 'mean'}).round(2)\n",
    "    \n",
    "    # Create Dataset\n",
    "    df_new = pd.DataFrame({ 'Foreclosed': Foreclosed, \n",
    "                            'Foreclosed (%)': ((Target['Total'] / df_sub.shape[0]) * 100).round(1),\n",
    "                            'Foreclosed (N)': df_sub.groupby(['Foreclosed']).size(),\n",
    "                            'Mortgage Amount ($)': ORM['Original Mortgage Amount'].tolist(),\n",
    "                            'Credit Score': CS['Harmonized Credit Score'].tolist(),\n",
    "                            'Debt to Income Ratio': DTI['Original Debt to Income Ratio'].tolist(),\n",
    "                            'First Time Home Buyer (%)': ((FT['Total'] / Target['Total']) * 100).round(1).tolist(),\n",
    "                            'Refinanced': ((LP['Total'] / Target['Total']) * 100).round(1).tolist(),\n",
    "                            'Interest Rate': IR['Original Interest Rate'].tolist(),\n",
    "                            'Loan Term': LT['Original Loan Term'].tolist(),\n",
    "                            'Combined Loan-to-Value (CLTV)': CLTV['Original Combined Loan-to-Value (CLTV)'].tolist(),\n",
    "                            'Single Borrower Ratio': SBR['Single Borrower'].tolist(),\n",
    "                            'Mortgage Insurance Ratio': MIT['Mortgage Insurance Type'].tolist(),\n",
    "                            'Mortgage Insurance %': MIP['Mortgage Insurance %'].tolist(),\n",
    "                            'Estimated Household Income ($)': MHI['Median Household Income'].tolist()\n",
    "                         })\n",
    "    \n",
    "    df_new = df_new.set_index('Foreclosed')\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bank Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bank_Data(date1 = \"\", date2 = \"\", subset = \"\", df = df):\n",
    "    # Subset by Date\n",
    "    if (date1 == \"\" and date2 == \"\"):\n",
    "        df_sub = df\n",
    "    elif (date1 != \"\" and date2 != \"\"):\n",
    "        date = pd.date_range(date1, date2)\n",
    "        month_yr = np.array([])\n",
    "        for i in range(len(date)): \n",
    "            month_yr = np.append( month_yr, str( str(date.month[i]) + '/' + str(date.year[i]) ) )\n",
    "        month_yr = np.unique(month_yr)\n",
    "        month_yr = np.char.zfill(month_yr, 7)\n",
    "        df_sub = df[df['Original Date'].isin(month_yr)]\n",
    "    elif (date2 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date1]\n",
    "    elif (date1 == \"\"):\n",
    "        df_sub = df[df['Original Date']==date2]\n",
    "    \n",
    "    # Subset by other variable\n",
    "    if (date1 == \"\" and date2 == \"\" and subset != \"\"):\n",
    "        df_sub = df[eval(subset)]\n",
    "        \n",
    "    # Banks represented\n",
    "    Banks = df_sub.groupby(['Bank']).size().reset_index(name='Total')\n",
    "    # Foreclosures\n",
    "    Target = df_sub.groupby(['Bank']).agg({'Foreclosed': 'mean'})\n",
    "    # Original Mortgage Amount\n",
    "    ORM = df_sub.groupby(['Bank']).agg({'Original Mortgage Amount': 'mean'}).round(2)\n",
    "    # Credit Score\n",
    "    CS = df_sub.groupby(['Bank']).agg({'Harmonized Credit Score': 'mean'}).astype(int)\n",
    "    # Original Debt to Income Ratio\n",
    "    DTI = df_sub.groupby(['Bank']).agg({'Original Debt to Income Ratio': 'mean'}).round(1)\n",
    "    # First Time Home Buyer\n",
    "    FT = df_sub[df_sub['First Time Home Buyer']=='Y'].groupby(['Bank']).size().reset_index(name='Total')\n",
    "    # Refinance\n",
    "    LP = df_sub[df_sub['Loan Purpose']!=0].groupby(['Bank']).size().reset_index(name='Total')\n",
    "    # Original Interest Rate\n",
    "    IR = df_sub.groupby(['Bank']).agg({'Original Interest Rate': 'mean'}).round(2)\n",
    "    # Original Loan Term\n",
    "    LT = df_sub.groupby(['Bank']).agg({'Original Loan Term': 'mean'}).astype(int)\n",
    "    # Original Combined Loan-to-Value (CLTV)\n",
    "    CLTV = df_sub.groupby(['Bank']).agg({'Original Combined Loan-to-Value (CLTV)': 'mean'}).round(1)\n",
    "    # Single Borrower Ratio\n",
    "    SBR = df_sub.groupby(['Bank']).agg({'Single Borrower': 'mean'}).round(2)\n",
    "    # Mortgage Insurance Type\n",
    "    MIT = df_sub.groupby(['Bank']).agg({'Mortgage Insurance Type': 'mean'}).round(2)\n",
    "    # Mortgage Insurance %\n",
    "    MIP = df_sub.groupby(['Bank']).agg({'Mortgage Insurance %': 'mean'}).round(2)     \n",
    "    # Median Household Income\n",
    "    MHI = df_sub.groupby(['Bank']).agg({'Median Household Income': 'mean'}).round(2)\n",
    "    \n",
    "    # Create Dataset\n",
    "    df_new = pd.DataFrame({ 'Bank': Banks['Bank'], \n",
    "                            'Bank (%)': ((Banks['Total'] / df_sub.shape[0]) * 100).round(1),\n",
    "                            'Bank (N)': Banks['Total'],\n",
    "                            'Foreclosed (%)': ((Target['Foreclosed'] * 100).round(1)).tolist(), \n",
    "                            'Mortgage Amount ($)': ORM['Original Mortgage Amount'].tolist(),\n",
    "                            'Credit Score': CS['Harmonized Credit Score'].tolist(),\n",
    "                            'Debt to Income Ratio': DTI['Original Debt to Income Ratio'].tolist(),\n",
    "                            'First Time Home Buyer (%)': ((FT['Total'] / Banks['Total']) * 100).round(1).tolist(),\n",
    "                            'Refinance': ((LP['Total'] / Banks['Total']) * 100).round(1).tolist(),\n",
    "                            'Interest Rate': IR['Original Interest Rate'].tolist(),\n",
    "                            'Loan Term': LT['Original Loan Term'].tolist(),\n",
    "                            'Combined Loan-to-Value (CLTV)': CLTV['Original Combined Loan-to-Value (CLTV)'].tolist(),\n",
    "                            'Single Borrower Ratio': SBR['Single Borrower'].tolist(),\n",
    "                            'Mortgage Insurance Ratio': MIT['Mortgage Insurance Type'].tolist(),\n",
    "                            'Mortgage Insurance %': MIP['Mortgage Insurance %'].tolist(),                           \n",
    "                            'Median Household Income ($)': MHI['Median Household Income'].tolist()\n",
    "                        })\n",
    "    \n",
    "    df_new = df_new.set_index(\"Bank\")\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate banks based on maximum, minimum, or other meaningful values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_Banks(col, df = Banks, func = max, subset = True):\n",
    "    print(col, func.__name__, \"value\")\n",
    "    if (subset): cols = col\n",
    "    else: cols = df.columns\n",
    "    values = pd.DataFrame(df[cols][df[col] == func(df[col])])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Data Frames based on Bank in Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create single-Bank only subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# List of banks for reference\n",
    "Banks = ['AMTRUST BANK', 'BANK OF AMERICA, N.A.', 'CITIMORTGAGE, INC.', \n",
    "         'FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB', \n",
    "         'FIRST TENNESSEE BANK NATIONAL ASSOCIATION', 'FLAGSTAR CAPITAL MARKETS CORPORATION', \n",
    "         'GMAC MORTGAGE', 'JPMORGAN CHASE BANK, NATIONAL ASSOCIATION', 'OTHER', \n",
    "         'PNC BANK, N.A.', 'SUNTRUST MORTGAGE INC.', 'CHASE HOME FINANCE', 'SMALL LOAN BANKS']\n",
    "'''\n",
    "\n",
    "# Function to subset banking datasets\n",
    "def Bank_Subsets(bank_strs, df_X = X_train, df_y = y_train):\n",
    "    # Initiate Bank dictionaries\n",
    "    X = {}\n",
    "    y = {}\n",
    "\n",
    "    # Bank Subset\n",
    "    for bank_str in bank_strs:\n",
    "        X[bank_str] = df_X.loc[:, df_X[str('Bank_' + bank_str)]==1] \\\n",
    "            .filter(regex=r'^(?!Bank_).*$')\n",
    "        y[bank_str] = df_y[np.array(df_X[str('Bank_' + bank_str)]==1)]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bank Interactions\n",
    "\n",
    "Create interaction terms for loan-based continuous features to understand unique aspects of bank in question on these terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bank_Interactions(df_X = X_train, bank_str = bank_str):\n",
    "    # Use loan-based continuous features as interaction terms\n",
    "    vars = ['Original Mortgage Amount', 'Original Interest Rate', \n",
    "            'Original Combined Loan-to-Value (CLTV)', 'Original Debt to Income Ratio', \n",
    "            'Mortgage Insurance %', 'Median Household Income',\n",
    "            'Loan Change (1 Year)', 'Loan Change (5 Years)']\n",
    "    \n",
    "    # Times each loan-based continuous feature by Bank binary feature\n",
    "    for var in vars:\n",
    "        df_X.loc[:, str(var + ' [Int]')] = df_X[var] * df_X.loc[:, str('Bank_' + bank_str)]\n",
    "        \n",
    "    return df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualizations of Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot target variable (and predictions)\n",
    "\n",
    "Visualize the percentage and frequency of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_values(df_depvar, data=False, prediction=False):\n",
    "        \n",
    "    # save target frequencies\n",
    "    target_frequency = df_depvar.value_counts()\n",
    "        \n",
    "    # save target percentage\n",
    "    target_percentage = round((df_depvar.value_counts()/df_depvar.count())*100).astype(int)\n",
    "        \n",
    "    # graphing target variable\n",
    "    jtplot.style(ticks=True, grid=False)\n",
    "    plt.figure(figsize=(14,4))\n",
    "    target_percentage.plot.barh(stacked=True, color='#ca2c92').invert_yaxis()\n",
    "    if data:\n",
    "        plt.suptitle('Bar Chart of Target Variable', fontsize=18)\n",
    "    elif prediction:\n",
    "        plt.suptitle('Bar Chart of Predictions', fontsize=18)\n",
    "    else:\n",
    "        plt.suptitle('Percent of Mortage Defaults', fontsize=18)\n",
    "    plt.ylabel('Foreclosed')\n",
    "    plt.xlabel('Percentage')\n",
    "    plt.xlim([0,100])\n",
    "    # plt.yticks([0, 1], ['Did not Foreclose', 'Foreclosed'])\n",
    "    plt.show()\n",
    "    \n",
    "    # display frequency of foreclosures\n",
    "    print('Frequency of Foreclosures\\n', target_frequency, '\\n', sep='')\n",
    "    \n",
    "    # display percentage of foreclosures\n",
    "    print('Percentage of Foreclosures\\n', target_percentage, '\\n', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize scores at various classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(target_prob, y_test = y_test):\n",
    "    # Determine threshold\n",
    "    threshold = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, \n",
    "                 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "    \n",
    "    acc = []\n",
    "    prec = []\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    best_auc = {'Threshold': 0.5, 'Best ROC AUC Score': 0.0}\n",
    "    best_acc = {'Threshold': 0.5, 'Best Accuracy Score': 0.0}\n",
    "    best_prec = {'Threshold': 0.5, 'Best Precision Score': 0.0}\n",
    "    best_f1 = {'Threshold': 0.5, 'Best F1 Score': 0.0}\n",
    "    for i in range(len(threshold)):\n",
    "        y_pred = target_prob.map(lambda x: 1 if x >= threshold[i] else 0)\n",
    "        \n",
    "        # Accuracy\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "        # Precision\n",
    "        prec.append(precision_score(y_test, y_pred))\n",
    "        # F1 \n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        # Avg\n",
    "        auc.append(roc_auc_score(y_test, y_pred))\n",
    "        \n",
    "        # Save best accuracy\n",
    "        if (best_acc['Best Accuracy Score'] < acc[i]):\n",
    "            best_acc = {'Threshold': threshold[i], 'Best Accuracy Score': acc[i]}\n",
    "        # Save best precision\n",
    "        if (best_prec['Best Precision Score'] < prec[i]):\n",
    "            best_prec = {'Threshold': threshold[i], 'Best Precision Score': prec[i]}      \n",
    "        # Save best f1\n",
    "        if (best_f1['Best F1 Score'] < f1[i]):\n",
    "            best_f1 = {'Threshold': threshold[i], 'Best F1 Score': f1[i]}       \n",
    "        # Save best Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "        if (best_auc['Best ROC AUC Score'] < auc[i]):\n",
    "            best_auc = {'Threshold': threshold[i], 'Best ROC AUC Score': auc[i]}   \n",
    "    \n",
    "    # Plot\n",
    "    df_plot = pd.DataFrame({'Threshold': threshold, 'Accuracy': acc, \n",
    "                            'Precision': prec, 'F1': f1, 'ROC AUC': auc})\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(df_plot['Threshold'], df_plot.iloc[:,1:5])\n",
    "    plt.title('Scores at Various Thresholds')\n",
    "    plt.legend(['Accuracy', 'Precision', 'F1', 'ROC AUC'])\n",
    "    print(plt.show())\n",
    "    \n",
    "    # Scores\n",
    "    y_pred = target_prob.map(lambda x: 1 if x >= best_auc['Threshold'] else 0)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return( best_auc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Target Classes\n",
    "\n",
    "Visualizes if there are any obvious classification boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
