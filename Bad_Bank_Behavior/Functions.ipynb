{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Bad Bank Behavior<br>Analyzing Bank Mortgage during the 2007 Housing Bubble</center>  \n",
    "\n",
    "<center>Michael Siebel</center>\n",
    "<center>August 2020</center>\n",
    "<br>\n",
    "\n",
    "## <center>Functions Script</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose  \n",
    "<br>\n",
    "\n",
    "> Load necessary packages and custom functions to be used in project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import zipfile\n",
    "# Convert Time Features\n",
    "from datetime import datetime as dt\n",
    "# Data Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='chesterish', grid=False)\n",
    "# Imputing Data\n",
    "from sklearn.impute import KNNImputer\n",
    "# Splitting Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Data Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "# Multicollinearity\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "# Machine Learning Packages\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier, BaggingClassifier \n",
    "from imblearn.ensemble import BalancedBaggingClassifier, RUSBoostClassifier, EasyEnsembleClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Neural Network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, BatchNormalization\n",
    "from imblearn.keras import BalancedBatchGenerator\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Save Runtime\n",
    "import time\n",
    "# Model Selection and Hyperparameter Tuning\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Output Statistics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Functions  \n",
    "<br>\n",
    "\n",
    "> I set up all data wrangling and data analysis as a series of functions, which will enable me to reuse on data from subsequent years (future projects) and various analysis techniques (this project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty data frames and arrays for function parameters called in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate function parameters\n",
    "df = pd.DataFrame()\n",
    "df_acq = pd.DataFrame()\n",
    "Banks = pd.DataFrame()\n",
    "X_train = pd.DataFrame()\n",
    "X_val = pd.DataFrame()\n",
    "X_test = pd.DataFrame()\n",
    "banks = ''\n",
    "bank_str = ''\n",
    "y_train = np.array([])\n",
    "y_val = np.array([])\n",
    "y_test = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "#### Fannie Mae data comes in two forms:  \n",
    "#### 1) acquistion data and 2) performance data  \n",
    "\n",
    "> 1) The acquisition data includes one observation for each loan with each feature representing knowledge Fannie Mae has when acquiring the loan (e.g., balance, primary lender, credit score, etc.).  \n",
    "\n",
    "> 2) The performance data includes observations for each month each loan is held and information on the payment of the loan.  \n",
    "\n",
    "I use the acquisition data as predictors for a dichotomous categorization of whether the homeowner defaulted on their loan, a target variable I create using the performance data and merging onto the acquisition data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target Variable\n",
    "<br>\n",
    "\n",
    "> Performance data is much larger as it is transaction based, while the acquistion data has the loan owner as its unit of analysis.  \n",
    "\n",
    "> I retain only the most recent performance transaction relating to foreclosure, then drop all other variables except  Loan ID (the primary key) and merge performance data onto acquisition data.  \n",
    "\n",
    "> I recode performance data into dichotomous categorization of whether loan was foreclosed upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL of External Data\n",
    "<br>\n",
    "\n",
    "> Federal Reserve Economic Data (FRED) includes macro economic data that is related to the housing market.  This data is merged on the date variable (mm/yyyy); it includes monthly data and quarterly data, the latter used carryforward hard coding (each quarter represented the beginning of the quarter) to cover each month.  Some FRED sets included four Census region subsets; these were merged on the date variable (mm/yyyy) and the property state variable, the latter was mapped to the four Census regions of Northeast, Midwest, South, and West.  Values were converted to quarterly and yearly deltas (e.g., the change in housing vacancies from 2006 Q4 to 2007 Q1 or 2006 Q1 to 2007 Q1).\n",
    "\n",
    "> Federal Deposit Insurance Corporation (FDIC) data includes information on FDIC-backed banks, such as their number of employees, assets, debts, etc.  I used regular expressions to map FDIC data to the Bank variable; this included summing various instances of the same bank (from a different branch or functional area).  This data is merged on the bank variable and the date variable (mm/yyyy); it includes quarterly data, which used carrybackward hard coding (each quarter represented the end of the quarter) to cover each month.  Values were converted to quarterly and yearly deltas (e.g., the change in Bank of America liabilities from 2006 Q4 to 2007 Q1 or 2006 Q1 to 2007 Q1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge FRED on Monthly Data\n",
    "\n",
    "Carryforward hard coding if data is quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fred_on_month(df_mnth, merge_df = df, varname = '', quarter=False, pct_change=1):\n",
    "    # Split date var\n",
    "    df_mnth['Month'] = df_mnth['DATE'].apply(str).apply(lambda x: x.split('/')[0].strip()).apply(str)\n",
    "    df_mnth['Year'] = df_mnth['DATE'].apply(str).apply(lambda x: x.split('/')[1].strip()).apply(str)\n",
    "    df_mnth = df_mnth.drop(labels='DATE', axis=1)\n",
    "        \n",
    "    # Retrieve name of main column\n",
    "    var = df_mnth.columns[0]\n",
    "    # Period change\n",
    "    df_mnth[var] = df_mnth[var].pct_change(pct_change)\n",
    "    # Ensure correct dtype\n",
    "    df_mnth[var] = df_mnth[var].astype(float)\n",
    "    \n",
    "    # If quarterly data,\n",
    "    # Carry first month of quarter forward\n",
    "    if quarter:\n",
    "        for i in range(df_mnth.shape[0]):\n",
    "            if df_mnth.loc[i, 'Month']=='01':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '01'}, '02')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '01'}, '03')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "            elif df_mnth.loc[i, 'Month']=='04':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '04'}, '05')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '04'}, '06')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "            elif df_mnth.loc[i, 'Month']=='07':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '07'}, '08')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '07'}, '09')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "            elif df_mnth.loc[i, 'Month']=='10':\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '10'}, '11')\n",
    "                df_mnth = df_mnth.append(new_row)\n",
    "                new_row = df_mnth.iloc[i,:].replace({'Month': '10'}, '12')\n",
    "                df_mnth = df_mnth.append(new_row)     \n",
    "    \n",
    "    # Create merge var\n",
    "    df_mnth['Original Date'] = (df_mnth['Month'].map(str) + '/' + df_mnth['Year']).apply(str)\n",
    "    df_mnth = df_mnth.drop(labels=['Year', 'Month'], axis=1)\n",
    "    df_mnth = df_mnth.rename(columns={var: varname})\n",
    "    \n",
    "    # Merge\n",
    "    merge_df = pd.merge(merge_df, df_mnth, on='Original Date', how='inner')\n",
    "    \n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State to Region Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_region(df_new, var, state_drop=False):\n",
    "    # Region-State crosswalk\n",
    "    Northeast = ['ME','VT','NH','MA','NY','RI','CT','PA','NJ']\n",
    "    South = ['DE','MD','DC','WV','VA','KY','NC','TN','SC','GA','FL','AL','MS','AR','LA','OK','TX']\n",
    "    Midwest = ['ND','SD','NE','KS','MN','IA','MO','WI','IL','MI','IN','OH']\n",
    "    West = ['WA','OR','ID','MT','WY','CA','NV','UT','AZ','CO','NM','AK','HI']\n",
    "    \n",
    "    # Replace States with Census regions\n",
    "    df_new['Region'] = df_new[var]\n",
    "    df_new['Region'] = df_new['Region'].replace(Northeast, 'Northeast')\n",
    "    df_new['Region'] = df_new['Region'].replace(South, 'South')\n",
    "    df_new['Region'] = df_new['Region'].replace(Midwest, 'Midwest')\n",
    "    df_new['Region'] = df_new['Region'].replace(West, 'West')\n",
    "    \n",
    "    # Drop State var\n",
    "    if state_drop:\n",
    "        df_new = df_new.drop(labels=var, axis=1)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge regional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_merge(NE, SO, MW, WE, varname, df_orig = df, quarter=False, pct_change=1):\n",
    "    # Northeast\n",
    "    Northeast = df_orig[df_orig['Region']=='Northeast']\n",
    "    Northeast = Northeast[['Loan ID', 'Region', 'Original Date']]\n",
    "    Northeast = merge_fred_on_month(df_mnth = NE, merge_df = Northeast, \n",
    "                                    varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    Northeast = Northeast.rename(columns={Northeast.columns[3]: varname})\n",
    "    \n",
    "    # South\n",
    "    South = df_orig[df_orig['Region']=='South']\n",
    "    South = South[['Loan ID', 'Region', 'Original Date']]\n",
    "    South = merge_fred_on_month(df_mnth = SO, merge_df = South, \n",
    "                                    varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    South = South.rename(columns={South.columns[3]: varname})\n",
    "    \n",
    "    # Midwest\n",
    "    Midwest = df_orig[df_orig['Region']=='Midwest']\n",
    "    Midwest = Midwest[['Loan ID', 'Region', 'Original Date']]\n",
    "    Midwest = merge_fred_on_month(df_mnth = MW, merge_df = Midwest, \n",
    "                                  varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    Midwest = Midwest.rename(columns={Midwest.columns[3]: varname})\n",
    "    \n",
    "    # West\n",
    "    West = df_orig[df_orig['Region']=='West']\n",
    "    West = West[['Loan ID', 'Region', 'Original Date']]\n",
    "    West = merge_fred_on_month(df_mnth = WE, merge_df = West, \n",
    "                               varname=varname, quarter=quarter, pct_change=pct_change)\n",
    "    West = West.rename(columns={West.columns[3]: varname})\n",
    "    \n",
    "    # Stack\n",
    "    df_region = pd.concat([Northeast, South, Midwest, West])\n",
    "    df_region = df_region[['Loan ID', varname]]\n",
    "    \n",
    "    # Merge\n",
    "    df_new = pd.merge(df_orig, df_region, on='Loan ID', how='inner')\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRED Data merge wrapper (full US) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fred_merge(fred_df, df_orig = df, quarter=True, varname = ''):\n",
    "    # Define units within year\n",
    "    if quarter:\n",
    "        pct_qtr = 1\n",
    "        pct_year = 4\n",
    "    else:\n",
    "        pct_qtr = 4\n",
    "        pct_year = 12    \n",
    "    # Define variable name, if not set\n",
    "    if varname=='':\n",
    "        varname = str(fred_df)\n",
    "    \n",
    "    # convert datetime\n",
    "    fred_df['DATE'] = pd.to_datetime(fred_df['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    \n",
    "    # merge FRED data and convert to percent change\n",
    "    df_new = merge_fred_on_month(fred_df, df_orig, varname, quarter=quarter, pct_change=pct_qtr)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Qtr)')})\n",
    "    df_new = merge_fred_on_month(fred_df, df_new, varname, quarter=quarter, pct_change=pct_year)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Yr)')})\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRED merge wrapper (region) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fred_merge_region(NE, SO, MW, WE, df_orig = df, varname = '', quarter=True):\n",
    "    # Define units within year\n",
    "    if quarter:\n",
    "        pct_qtr = 1\n",
    "        pct_year = 4\n",
    "    else:\n",
    "        pct_qtr = 4\n",
    "        pct_year = 12    \n",
    "    # Define variable name, if not set\n",
    "    if varname=='':\n",
    "        varname = str(fred_df)\n",
    "    \n",
    "    # convert datetime\n",
    "    NE['DATE'] = pd.to_datetime(NE['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    SO['DATE'] = pd.to_datetime(SO['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    MW['DATE'] = pd.to_datetime(MW['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "    WE['DATE'] = pd.to_datetime(WE['DATE']).dt.strftime('%m/%Y').apply(str)\n",
    "\n",
    "    # merge FRED data and convert to percent change\n",
    "    df_new = region_merge(NE=NE, SO=SO, MW=MW, WE=WE, df_orig = df_orig, \n",
    "                          varname=varname, quarter=quarter, pct_change=pct_qtr)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Qtr)')})\n",
    "    df_new = region_merge(NE=NE, SO=SO, MW=MW, WE=WE, df_orig = df_new, \n",
    "                          varname=varname, quarter=quarter, pct_change=pct_year)\n",
    "    df_new = df_new.rename(columns={varname: str(varname + ' (Yr)')})\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize bank names of FDIC data to Fannie Mae data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_bank_groupings(df, Bank = 'Bank'):\n",
    "    # Map similar names to bank\n",
    "    ## Group Bank of America\n",
    "    BoA = df[Bank].str.contains('Bank of America', case = False)\n",
    "    df.loc[BoA, 'Bank'] = 'Bank of America'\n",
    "    ## Group Citi Mortgage\n",
    "    Citi = df[Bank].str.contains('Citibank|Citicorp|CitiMortgage', case = False)\n",
    "    df.loc[Citi, 'Bank'] = 'CitiMortgage'\n",
    "    ## Group GMac\n",
    "    GMac = df[Bank].str.contains('GMAC', case = False)\n",
    "    df.loc[GMac, 'Bank'] = 'GMAC Mortgage'\n",
    "    ## Group PNC\n",
    "    PNC = df[Bank].str.contains('PNC Bank', case = False)\n",
    "    df.loc[PNC, 'Bank'] = 'PNC Bank'\n",
    "    ## Group SunTrust\n",
    "    SunTrust = df[Bank].str.contains('SunTrust', case = False)\n",
    "    df.loc[SunTrust, 'Bank'] = 'SunTrust Mortgage'\n",
    "    ## Group AmTrust\n",
    "    AmTrust = df[Bank].str.contains('AmTrust', case = False)\n",
    "    df.loc[AmTrust, 'Bank'] = 'AmTrust Bank'\n",
    "    ## Group Flagstar\n",
    "    Flagstar = df[Bank].str.contains('Flagstar', case = False)\n",
    "    df.loc[Flagstar, 'Bank'] = 'Flagstar Bank'\n",
    "    ## Group Chase\n",
    "    Chase = df[Bank].str.contains('Chase|JP Morgan|J. P. Morgan|JPMorgan', case = False)\n",
    "    df.loc[Chase, 'Bank'] = 'JPMorgan Chase'\n",
    "    ## Group Wells Fargo\n",
    "    Wells = df[Bank].str.contains('Wells Fargo', case = False)\n",
    "    df.loc[Wells, 'Bank'] = 'Wells Fargo Bank'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert FDIC Data to Monthly Data\n",
    "\n",
    "Carrybackwards hard coding to convert quarterly data to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdic_on_month(df_mnth):\n",
    "    # Retrieve names of columns\n",
    "    cols = df_mnth.columns\n",
    "    \n",
    "    # convert datetime\n",
    "    df_mnth['repdte'] = pd.to_datetime(df_mnth['repdte']).dt.strftime('%m/%Y').apply(str)\n",
    "    \n",
    "    # Split date var\n",
    "    df_mnth['Month'] = df_mnth['repdte'].apply(str).apply(lambda x: x.split('/')[0].strip()).apply(str)\n",
    "    df_mnth['Year'] = df_mnth['repdte'].apply(str).apply(lambda x: x.split('/')[1].strip()).apply(str)\n",
    "    \n",
    "    # Carry first month of quarter forward\n",
    "    for i in range(df_mnth.shape[0]):\n",
    "        if df_mnth.loc[i, 'Month']=='03':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '03'}, '01')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '03'}, '02')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "        elif df_mnth.loc[i, 'Month']=='06':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '06'}, '04')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '06'}, '05')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "        elif df_mnth.loc[i, 'Month']=='09':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '09'}, '07')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '09'}, '08')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "        elif df_mnth.loc[i, 'Month']=='12':\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '12'}, '10')\n",
    "            df_mnth = df_mnth.append(new_row)\n",
    "            new_row = df_mnth.iloc[i,:].replace({'Month': '12'}, '11')\n",
    "            df_mnth = df_mnth.append(new_row)          \n",
    "        \n",
    "    # Create merge var\n",
    "    df_mnth['Original Date'] = (df_mnth['Month'].map(str) + '/' + df_mnth['Year']).apply(str)\n",
    "    df_mnth = df_mnth.drop(labels=['Month', 'Year', 'repdte'], axis=1)\n",
    "    \n",
    "    return df_mnth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of features with really high missingness or no data variation, and then mean/mode hard coding on features with low missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_treat(df):\n",
    "    # Find features with 10% missing or more\n",
    "    condition = ( df.isnull().sum(axis=0)/df.shape[0]*100 ) \n",
    "    df_HighMissing = condition > 10 \n",
    "    \n",
    "    # Save features that contain missing data\n",
    "    df_HighMissing = df_HighMissing.index[df_HighMissing.values == True]\n",
    "    \n",
    "    # remove high missing features\n",
    "    df = df.drop(labels=df_HighMissing, axis=1)\n",
    "        \n",
    "    # impute on the mean for low missing features that are continuous   \n",
    "    df_cont = df.select_dtypes(include=['float64', 'int64'])\n",
    "    df[df_cont.columns] = df_cont.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    \n",
    "    # impute on the mode for low missing features that are categorical   \n",
    "    df_cat = df.select_dtypes(include=['object'])\n",
    "    df[df_cat.columns] = df_cat.apply(lambda x: x.fillna(x.mode()),axis=0)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_imputations(df_X, df_y, X_cols, n_neighbors=3):\n",
    "    # Subset columns and add target variable\n",
    "    df_imp = df_X[X_cols]\n",
    "    df_imp = pd.concat([df_X, df_y], axis=1)\n",
    "    \n",
    "    # Run imputations\n",
    "    KNN_impute = KNNImputer(n_neighbors=n_neighbors, weights=\"distance\")\n",
    "    df_imp = KNN_impute.fit_transform(df_imp)\n",
    "    df_imp = pd.DataFrame(df_imp, columns=[X_cols, 'Target'])\n",
    "    \n",
    "    # Drop target variable\n",
    "    df_imp = df_imp.drop(labels='Target', axis=1)\n",
    "    # Drop non-imputed data\n",
    "    df_X = df_X.drop(labels=X_cols, axis=1)\n",
    "    \n",
    "    # Merge to full data\n",
    "    df_X = pd.concat([df_X, df_imp], axis=1)\n",
    "    \n",
    "    return df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing date features to numeric, if one decides to use time as a ordinal feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_date(df, var_str):\n",
    "    \n",
    "    # Convert to ordinal\n",
    "    df[var_str] = df[var_str].apply(lambda x: dt.strptime(x, '%m/%Y').toordinal())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding\n",
    "\n",
    "Converts categorical variables to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(df):\n",
    "    columns = df.columns[df.isnull().any()]\n",
    "    nan_cols = df[columns]\n",
    "\n",
    "    df = df.drop(nan_cols.columns, axis=1)\n",
    "\n",
    "    df_cat = df.select_dtypes(include=['object'])\n",
    "    onehot = pd.get_dummies(df_cat)\n",
    "    \n",
    "    df_cont = df.drop(df_cat.columns, axis=1)\n",
    "\n",
    "    df = pd.concat([df_cont,onehot,nan_cols], axis=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension Reduction on Macroeconomic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fred(X_train=X_train, X_val=X_val, X_test=X_test, n_components=5):\n",
    "    # FRED Subset\n",
    "    columns = ['Household Financial Obligations (Qtr)', 'Household Financial Obligations (Yr)', \n",
    "         'Consumer Debt Service Payment (Qtr)', 'Consumer Debt Service Payment (Yr)',\n",
    "         'National Home Price Index (Qtr)', 'National Home Price Index (Yr)',\n",
    "         'Mortgage Debt Service Payments (Qtr)', 'Mortgage Debt Service Payments (Yr)',\n",
    "         'Monthly Supply of Houses (Qtr)', 'Monthly Supply of Houses (Yr)',\n",
    "         'Vacant Housing Units for Sale (Qtr)', 'Vacant Housing Units for Sale (Yr)',\n",
    "         'Homeownership Rate (Qtr)', 'Homeownership Rate (Yr)', 'Vacant Housing Units for Rent (Qtr)',\n",
    "         'Vacant Housing Units for Rent (Yr)', 'Rental Vacancy Rate (Qtr)', 'Rental Vacancy Rate (Yr)']\n",
    "    fred_train =  X_train[columns]\n",
    "    fred_val = X_val[columns]\n",
    "    fred_test = X_test[columns]\n",
    "    \n",
    "    # Fit PCA\n",
    "    dimredu = PCA(n_components=n_components, random_state=2020).fit(fred_train)\n",
    "    fred_train = pd.DataFrame(dimredu.transform(fred_train), columns=['Macroeconomy PCA 1',\n",
    "                                                                      'Macroeconomy PCA 2',\n",
    "                                                                      'Macroeconomy PCA 3',\n",
    "                                                                      'Macroeconomy PCA 4',\n",
    "                                                                      'Macroeconomy PCA 5'])\n",
    "    fred_val = pd.DataFrame(dimredu.transform(fred_val), columns=['Macroeconomy PCA 1',\n",
    "                                                                    'Macroeconomy PCA 2',\n",
    "                                                                    'Macroeconomy PCA 3',\n",
    "                                                                    'Macroeconomy PCA 4',\n",
    "                                                                    'Macroeconomy PCA 5'])\n",
    "    fred_test = pd.DataFrame(dimredu.transform(fred_test), columns=['Macroeconomy PCA 1',\n",
    "                                                                    'Macroeconomy PCA 2',\n",
    "                                                                    'Macroeconomy PCA 3',\n",
    "                                                                    'Macroeconomy PCA 4',\n",
    "                                                                    'Macroeconomy PCA 5'])    \n",
    "    # Subsitute PCA columns\n",
    "    X_train = X_train.drop(labels=columns, axis=1)\n",
    "    X_train = pd.concat([X_train, fred_train], axis=1)\n",
    "    X_val = X_val.drop(labels=columns, axis=1)\n",
    "    X_val = pd.concat([X_val, fred_val], axis=1)\n",
    "    X_test = X_test.drop(labels=columns, axis=1)\n",
    "    X_test = pd.concat([X_test, fred_test], axis=1)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "\n",
    "Run permutation importance and score based on ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_importance(X_train, y_train, bank_str, method='bal', max_features='sqrt'):\n",
    "    # Transform X\n",
    "    ## define datasets \n",
    "    y = y_train  \n",
    "    X = X_train\n",
    "    \n",
    "    ## Standardize Vars\n",
    "    X_cols = X.columns\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    # Permutation importance for feature evaluation\n",
    "    if method=='bal':\n",
    "        clf = BalancedRandomForestClassifier(n_estimators=50, random_state=2020, max_features=max_features, \n",
    "                                             replacement=False, n_jobs=-1)\n",
    "    elif method=='wgt':\n",
    "        clf = RandomForestClassifier(n_estimators=50, random_state=2020, max_features=max_features, \n",
    "                                     class_weight={1: 0.08, 0: 0.02}, n_jobs=-1)     \n",
    "    elif method=='rus':\n",
    "        clf = RUSBoostClassifier(n_estimators=10, random_state=2020)\n",
    "    \n",
    "    clf = clf.fit(X, y)\n",
    "    result = permutation_importance(clf, X, y, n_repeats=5, scoring='f1',\n",
    "                                    random_state=2020)\n",
    "    \n",
    "    # Save results\n",
    "    importances = pd.Series(result.importances_mean, index=X_cols)\n",
    "    \n",
    "    # Graph\n",
    "    sorted_idx = importances.argsort()\n",
    "    y_ticks = np.arange(0, 15)\n",
    "    fig, ax = plt.subplots()\n",
    "    if importances.size > 15:\n",
    "        ax.barh(y_ticks, importances[sorted_idx].iloc[-15:])\n",
    "        ax.set_yticklabels(importances[sorted_idx].index[-15:])\n",
    "    elif importances.size > 10:\n",
    "        ax.barh(y_ticks, importances[sorted_idx].iloc[-10:])\n",
    "        ax.set_yticklabels(importances[sorted_idx].index[-10:])\n",
    "    elif importances.size > 7:\n",
    "        ax.barh(y_ticks, importances[sorted_idx].iloc[-7:])\n",
    "        ax.set_yticklabels(importances[sorted_idx].index[-7:])        \n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_title(str('Feature Importances for\\n' + bank_str))\n",
    "    fig.tight_layout()\n",
    "    print(plt.show())\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "\n",
    "Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFE_importance(X_train, y_train, bank_str, method='bal', max_features='sqrt'):\n",
    "    # Transform X\n",
    "    ## define datasets \n",
    "    y = y_train  \n",
    "    X = X_train\n",
    "    \n",
    "    ## Standardize Vars\n",
    "    X_cols = X.columns\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    # Permutation importance for feature evaluation\n",
    "    if method=='bal':\n",
    "        clf = BalancedRandomForestClassifier(n_estimators=200, random_state=2020, max_features=max_features, \n",
    "                                             replacement=False, n_jobs=-1)\n",
    "    elif method=='wgt':\n",
    "        clf = RandomForestClassifier(n_estimators=200, random_state=2020, max_features=max_features, \n",
    "                                     class_weight={1: 0.08, 0: 0.02}, n_jobs=-1)     \n",
    "    elif method=='rus':\n",
    "        clf = RUSBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=2020)\n",
    "    \n",
    "    # Cross-Validate RFE\n",
    "    result = RFECV(clf, min_features_to_select=7, step=1, cv=StratifiedKFold(2), \n",
    "                   scoring='f1', n_jobs=-1)\n",
    "    # Fit Model\n",
    "    result.fit(X, y)\n",
    "    # Preview Results\n",
    "    print(\"Optimal number of features : %d\" % result.n_features_)\n",
    "    print(\"Best F1 Score: {:.2f}\".format(max(result.grid_scores_)))\n",
    "    \n",
    "    # Save results\n",
    "    importances = pd.Series(result.ranking_, index=X_cols)\n",
    "    sorted_idx = importances.argsort()\n",
    "    df = pd.DataFrame(importances[sorted_idx], columns=['Ranking'])\n",
    "    subset = result.support_\n",
    "    \n",
    "    # Graph\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.plot(range(1, len(result.grid_scores_) + 1), result.grid_scores_)\n",
    "    print(plt.show())\n",
    "\n",
    "    return df, subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vote by Committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_by_committee(X_train, y_train, bank_str, method='bal', max_features='sqrt'):\n",
    "    # Transform X\n",
    "    ## define datasets \n",
    "    y = y_train  \n",
    "    X = X_train\n",
    "    \n",
    "    ## Standardize Vars\n",
    "    X_cols = X.columns\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    # Permutation importance for feature evaluation\n",
    "    if method=='bal':\n",
    "        clf = BalancedRandomForestClassifier(n_estimators=50, random_state=2020, max_features=max_features, \n",
    "                                             replacement=False, n_jobs=-1)\n",
    "    elif method=='wgt':\n",
    "        clf = RandomForestClassifier(n_estimators=50, random_state=2020, max_features=max_features, \n",
    "                                     class_weight={1: 0.08, 0: 0.02}, n_jobs=-1)     \n",
    "    elif method=='rus':\n",
    "        clf = RUSBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=2020)\n",
    "    \n",
    "    # Cross-Validate RFE\n",
    "    result = RFECV(clf, min_features_to_select=7, step=1, cv=StratifiedKFold(2), \n",
    "                   scoring='f1', n_jobs=-1)\n",
    "    # Fit Model\n",
    "    result.fit(X, y)\n",
    "    # Preview Results\n",
    "    print(\"Optimal number of features : %d\" % result.n_features_)\n",
    "    print(\"Best F1 Score: {:.2f}\".format(max(result.grid_scores_)))\n",
    "    \n",
    "    # Save results\n",
    "    importances = pd.Series(result.ranking_, index=X_cols)\n",
    "    sorted_idx = importances.argsort()\n",
    "    df = pd.DataFrame(importances[sorted_idx], columns=['Ranking'])\n",
    "    subset = result.support_\n",
    "    \n",
    "    # Graph\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.plot(range(1, len(result.grid_scores_) + 1), result.grid_scores_)\n",
    "    print(plt.show())\n",
    "\n",
    "    return df, subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foreclosure Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Overall_Data(YrQtr = \"\", subset = \"\", df = df, allvars = False, quarter = False):\n",
    "    # Subset by Date\n",
    "    if (YrQtr == \"\"):\n",
    "        df_sub = pd.DataFrame(df)\n",
    "    else:\n",
    "        yr = int(str(YrQtr)[:4])\n",
    "        df_sub = pd.DataFrame(df)\n",
    "        df_sub = df_sub.loc[df_sub['File Year']==yr,:]\n",
    "        if quarter:\n",
    "            qtr = str(YrQtr)[4:6]\n",
    "            df_sub = df_sub.loc[df_sub['File Quarter']==qtr,:]\n",
    "            \n",
    "    # Subset by other variable\n",
    "    if (subset != \"\"):\n",
    "        df_sub = df_sub.loc[eval(subset),:]\n",
    "        \n",
    "    # Foreclosures represented\n",
    "    Foreclosed = ['Did not Foreclose', 'Foreclosed']\n",
    "    Target = df_sub.groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "    if allvars:\n",
    "        # Original Mortgage Amount\n",
    "        ORM = df_sub.groupby(['Foreclosed']).agg({'Original Mortgage Amount': 'mean'}).round(2)\n",
    "        # Credit Score\n",
    "        CS = df_sub.groupby(['Foreclosed']).agg({'Credit Score': 'mean'}).astype(int)\n",
    "        # Original Debt to Income Ratio\n",
    "        DTI = df_sub.groupby(['Foreclosed']).agg({'Original Debt to Income Ratio': 'mean'}).round(1)\n",
    "        # First Time Home Buyer\n",
    "        FT = df_sub[df_sub['First Time Home Buyer']=='Y'].groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "        # Refinance\n",
    "        LP = df_sub[df_sub['Loan Purpose']!=0].groupby(['Foreclosed']).size().reset_index(name='Total')\n",
    "        # Original Interest Rate\n",
    "        IR = df_sub.groupby(['Foreclosed']).agg({'Original Interest Rate': 'mean'}).round(2)\n",
    "        # Original Loan Term\n",
    "        LT = df_sub.groupby(['Foreclosed']).agg({'Original Loan Term': 'mean'}).astype(int)\n",
    "        # Original Combined Loan-to-Value (CLTV)\n",
    "        CLTV = df_sub.groupby(['Foreclosed']).agg({'Original Combined Loan-to-Value (CLTV)': 'mean'}).round(1)\n",
    "        # Single Borrower Ratio\n",
    "        SBR = df_sub.groupby(['Foreclosed']).agg({'Single Borrower': 'mean'}).round(2)\n",
    "        # Mortgage Insurance %\n",
    "        MIP = df_sub.groupby(['Foreclosed']).agg({'Mortgage Insurance %': 'mean'}).round(2)    \n",
    "        # Median Household Income\n",
    "        MHI = df_sub.groupby(['Foreclosed']).agg({'Median Household Income': 'mean'}).round(2)\n",
    "        # Loan Change (1 Year)\n",
    "        LC1 = df_sub.groupby(['Foreclosed']).agg({'Loan Change (1 Year)': 'mean'}).round(2)\n",
    "        # Loan Change (5 Years)\n",
    "        LC5 = df_sub.groupby(['Foreclosed']).agg({'Loan Change (5 Years)': 'mean'}).round(2)\n",
    "        # Lnlsnet (1 Yr)\n",
    "        LNL1 = df_sub.groupby(['Foreclosed']).agg({'Lnlsnet (1 Yr)': 'mean'}).round(2)\n",
    "        # Lnlsnet (5 Yr)\n",
    "        LNL5 = df_sub.groupby(['Foreclosed']).agg({'Lnlsnet (5 Yr)': 'mean'}).round(2)        \n",
    "    \n",
    "    # Create Dataset\n",
    "    df_new = pd.DataFrame({ 'Foreclosed': Foreclosed, \n",
    "                            'Foreclosed (%)': ((Target['Total'] / df_sub.shape[0]) * 100).round(1),\n",
    "                            'Foreclosed (N)': df_sub.groupby(['Foreclosed']).size()\n",
    "                        })\n",
    "    \n",
    "    if allvars:\n",
    "        df_all = pd.DataFrame({ 'Mortgage Amount ($)': ORM['Original Mortgage Amount'].tolist(),\n",
    "                                'Credit Score': CS['Credit Score'].tolist(),\n",
    "                                'Original Debt to Income Ratio': DTI['Original Debt to Income Ratio'].tolist(),\n",
    "                                'First Time Home Buyer (%)': ((FT['Total'] / Target['Total']) * 100).round(1).tolist(),\n",
    "                                'Refinanced': ((LP['Total'] / Target['Total']) * 100).round(1).tolist(),\n",
    "                                'Interest Rate': IR['Original Interest Rate'].tolist(),\n",
    "                                'Loan Term': LT['Original Loan Term'].tolist(),\n",
    "                                'Original Combined Loan-to-Value (CLTV)': CLTV['Original Combined Loan-to-Value (CLTV)'].tolist(),\n",
    "                                'Single Borrower Ratio': SBR['Single Borrower'].tolist(),\n",
    "                                'Mortgage Insurance %': MIP['Mortgage Insurance %'].tolist(),\n",
    "                                'Median Household Income': MHI['Median Household Income'].tolist(),\n",
    "                                'Loan Change (1 Year)': LC1['Loan Change (1 Year)'].tolist(),\n",
    "                                'Loan Change (5 Years)': LC5['Loan Change (5 Years)'].tolist(),\n",
    "                                'Lnlsnet (1 Yr)': LNL1['Lnlsnet (1 Yr)'].tolist(),\n",
    "                                'Lnlsnet (5 Yr)': LNL5['Lnlsnet (5 Yr)'].tolist()\n",
    "                         })\n",
    "        df_new = pd.concat([df_new, df_all], axis=1)\n",
    "\n",
    "    df_new = df_new.set_index('Foreclosed')\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bank Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bank_Data(YrQtr = \"\", subset = \"\", df = df, allvars = False, quarter = False, rounding = 1):\n",
    "    # Subset by Date\n",
    "    if (YrQtr == \"\"):\n",
    "        df_sub = pd.DataFrame(df)\n",
    "    else:\n",
    "        yr = int(str(YrQtr)[:4])\n",
    "        df_sub = pd.DataFrame(df)\n",
    "        df_sub = df_sub.loc[df_sub['File Year']==yr,:]\n",
    "        if quarter:\n",
    "            qtr = str(YrQtr)[4:6]\n",
    "            df_sub = df_sub.loc[df_sub['File Quarter']==qtr,:]\n",
    "            \n",
    "    # Subset by other variable\n",
    "    if (subset != \"\"):\n",
    "        df_sub = df_sub.loc[eval(subset),:]\n",
    "        \n",
    "    # Banks represented\n",
    "    Banks = df_sub.groupby(['Bank']).size().reset_index(name='Total')\n",
    "    # Foreclosures\n",
    "    Target = df_sub.groupby(['Bank']).agg({'Foreclosed': 'mean'})\n",
    "    if allvars:\n",
    "        # Original Mortgage Amount\n",
    "        ORM = df_sub.groupby(['Bank']).agg({'Original Mortgage Amount': 'mean'}).round(2)\n",
    "        # Credit Score\n",
    "        CS = df_sub.groupby(['Bank']).agg({'Credit Score': 'mean'}).astype(int)\n",
    "        # Original Debt to Income Ratio\n",
    "        DTI = df_sub.groupby(['Bank']).agg({'Original Debt to Income Ratio': 'mean'}).round(1)\n",
    "        # First Time Home Buyer\n",
    "        FT = df_sub[df_sub['First Time Home Buyer']=='Y'].groupby(['Bank']).size().reset_index(name='Total')\n",
    "        # Refinance\n",
    "        LP = df_sub[df_sub['Loan Purpose']!=0].groupby(['Bank']).size().reset_index(name='Total')\n",
    "        # Original Interest Rate\n",
    "        IR = df_sub.groupby(['Bank']).agg({'Original Interest Rate': 'mean'}).round(2)\n",
    "        # Original Loan Term\n",
    "        LT = df_sub.groupby(['Bank']).agg({'Original Loan Term': 'mean'}).astype(int)\n",
    "        # Original Combined Loan-to-Value (CLTV)\n",
    "        CLTV = df_sub.groupby(['Bank']).agg({'Original Combined Loan-to-Value (CLTV)': 'mean'}).round(1)\n",
    "        # Single Borrower Ratio\n",
    "        SBR = df_sub.groupby(['Bank']).agg({'Single Borrower': 'mean'}).round(2)\n",
    "        # Mortgage Insurance %\n",
    "        MIP = df_sub.groupby(['Bank']).agg({'Mortgage Insurance %': 'mean'}).round(2)     \n",
    "        # Median Household Income\n",
    "        MHI = df_sub.groupby(['Bank']).agg({'Median Household Income': 'mean'}).round(2)\n",
    "        # Loan Change (1 Year)\n",
    "        LC1 = df_sub.groupby(['Bank']).agg({'Loan Change (1 Year)': 'mean'}).round(2)\n",
    "        # Loan Change (5 Years)\n",
    "        LC5 = df_sub.groupby(['Bank']).agg({'Loan Change (5 Years)': 'mean'}).round(2)\n",
    "        # Lnlsnet (1 Yr)\n",
    "        LNL1 = df_sub.groupby(['Bank']).agg({'Lnlsnet (1 Yr)': 'mean'}).round(2)\n",
    "        # Lnlsnet (5 Yr)\n",
    "        LNL5 = df_sub.groupby(['Bank']).agg({'Lnlsnet (5 Yr)': 'mean'}).round(2)\n",
    "        \n",
    "    # Create Dataset\n",
    "    df_new = pd.DataFrame({ 'Bank': Banks['Bank'], \n",
    "                            'Bank (%)': ((Banks['Total'] / df_sub.shape[0]) * 100).round(1),\n",
    "                            'Bank (N)': Banks['Total'],\n",
    "                            'Foreclosed (%)': ((Target['Foreclosed'] * 100).round(rounding)).tolist(), \n",
    "                        })\n",
    "    if allvars:\n",
    "        df_all = pd.DataFrame({ 'Mortgage Amount ($)': ORM['Original Mortgage Amount'].tolist(),\n",
    "                                'Credit Score': CS['Credit Score'].tolist(),\n",
    "                                'Original Debt to Income Ratio': DTI['Original Debt to Income Ratio'].tolist(),\n",
    "                                'First Time Home Buyer (%)': ((FT['Total'] / Banks['Total']) * 100).round(1).tolist(),\n",
    "                                'Refinance': ((LP['Total'] / Banks['Total']) * 100).round(1).tolist(),\n",
    "                                'Interest Rate': IR['Original Interest Rate'].tolist(),\n",
    "                                'Loan Term': LT['Original Loan Term'].tolist(),\n",
    "                                'Original Combined Loan-to-Value (CLTV)': CLTV['Original Combined Loan-to-Value (CLTV)'].tolist(),\n",
    "                                'Single Borrower Ratio': SBR['Single Borrower'].tolist(),\n",
    "                                'Mortgage Insurance %': MIP['Mortgage Insurance %'].tolist(),                           \n",
    "                                'Median Household Income': MHI['Median Household Income'].tolist(),\n",
    "                                'Loan Change (1 Year)': LC1['Loan Change (1 Year)'].tolist(),\n",
    "                                'Loan Change (5 Years)': LC5['Loan Change (5 Years)'].tolist(),\n",
    "                                'Lnlsnet (1 Yr)': LNL1['Lnlsnet (1 Yr)'].tolist(),\n",
    "                                'Lnlsnet (5 Yr)': LNL5['Lnlsnet (5 Yr)'].tolist()\n",
    "                             })\n",
    "        df_new = pd.concat([df_new, df_all], axis=1)\n",
    "                                \n",
    "    df_new = df_new.set_index(\"Bank\")\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate banks based on maximum, minimum, or other meaningful values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_Banks(col, df = Banks, func = max, subset = True):\n",
    "    # print(col, func.__name__, \"value\")\n",
    "    if (subset): cols = col\n",
    "    else: cols = df.columns\n",
    "    values = pd.DataFrame(df[cols][df[col] == func(df[col])])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density plot of a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_density(var, hist=True, bins=None, l_xlim=None, r_xlim=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "    sns.distplot(df.loc[df['Foreclosed']==1, var], color='#51c0ef', hist=hist, bins=bins)\n",
    "    sns.distplot(df.loc[df['Foreclosed']==0, var], color='#61ba86', hist=hist, bins=bins)\n",
    "    ax.legend(labels=['Foreclosed', 'Did not Foreclose'], loc='upper right')\n",
    "    ax.set_xlim(left=l_xlim, right=r_xlim)\n",
    "    plt.show()\n",
    "    \n",
    "    print(Overall_Data(df = df, allvars = True)[[var]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank features of each bank via bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_rank_gph(var, df, df_bank, b_ylim=None, t_ylim=None):\n",
    "    v = df[var].mean().astype(int)\n",
    "    avg = pd.DataFrame(data={var: v}, index=['All Banks'])\n",
    "    tbl = pd.concat([avg, df_bank.sort_values(by=[var])], axis=0) \n",
    "    display(tbl)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[6,7])\n",
    "    plt.rcParams['font.size'] = '10'\n",
    "    plt.bar(tbl.index, tbl.loc[:,var], color = '#457b9d')\n",
    "    plt.bar('All Banks', tbl.loc['All Banks',var], color = '#ca2c92')\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylim(bottom=b_ylim, top=t_ylim)\n",
    "    plt.title(var + '\\n2006 - 2008')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar graphs of the best and worst banks for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_worst_gph(var, df, func=[max, min], l_xlim=None, r_xlim=None):\n",
    "    best = search_Banks(var, df = df, func = func[0])\n",
    "    worst = search_Banks(var, df = df, func = func[1])\n",
    "    v = df[var].mean().astype(int)\n",
    "    avg = pd.DataFrame(data={var: v}, index=['Overall'])\n",
    "    label = pd.Series(['Worst Bank', 'Best Bank', 'Average'])\n",
    "    tbl = pd.concat([worst, best, avg], axis=0).reset_index()\n",
    "    tbl.columns = ['Bank', var]\n",
    "    tbl.index = label\n",
    "    display(tbl)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[9,3])\n",
    "    plt.barh(tbl.index, tbl.loc[:,var], color = '#457b9d')\n",
    "    plt.barh('Average', tbl.loc['Average',var], color = '#ca2c92')\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_xlim(left=l_xlim, right=r_xlim)\n",
    "    plt.title(var + '\\n2006 - 2008')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density plots of best and worst banks for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_worst_density(var, df, df_bank, func=[max, min], \n",
    "                       hist = True, bins = None, l_xlim = None, r_xlim = None):\n",
    "    best = search_Banks(var, df = df_bank, func = func[0]).reset_index()\n",
    "    worst = search_Banks(var, df = df_bank, func = func[1]).reset_index()\n",
    "    title = ['Best Actor', 'Worst Actor']\n",
    "    i = 0\n",
    "    for v in [best.loc[0,'Bank'], worst.loc[0,'Bank']]:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "        sns.distplot(df.loc[(df['Foreclosed']==1) & (df['Bank']==v), var], color='#51c0ef', hist=hist, bins=bins)\n",
    "        sns.distplot(df.loc[(df['Foreclosed']==0) & (df['Bank']==v), var], color='#61ba86', hist=hist, bins=bins)\n",
    "        plt.title(title[i] + '\\n' + v)\n",
    "        ax.set_xlim(left=l_xlim, right=r_xlim)\n",
    "        ax.legend(labels=['Foreclosed', 'Did not Foreclose'], loc='upper left')\n",
    "        plt.show()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Data Frames based on Bank in Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create single-bank only subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of banks for reference\n",
    "banks = ['Bank of America','Wells Fargo Bank','CitiMortgage',\n",
    "         'JPMorgan Chase','GMAC Mortgage','SunTrust Mortgage',\n",
    "         'AmTrust Bank','PNC Bank','Flagstar Bank']\n",
    "\n",
    "# Function to subset banking datasets\n",
    "def Bank_Subsets(bank_strs, df_X = X_train, df_y = y_train):\n",
    "    # Initiate Bank dictionaries\n",
    "    X = {}\n",
    "    y = {}\n",
    "\n",
    "    # Bank Subset\n",
    "    for bank_str in bank_strs:\n",
    "        X[bank_str] = df_X.loc[df_X[str('Bank_' + bank_str)]==1, :] \\\n",
    "            .filter(regex=r'^(?!Bank_).*$')\n",
    "        y[bank_str] = df_y[np.array(df_X[str('Bank_' + bank_str)]==1)]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update single-bank only subsets with predicted probability assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Frames with Updated Assumptions\n",
    "def pp_dfs(varlist, values, banks, \n",
    "           Banks_X, Banks_X_val, Banks_X_test,\n",
    "           Banks_y, Banks_y_val, Banks_y_test):\n",
    "    # Initiate Dictionaries\n",
    "    X = {}\n",
    "    y = {}\n",
    "    \n",
    "    for bank_str in banks:\n",
    "        # Bank-specific concatenation of train, val, and test data\n",
    "        X[bank_str] = pd.concat([Banks_X[bank_str], Banks_X_val[bank_str], Banks_X_test[bank_str]], axis=0)\n",
    "        y[bank_str] = pd.concat([Banks_y[bank_str], Banks_y_val[bank_str], Banks_y_test[bank_str]], axis=0)\n",
    "        # Bank-specific update prediction assumption\n",
    "        for i in range(len(varlist)):\n",
    "            X[bank_str][varlist[i]] = values[i]\n",
    "        \n",
    "    # All bank concatenation of train, val, and test data\n",
    "    X['All Banks'] = pd.concat([X_train, X_val, X_test], axis=0)\n",
    "    y['All Banks'] = pd.concat([y_train, y_val, y_test], axis=0)\n",
    "    # All bank update prediction assumption\n",
    "    for i in range(len(varlist)):\n",
    "        X['All Banks'][varlist[i]] = values[i]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Four Types of Models\n",
    "\n",
    "For the bottom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize Vars\n",
    "sclr = StandardScaler()\n",
    "    \n",
    "## RFC\n",
    "## 5:1 balance\n",
    "## Subset columns to 20%\n",
    "rfc1_m = BalancedRandomForestClassifier(random_state=2020, \n",
    "                                        max_features='sqrt', criterion='entropy', \n",
    "                                        sampling_strategy={0:1500, 1:300},\n",
    "                                        replacement=False, n_jobs=-1)\n",
    "rfc1 = Pipeline(steps=[('sclr', sclr), ('rfc1', rfc1_m)])\n",
    "\n",
    "## RFC\n",
    "## PCA reduction to 10 columns\n",
    "## Fully balanced\n",
    "rfc2_m = BalancedRandomForestClassifier(random_state=2022, \n",
    "                                        max_features=None, criterion='entropy', \n",
    "                                        sampling_strategy='auto', \n",
    "                                        replacement=True, n_jobs=-1)\n",
    "rfc2 = Pipeline(steps=[('sclr', sclr), ('pca', PCA()), ('rfc2', rfc2_m)])\n",
    "\n",
    "## RUS Boost\n",
    "## 3:1 balance\n",
    "rus_m = RUSBoostClassifier(n_estimators=500, random_state=2023, \n",
    "                          sampling_strategy={0:900, 1:300}, \n",
    "                          replacement=False)\n",
    "rus = Pipeline(steps=[('sclr', sclr), ('rus', rus_m)])\n",
    "\n",
    "## Keras\n",
    "## 5 Layer model\n",
    "## Fully balanced\n",
    "def make_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_shape=(n_features,),\n",
    "              kernel_initializer='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(250, kernel_initializer='glorot_normal', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    #model.add(Dense(500, kernel_initializer='glorot_normal', use_bias=False))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.15))\n",
    "    model.add(Dense(100, kernel_initializer='glorot_normal', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_predict_balanced_model(X, y):\n",
    "    model = make_model(X.shape[1])\n",
    "    training_generator = BalancedBatchGenerator(X, y,\n",
    "                                                batch_size=2000,\n",
    "                                                random_state=2024)\n",
    "    model.fit_generator(generator=training_generator, epochs=1000, verbose=0)\n",
    "    return model\n",
    "\n",
    "keras = KerasClassifier(build_fn=fit_predict_balanced_model, verbose=0)\n",
    "keras = Pipeline(steps=[('sclr', sclr), ('pca', PCA(n_components=10)), ('keras', keras)])\n",
    "\n",
    "## Create an environment variable to avoid using the GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for bottom layer models\n",
    "\n",
    "Runs on individual bank models and all banks model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for bottom layer models\n",
    "def bottom_layer_func(bank_str, estimator, clf_str,\n",
    "                      Banks_y, Banks_X, \n",
    "                      Banks_y_val, Banks_X_val,\n",
    "                      Banks_y_test, Banks_X_test):    \n",
    "    print('==============================')\n",
    "    print('==============================')\n",
    "    print(bank_str, 'Model')\n",
    "    print(' - ', estimator) \n",
    "    \n",
    "    # Load Bank Data\n",
    "    y_train = Banks_y[bank_str]\n",
    "    X_train = Banks_X[bank_str] \n",
    "    y_val = Banks_y_val[bank_str]\n",
    "    X_val = Banks_X_val[bank_str]\n",
    "    y_test = Banks_y_test[bank_str]\n",
    "    X_test = Banks_X_test[bank_str]\n",
    "    \n",
    "    # Param tuning\n",
    "    rfc1_param_grid = {\n",
    "        'rfc1__n_estimators': [50, 75],\n",
    "        'rfc1__min_samples_split': [2, 4, 6]\n",
    "    }\n",
    "    rfc2_param_grid = {\n",
    "        'pca__n_components': [10, 15],\n",
    "        'rfc2__n_estimators': [500, 750],\n",
    "        'rfc2__min_samples_split': [10, 13, 16]\n",
    "    }    \n",
    "    rus_param_grid = {\n",
    "        'rus__learning_rate': [0.05, 0.1, 0.15, 0.2]\n",
    "    }\n",
    "\n",
    "    # Set for classifier\n",
    "    if clf_str == 'RFC':\n",
    "        param_grid=rfc1_param_grid\n",
    "    elif clf_str == 'RFC PCA':\n",
    "        param_grid=rfc2_param_grid      \n",
    "    elif clf_str == 'RUS Boost':\n",
    "        param_grid=rus_param_grid      \n",
    "    else:\n",
    "        param_grid={}\n",
    "    \n",
    "    # Model\n",
    "    if clf_str == 'Keras NN':\n",
    "        model = fit_predict_balanced_model(X_train, y_train)\n",
    "\n",
    "        # Determine Thresholds\n",
    "        proba_val = pd.DataFrame(model.predict_proba(X_val, batch_size=2000)).iloc[:,0]\n",
    "        print('Best Threshold')\n",
    "        thres = threshold(y_val, proba_val)\n",
    "\n",
    "        # Classification\n",
    "        proba_test = pd.DataFrame(model.predict_proba(X_test, batch_size=2000)).iloc[:,0]\n",
    "        pred = proba_test.map(lambda x: 1 if x >= thres['Threshold'] else 0)\n",
    "        \n",
    "    else:\n",
    "        model = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "        CV = pickle.dumps(model)\n",
    "        model = pickle.loads(CV)\n",
    "        model.fit(X_train, y_train)\n",
    "        print('Best Params')\n",
    "        print(model.best_params_)\n",
    "\n",
    "        # Determine Thresholds\n",
    "        proba_val = pd.DataFrame(model.predict_proba(X_val)).loc[:,1]  \n",
    "        print('Best Threshold')\n",
    "        thres = threshold(y_val, proba_val)\n",
    "\n",
    "        # Classification\n",
    "        proba_test = pd.DataFrame(model.predict_proba(X_test)).loc[:,1]\n",
    "        pred = proba_test.map(lambda x: 1 if x >= thres['Threshold'] else 0)\n",
    "    \n",
    "    # Prediction on test\n",
    "    print(target_values(pred, prediction=True))\n",
    "    print('F1 Score', f1_score(y_test, pred).round(2))\n",
    "    print('')\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "    print('')\n",
    "    \n",
    "    return model, thres['Threshold'], proba_test, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Function\n",
    "\n",
    "Wrapper for running and saving bottom layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Function\n",
    "def clf_pred_func(bnk_list, clfs, clfs_str,\n",
    "                  Banks_y, Banks_X, \n",
    "                  Banks_y_val, Banks_X_val,\n",
    "                  Banks_y_test, Banks_X_test): \n",
    "    ## Model Dictionaries\n",
    "    vote_models = {}\n",
    "    vote_thresholds = {}\n",
    "    vote_proba = {}\n",
    "    vote_pred = {}\n",
    "    \n",
    "    ## Model for Classifier Predictions\n",
    "    for bank_str in bnk_list:\n",
    "        vote_models[bank_str] = {}\n",
    "        vote_thresholds[bank_str] = {}\n",
    "        vote_proba[bank_str] = {}\n",
    "        vote_pred[bank_str] = {}\n",
    "    \n",
    "        for i in range(len(clfs)):\n",
    "            vote_models[bank_str][clfs_str[i]], \\\n",
    "            vote_thresholds[bank_str][clfs_str[i]], \\\n",
    "            vote_proba[bank_str][clfs_str[i]], \\\n",
    "            vote_pred[bank_str][clfs_str[i]] = bottom_layer_func(bank_str = bank_str, \\\n",
    "                                                                 estimator = clfs[i], \\\n",
    "                                                                 clf_str = clfs_str[i], \\\n",
    "                                                                 Banks_y = Banks_y, Banks_X = Banks_X, \\\n",
    "                                                                 Banks_y_val = Banks_y_val, Banks_X_val = Banks_X_val, \\\n",
    "                                                                 Banks_y_test = Banks_y_test, Banks_X_test = Banks_X_test)\n",
    "        \n",
    "    return vote_models, vote_thresholds, vote_proba, vote_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vote classifier function\n",
    "\n",
    "Saves prediction of bottom layer into dataframes for voting in middle and top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vote classifier\n",
    "## Voting dataframes function\n",
    "def votes_clf_func(vote_pred, bnk_list, clfs_str, X):\n",
    "    # Create dictionary of dataframes for voting\n",
    "    votes = {}\n",
    "    \n",
    "    # Save all classifier predictions into a dataframe for a given bank\n",
    "    for bank_str in bnk_list:\n",
    "        votes[bank_str] = pd.DataFrame()\n",
    "        \n",
    "        # Subset all bank data by bank\n",
    "        for clf in clfs_str:\n",
    "            votes[bank_str].loc[:,clf] = vote_pred[bank_str][clf]\n",
    "            \n",
    "    return votes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction function; includes bottom, middle, and top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Probabilities\n",
    "def proba_func(X, banks_str, bank_idx, clfs_str,\n",
    "               vote_models, vote_thresholds):\n",
    "    # Initiate Dictionary\n",
    "    votes = {}\n",
    "    \n",
    "    # Bottom Layer\n",
    "    ## Bank-specific\n",
    "    for bank_str in banks_str:\n",
    "        # Initiate Data Frames\n",
    "        votes[bank_str] = pd.DataFrame()\n",
    "        \n",
    "        ## Classifier-specific\n",
    "        for clf in clfs_str:\n",
    "            ### Predicted Probility\n",
    "            proba = pd.DataFrame(vote_models[bank_str][clf].predict_proba(X[bank_str])).iloc[:,1]\n",
    "            ### Classification\n",
    "            votes[bank_str][clf] = proba.map(lambda x: 1 if x >= vote_thresholds[bank_str][clf] else 0)                                                 \n",
    "\n",
    "    # Middle Layer\n",
    "    all_bnks_pred = ( votes['All Banks'].iloc[:,:len(clfs_str)].sum(axis=1) / \n",
    "                      len(clfs_str) ) \\\n",
    "                    .map(lambda x: 1 if x == 1 else 0)\n",
    "    for bank_str in banks_str:\n",
    "        votes[bank_str].loc[:,'All Banks'] = all_bnks_pred.loc[bank_idx == bank_str].reset_index().iloc[:,1]\n",
    "\n",
    "    # Top Layer\n",
    "    for bank_str in banks_str:\n",
    "        votes[bank_str].loc[:,'Majority'] = ( votes[bank_str].iloc[:,:(len(clfs_str)+1)].sum(axis=1) / \n",
    "                                            ( len(clfs_str)+1 ) ) \\\n",
    "                                            .map(lambda x: 1 if x > 0.67 else 0)\n",
    "    \n",
    "    return votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes assumptions in models by select a variable or multiple columns and changing their values while keeping other variable values the same, while predicting foreclosures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_assumptions(varlist, percentile, banks, bank_idx, X,\n",
    "                         vote_models, vote_thresholds,\n",
    "                         Banks_X, Banks_X_val, Banks_X_test,\n",
    "                         Banks_y, Banks_y_val, Banks_y_test):\n",
    "    # Ensure cols is list\n",
    "    if type(varlist) is not list:\n",
    "        varlist = [varlist]\n",
    "    if type(percentile) is not list:\n",
    "        percentile = [percentile]\n",
    "        \n",
    "    # Define value of var using total data's percentile\n",
    "    values = []\n",
    "    for i in range(len(varlist)):\n",
    "        values.append(np.percentile(X[varlist[i]], percentile[i]))\n",
    "        print('Converting', varlist[i], 'to the', percentile[i], 'percentile:', values[i].round(0))\n",
    "        print('')\n",
    "    \n",
    "    # Credit Score Data\n",
    "    X_df, y_act = pp_dfs(varlist, values, banks,\n",
    "                         Banks_X, Banks_X_val, Banks_X_test,\n",
    "                         Banks_y, Banks_y_val, Banks_y_test)\n",
    "    \n",
    "    # Saved Predicted Probabilities\n",
    "    bank_plus = banks + ['All Banks']\n",
    "    X_pp = proba_func(X_df, bank_plus, bank_idx, clfs_str,\n",
    "                      vote_models, vote_thresholds)\n",
    "    \n",
    "    # Initiate Data Frames and Dictionaries\n",
    "    final_frcls = pd.DataFrame(columns=['Original Foreclosures', 'Predicted Foreclosures'])\n",
    "    combined_votes = pd.Series()\n",
    "    combined_orig = pd.Series()\n",
    "    \n",
    "    for bank_str in banks:\n",
    "        pred = X_pp[bank_str]['Majority']\n",
    "        final_frcls.loc[bank_str, 'Original Foreclosures'] = (np.mean(y_act[bank_str]) * 100).round(1)\n",
    "        final_frcls.loc[bank_str, 'Predicted Foreclosures'] = (np.mean(pred) * 100).round(1)\n",
    "        \n",
    "        print(bank_str)\n",
    "        print('Original Foreclosures', final_frcls.loc[bank_str, 'Original Foreclosures'], '%') \n",
    "        print('Predicted Foreclosures', final_frcls.loc[bank_str, 'Predicted Foreclosures'], '%')\n",
    "        print('')\n",
    "    \n",
    "        # Combine banks\n",
    "        combined_votes = pd.concat([combined_votes, pred], axis=0)\n",
    "        combined_orig = pd.concat([combined_orig, y_act[bank_str]], axis=0)\n",
    "    \n",
    "    pred = X_pp['All Banks']['Majority']\n",
    "    final_frcls.loc['All Banks', 'Original Foreclosures'] = (np.mean(y_act['All Banks']) * 100).round(1)\n",
    "    final_frcls.loc['All Banks', 'Predicted Foreclosures'] = (np.mean(pred) * 100).round(1)\n",
    "    print('All Banks')\n",
    "    print('Original Foreclosures', final_frcls.loc['All Banks', 'Original Foreclosures'], '%')\n",
    "    print('Predicted Foreclosures', final_frcls.loc['All Banks', 'Predicted Foreclosures'], '%')\n",
    "    \n",
    "    return final_frcls, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of foreclosures before and after changed assumptions (i.e., predicted probabilities were run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changed_assumptions_tbl(var, improved, weakened):\n",
    "    tbl = pd.concat([improved[var], weakened[var].iloc[:,1]], axis=1)\n",
    "    header = [['Foreclosures', str('Improved '), str('Weakened ')],\n",
    "              ['(2006-2008)', var, var]]\n",
    "    tbl = pd.DataFrame(data=tbl.values, columns=header, index=tbl.index)\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar graph of foreclosures before and after changed assumptions (i.e., predicted probabilities were run)\n",
    "\n",
    "Only best and worst banks for a given feature a graphed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_gph(var, df, proba, proba_value, improved=True,\n",
    "                  func=[max, min], l_xlim=None, r_xlim=None):\n",
    "    best_bnk = search_Banks(var, df = df, func = func[0]).index[0]\n",
    "    worst_bnk = search_Banks(var, df = df, func = func[1]).index[0]\n",
    "    best_orig = proba[var]['Original Foreclosures'].loc[best_bnk]\n",
    "    best_new = proba[var]['Predicted Foreclosures'].loc[best_bnk]\n",
    "    worst_orig = proba[var]['Original Foreclosures'].loc[worst_bnk]\n",
    "    worst_new = proba[var]['Predicted Foreclosures'].loc[worst_bnk]\n",
    "    all_orig = proba[var]['Original Foreclosures'].loc['All Banks']\n",
    "    all_new = proba[var]['Predicted Foreclosures'].loc['All Banks']  \n",
    "    \n",
    "    labels = ['All Banks', best_bnk, worst_bnk]\n",
    "    orig = [all_orig, best_orig, worst_orig]\n",
    "\n",
    "    imp = [all_new, best_new, worst_new]\n",
    "    y = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8,3])\n",
    "    if improved:\n",
    "        bar1 = ax.barh(y + width/2, imp, width, label='Improved Assumption', color = 'darkgreen')\n",
    "    else:\n",
    "        bar1 = ax.barh(y + width/2, imp, width, label='Weakened Assumption', color = 'maroon')\n",
    "    bar2 = ax.barh(y - width/2, orig, width, label='Original Score', color = '#457b9d') # 457b9d\n",
    "    \n",
    "    ax.set_xlim(left=l_xlim, right=r_xlim)\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_xlabel('Foreclosure Rate')\n",
    "    ax.invert_yaxis() \n",
    "    ax.set_yticklabels(labels)\n",
    "    if improved:\n",
    "        ax.text(1.075, 0, str('Improved assumption changed\\n' + var + ' to ' + str(proba_value[var][0])),\n",
    "               fontsize='large', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.text(1.075, 0, str('Weakened assumption changed\\n' + var + ' to ' + str(proba_value[var][0])),\n",
    "               fontsize='large', transform=ax.transAxes)        \n",
    "    plt.title('Adjusting ' + var + '\\nAssumptions')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot target variable (and predictions)\n",
    "\n",
    "Visualize the percentage and frequency of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_values(df_depvar, data=False, prediction=False):\n",
    "        \n",
    "    # save target frequencies\n",
    "    target_frequency = df_depvar.value_counts()\n",
    "        \n",
    "    # save target percentage\n",
    "    target_percentage = round((df_depvar.value_counts()/df_depvar.count())*100).astype(int)\n",
    "        \n",
    "    # graphing target variable\n",
    "    jtplot.style(ticks=True, grid=False)\n",
    "    plt.figure(figsize=(14,4))\n",
    "    target_percentage.plot.barh(stacked=True, color='#ca2c92').invert_yaxis()\n",
    "    if data:\n",
    "        plt.suptitle('Bar Chart of Target Variable', fontsize=18)\n",
    "    elif prediction:\n",
    "        plt.suptitle('Bar Chart of Predictions', fontsize=18)\n",
    "    else:\n",
    "        plt.suptitle('Percent of Mortage Defaults', fontsize=18)\n",
    "    plt.ylabel('Foreclosed')\n",
    "    plt.xlabel('Percentage')\n",
    "    plt.xlim([0,100])\n",
    "    # plt.yticks([0, 1], ['Did not Foreclose', 'Foreclosed'])\n",
    "    plt.show()\n",
    "    \n",
    "    # display frequency of foreclosures\n",
    "    print('Frequency of Foreclosures\\n', target_frequency, '\\n', sep='')\n",
    "    \n",
    "    # display percentage of foreclosures\n",
    "    print('Percentage of Foreclosures\\n', target_percentage, '\\n', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize scores at various classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(y_test, target_prob):\n",
    "    # Determine threshold\n",
    "    threshold = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, \n",
    "                 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "    \n",
    "    acc = []\n",
    "    prec = []\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    best_auc = {'Threshold': 0.5, 'Best ROC AUC Score': 0.0}\n",
    "    best_acc = {'Threshold': 0.5, 'Best Accuracy Score': 0.0}\n",
    "    best_prec = {'Threshold': 0.5, 'Best Precision Score': 0.0}\n",
    "    best_f1 = {'Threshold': 0.5, 'Best F1 Score': 0.0}\n",
    "    for i in range(len(threshold)):\n",
    "        y_pred = target_prob.map(lambda x: 1 if x >= threshold[i] else 0)\n",
    "        \n",
    "        # Accuracy\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "        # Precision\n",
    "        prec.append(precision_score(y_test, y_pred))\n",
    "        # F1 \n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        # AUC\n",
    "        auc.append(roc_auc_score(y_test, y_pred))\n",
    "        \n",
    "        # Save best accuracy\n",
    "        if (best_acc['Best Accuracy Score'] < acc[i]):\n",
    "            best_acc = {'Threshold': threshold[i], 'Best Accuracy Score': acc[i]}\n",
    "        # Save best precision\n",
    "        if (best_prec['Best Precision Score'] < prec[i]):\n",
    "            best_prec = {'Threshold': threshold[i], 'Best Precision Score': prec[i]}      \n",
    "        # Save best f1\n",
    "        if (best_f1['Best F1 Score'] < f1[i]):\n",
    "            best_f1 = {'Threshold': threshold[i], 'Best F1 Score': f1[i]}       \n",
    "        # Save best Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "        if (best_auc['Best ROC AUC Score'] < auc[i]):\n",
    "            best_auc = {'Threshold': threshold[i], 'Best ROC AUC Score': auc[i]}   \n",
    "    \n",
    "    # Plot\n",
    "    df_plot = pd.DataFrame({'Threshold': threshold, 'Accuracy': acc, \n",
    "                            'Precision': prec, 'ROC AUC': auc, 'F1': f1})\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(df_plot['Threshold'], df_plot.iloc[:,1:5])\n",
    "    plt.title('Scores at Various Thresholds')\n",
    "    plt.legend(['Accuracy', 'Precision', 'ROC AUC', 'F1'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Scores\n",
    "    y_pred = target_prob.map(lambda x: 1 if x >= best_f1['Threshold'] else 0)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return( best_f1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Target Classes\n",
    "\n",
    "Visualizes if there are any obvious classification boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
