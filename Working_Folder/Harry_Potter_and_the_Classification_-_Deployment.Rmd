---
title: "Harry Potter NLP"
author: "Michael Siebel"
date: "`r date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    css: "../Rstyles.css" 
    code_folding: hide
    highlight: tango
    includes:
      in_header: "menu.html"
---

<br>

>  Model Ensembling and Classification for: <br>
>  Harry Potter and the Philosopher's Stone (1997) <br>
>  Harry Potter and the Chamber of Secrets (1998) <br>
>  Harry Potter and the Prisoner of Azkaban (1999) <br>
>  Harry Potter and the Goblet of Fire (2000) <br>
>  Harry Potter and the Order of the Phoenix (2003) <br>
>  Harry Potter and the Half-Blood Prince (2005) <br>
>  Harry Potter and the Deathly Hallows (2007)

# Bottom Line Up Front

I intend to answer the question:
<h6>"Which Harry Potter film is closest to its corresponding book?"</h6>

In this document, I built a model classifying portions of each books' text into the book that it belongs.  In the next, I will run film scripts through that model to determine how will it aligns with its corresponding book.

Classifying these books took 6 steps:

III) Structure text for 4 models

   * Description: Build 4 document term matricies (DTM) using different NLP techniques
   * Purpose: Use multiple NLP techniques in order to take advantage of each of their strengths

IV) Run 4 models

   * Description: Run 4 models independently with hyper-parameter tuning
   * Purpose: Optimize 4 models   

V) Perform stacked ensemble modeling

   * Description: Ensemble 4 bottom layer models with top layer model
   * Purpose: Take strengths of each model and minimize each model's weaknesses 

VI) Determine final model performance

   * Description: Test results of stacked model ensemble on testing data
   * Purpose: Ensure model process and outcome is generalizable

# Setup
```{r setup, results=FALSE, echo=TRUE, message=FALSE, comment=NA, warning=FALSE, tidy=TRUE} 
rm(list=ls())
gc()


library(pacman)
pacman::p_load(tidyverse, tidytext, rvest, xgboost, sentimentSetsR, caret, textTinyR, text2vec, tm, stm, SnowballC, stopwords, corpus, glue, readtext)
  
knitr::opts_chunk$set(echo=TRUE, message=FALSE, comment=NA, warning=FALSE, tidy=TRUE, results="hold", cache=TRUE, dpi=360)

# Custom functions
## Remove quotation marks
pasteNQ <- function(...) {
  output <- paste(...)
  noquote(output)
}
pasteNQ0 <- function(...) {
  output <- paste0(...)
  noquote(output)
}

## Chart Template
Grph_theme <- function() {
  theme_minimal(base_size=8) + 
  theme(legend.position="none")
}

## Clean Corpus
basicclean <- function(rawtext, contractions = TRUE) {
  
  # Set to lowercase
  rawtext <- tolower(rawtext)

  # Fix apostorphe
  rawtext <- gsub("â€™", "'", rawtext)

  # Remove contractions
  fix_contractions <- function(rawtext) {
    rawtext <- gsub("will not", "won't", rawtext)
    rawtext <- gsub("can't", "cannot", rawtext)
    rawtext <- gsub("can not", "cannot", rawtext)
    rawtext <- gsub("shall not", "shant", rawtext)
    rawtext <- gsub("n't", " not", rawtext)
    rawtext <- gsub("'ll", " will", rawtext)
    rawtext <- gsub("'re", " are", rawtext)
    rawtext <- gsub("'ve", " have", rawtext)
    rawtext <- gsub("'m", " am", rawtext)
    rawtext <- gsub("'d", " would", rawtext)
    rawtext <- gsub("'ld", " would", rawtext)
    rawtext <- gsub("'ld", " would", rawtext)
    rawtext <- gsub("'s", "", rawtext)
    return(rawtext)
  }
  if (contractions==TRUE) {
    rawtext <- fix_contractions(rawtext)
  }
  
  # Strip whitespace
  rawtext <- stripWhitespace(rawtext)

  return(rawtext)
}

# Remove stop words
removestopwords <- function(rawtext, remove=NULL, retain=NULL) {

  # Remove stop words
  stopwords_custom <- stopwords::stopwords("en", source = "snowball")
  stopwords_custom <- c(stopwords_custom, remove)
  stopwords_retain <- retain
  stopwords_custom <- stopwords_custom[!stopwords_custom %in% stopwords_retain]
  rawtext <- removeWords(rawtext, stopwords_custom)

  return(rawtext)
}

## Word Stemming
wordstem <- function(rawtext) {
  # Stemming words
  rawtext <- stemDocument(rawtext)

  return(rawtext)
}

## Remove Non-Alpha
removenonalpha <- function(rawtext) {
  # Remove puncutation, numbers, and other none characters
  rawtext <- removePunctuation(rawtext)
  rawtext <- removeNumbers(rawtext)
  rawtext <- gsub("[^[:alnum:]///' ]", "", rawtext)
  rawtext <- gsub("[']", "", rawtext)

  return(rawtext)
}
```


# Define Documents
**Description:** Create documents at the page-level
**Purpose:** Define portions of text that are small enough to provide many examples for the model but large enough to capture meaningful differences in text per book
   
* Page is defined as 250 words
* Series has 4,347 pages
* Create paragraph-level documents and link to page ID for later oversampling

```{r}
load("Harry_Potter_and_the_Classification_-_Documents.RData")
# load("Harry_Potter_and_the_Classification_-_Structure.RData")
# load("Harry_Potter_and_the_Classification_-_Models.RData")
load("Harry_Potter_and_the_Classification_-_Params.RData")

URL <- "http://www.hogwartsishere.com/library/book/7391/"
folder <- "C:/Users/siebe/Documents/01_R/Harry Potter/"

film_titles <- c()
for(i in 1:8) {
  films <- paste0('chapter/',i,'/')

    # Titles
  title <- read_html(paste0(URL, films)) %>%
    html_nodes(xpath = '//*[@id="wrapper"]/section[3]/div/div/div[2]/div/h4[2]') %>%
      html_text()
  title <- gsub('Sorcerer/', "", title, fixed=F, perl=T)
  title <- paste(i, title)
  print(title)
  film_titles <- c(film_titles, title)
}

txt <- paste0( substring(film_titles, 3), ".txt")

scripts <- tibble(Text=as.character(), Film=as.character())
for (i in 1:8) {  
  rawtext <- readtext(paste0(folder, txt[i]))
  rawtext <- rawtext$text
  
  # Scripts
  scripts <- rbind(scripts,
                   tibble(Text = rawtext, Film = film_titles[i]))
}

# Page Level Documents
scripts <- scripts %>%
  unnest_tokens(Text, Text, 
                token = "regex", pattern = "[[:space:]]",
                to_lower = F) %>%
  group_by(Film, Page = dplyr::row_number() %/% 250) %>%
  dplyr::summarize(Text = stringr::str_c(Text, collapse = " ")) %>%
  mutate(Page = dplyr::row_number()) %>%
  ungroup()

## Wordcount
scripts$Wordcount <- sapply(strsplit(scripts$Text %>% 
                            as.character(), " "), length)
## Progress
scripts$Progress <- row.names(scripts) %>% as.numeric() / nrow(scripts) * 100

# Place film in chronolgical order
scripts$Film <- factor(scripts$Film, levels=film_titles)
```

## Meta Data

### Sentiment Scores
```{r}
# Sentiment Scores
average <- function(x) {
  pos <- sum(x[x>0], na.rm = T)
  neg <- sum(x[x<0], na.rm = T) %>% abs()
  neu <- length(x[x==0])
  bal <- ( (pos-neg)/(pos+neg) )*100
  y <- ifelse(is.nan(bal),0,bal %>% as.numeric())
  return(y)
}

CleanText_scripts <- basicclean(scripts$Text)
scripts$Sentiment <- sapply(CleanText_scripts, 
                          function(x) getSentiment(x, dictionary = "vader", 
                                                   score.type = average))
```


### Animated Writing
```{r, include=F}
# count exclamation marks
scripts$Exclamation_Mark <- str_count(scripts$Text, "[!]") / 
                scripts$Wordcount %>%
                ifelse(is.na(.),0,.)

# count question marks
scripts$Question_Mark <- str_count(scripts$Text, "[?]") / 
                scripts$Wordcount %>%
                ifelse(is.na(.),0,.)

# sentence length
scripts$Declarative <- str_count(scripts$Text, "[.]") / 
                scripts$Wordcount %>%
                ifelse(is.na(.),0,.)
```


### Lead Characters
```{r}
heroes <- c("lily","james","hagrid","dumbledore","sirius","lupin","moody","slughorn","dobby","cedric","luna","tonks","mcgonagall","ginny","order of the phoenix","neville")
villians <- c("voldemort","nagini","snape","draco","lucius","umbridge","pettigrew","dementor","dementors","greyback","bellatrix","quirrell","riddle","death eaters","aragog","basilisk","dudley","vernon","petunia")

Sentiment <- c(heroes, villians)

# Script Level
for(string in Sentiment) {
  findstr <- paste0("\\b",string,"\\b")
  assign( string, ifelse(grepl(findstr,CleanText_scripts),1,0) )
}
scripts <- cbind(scripts, sapply( Sentiment, get ) )
```



# Structure Text for Four Models
**Description:** Build 4 document term matricies (DTM) using different NLP techniques
**Purpose:** Use multiple NLP techniques in order to take advantage of each of their strengths
   
## Create Four Datasets
```{r}
# Top layer 
set.seed(2020)
up_top_full <- upsample_train(pages, para3, para1)
top_full_target <- up_top_full$Book
top_num_full <- as.factor(top_full_target) %>% as.numeric() - 1

# Topic Model
set.seed(20)
up_full <- upsample_train(pages, para3, para1)
stm_full_target <- up_full$Book
stm_df <- stm_func(up_full, script = scripts)
stm_full <- stm_df[[1]] %>% cbind(up_full[ , c("Exclamation_Mark",
                                               "Question_Mark",
                                               "Declarative",
                                               # "Progress",
                                               "Sentiment")])
stm_scripts <- stm_df[[4]] %>% cbind(scripts[ , c("Exclamation_Mark",
                                               "Question_Mark",
                                               "Declarative",
                                               # "Progress",
                                               "Sentiment")])
# set.seed(20)
# up_full <- upsample_train(pages, para3, para1)
# stm_full_target <- up_full$Book
# stm_full <- subset(up_full, select = -c(Text, Book, Wordcount, Page))
# stm_scripts <- subset(scripts, select = -c(Text, Film, Wordcount, Page))

# Bag of Words (TF) Model
set.seed(21)
up_full <- upsample_train(pages, para3, para1)
bow_full_target <- up_full$Book
bow_dtm <- bow_func(up_full$Text, script_text = scripts$Text)
bow_full <- bow_dtm[[1]] %>% text2vec::normalize("l1")
bow_scripts <- bow_dtm[[4]]

# Bag of Words (TF-IDF) Model
set.seed(22)
up_full <- upsample_train(pages, para3, para1)
bowtfidf_full_target <- up_full$Book
bowtfidf_dtm <- bow_func(up_full$Text, script_text = scripts$Text)
bowtfidf_full <- bowtfidf_dtm[[1]]
tfidf <- TfIdf$new()
bowtfidf_full <- fit_transform(bowtfidf_full, tfidf)
bowtfidf_scripts <- bowtfidf_dtm[[4]]

# Word Embeddings (Sum-SQRT) Model
set.seed(23)
up_full <- upsample_train(pages, para3, para1)
we_full_target <- up_full$Book
we_dtm <- we_func(up_full$Text, script_text = scripts$Text, doc2vec = "sum_sqrt")
we_full <- we_dtm[[1]]
we_scripts <- we_dtm[[4]]

# Save results
save(stm_func, bow_func, we_func, titles,
     stm_full_target, stm_full, stm_scripts,
     bow_full_target, bow_full, bow_scripts,
     bowtfidf_full_target, bowtfidf_full, bowtfidf_scripts,
     we_full_target, we_full, we_scripts,
     top_full_target, top_num_full, up_top_full,
     file = "Harry_Potter_and_the_Classification_-_Structure_Deployment.RData")
```


# Run Four Models
**Description:** Run 4 models independently with hyper-parameter tuning
**Purpose:** Optimize 4 models

* Test 25 different versions of hyper-parameters on 4-fold cross validation
* Take best hyper-parameters and rerun on all training data 
* Repeat for each model


## Bottom Layer Models
```{r}
load("Harry_Potter_and_the_Classification_-_Structure_Deployment.RData")

num_class <- 7

# Results
pred_full <- data.frame(matrix(nrow=length(top_num_full), ncol=7*3))

# 1) Topic Model
## XGB format
stm_full <- xgb.DMatrix(as.matrix(stm_full),
                        label=as.factor(stm_full_target) %>% as.numeric() - 1)
## best model
stm_model <- xgboost(data=stm_full, verbose=F,
                     nrounds=stm_nrounds, param=stm_params)
## prediction
cols <- 1:7
pred_full[ , cols] <- predict(stm_model, stm_full) %>% 
                              matrix(ncol=num_class, byrow=TRUE)


# 2) Bag of Words TF Model
## XGB format
bow_full <- xgb.DMatrix(bow_full,
                        label=as.factor(bow_full_target) %>% as.numeric() - 1)
## best model
bow_model <- xgboost(data=bow_full, verbose=F,
                     nrounds=bow_nrounds, param=bow_params)
## prediction
cols <- 8:14
pred_full[ , cols] <- predict(bow_model, bow_full) %>% 
                              matrix(ncol=num_class, byrow=TRUE)


# 3) Bag of Words (TF-IDF) Model
## XGB format
bowtfidf_full <- xgb.DMatrix(bowtfidf_full,
                             label=as.factor(bowtfidf_full_target) %>% as.numeric() - 1)
## best model
bowtfidf_model <- xgboost(data=bowtfidf_full, verbose=F,
                          nrounds=bowtfidf_nrounds, param=bowtfidf_params)
## prediction
cols <- 15:21
pred_full[ , cols] <- predict(bowtfidf_model, bowtfidf_full) %>% 
                              matrix(ncol=num_class, byrow=TRUE)


# 4) Word Embeddings (Sum-SQRT) Model
## XGB format
we_full <- xgb.DMatrix(we_full,
                       label=as.factor(we_full_target) %>% as.numeric() - 1)
## best model
we_model <- xgboost(data=we_full, verbose=F,
                    nrounds=we_nrounds, param=we_params)
## prediction
cols <- 22:28
pred_full[ , cols] <- predict(we_model, we_full) %>%
                              matrix(ncol=num_class, byrow=TRUE)
```


# Perform Stacked Ensemble Modeling
**Description:** Ensemble 4 bottom layer models with top layer model
**Purpose:** Take strengths of each model and minimize each model's weaknesses    
     
* Resample train data for each of the 4 models with the same seed to ensure all rows contain same documents across bottom-layer models
* Use 4 training models to generate predicted probabilities and save as seven columns per model in a new data frame (28 columns in total)
* Model bottom layer models with top layer K Nearest Neighbors (KNN) to priviledge model strengths


## Top Layer (KNN)
```{r}
top_knn_full <- pred_full %>% data.frame()
top_knn_full$target <- top_full_target

# cross validation
trctrl <- trainControl(method = "repeatedcv", number = 4, repeats = 3)
model_top_knn <- train(
  target ~ .,
  data = top_knn_full,
  method = "knn",
  trControl = trctrl,
  # preProcess = c("center", "scale"),
  tuneLength = 1
)
library(nnet)
model_mlogit <- multinom(  target ~ ., data = top_knn_full)

# prediction
results_top_knn_full <- predict(model_top_knn, pred_full)
confusionMatrix(results_top_knn_full, top_knn_full$target)[[3]]
```


# Predictions

```{r}
# Results
pred_scripts <- data.frame(matrix(nrow=nrow(scripts), ncol=7*3))

# 1) Topic Model
## XGB format
stm_scripts <- xgb.DMatrix(as.matrix(stm_scripts))
## prediction
cols <- 1:7
pred_scripts[ , cols] <- predict(stm_model, stm_scripts) %>% 
                              matrix(ncol=num_class, byrow=TRUE)


# 2) Bag of Words TF Model
## XGB format
bow_scripts <- xgb.DMatrix(bow_scripts)
## prediction
cols <- 8:14
pred_scripts[ , cols] <- predict(bow_model, bow_scripts) %>% 
                              matrix(ncol=num_class, byrow=TRUE)


# 3) Bag of Words (TF-IDF) Model
## XGB format
bowtfidf_scripts <- xgb.DMatrix(bowtfidf_scripts)
## prediction
cols <- 15:21
pred_scripts[ , cols] <- predict(bowtfidf_model, bowtfidf_scripts) %>% 
                              matrix(ncol=num_class, byrow=TRUE)


# # 4) Word Embeddings (Sum-SQRT) Model
## XGB format
we_scripts <- xgb.DMatrix(we_scripts)
## prediction
cols <- 22:28
pred_scripts[ , cols] <- predict(we_model, we_scripts) %>%
                              matrix(ncol=num_class, byrow=TRUE)

# Top Layer KNN
## final predictions
mlogit_pred <- predict(model_mlogit, pred_scripts, type="prob") %>% 
               as.data.frame()
collapse <- cbind(Film = scripts$Film %>% as.character(), 
                  mlogit_pred)
collapse <- collapse %>%
  dplyr::group_by(Film) %>%
  summarize(`1 Philosopher's Stone` = mean(`1 Philosopher's Stone`, na.rm = TRUE),
            `2 Chamber of Secrets` = mean(`2 Chamber of Secrets`, na.rm = TRUE),
            `3 Prisoner of Azkaban` = mean(`3 Prisoner of Azkaban`, na.rm = TRUE),
            `4 Goblet of Fire` = mean(`4 Goblet of Fire`, na.rm = TRUE),
            `5 Order of the Phoenix` = mean(`5 Order of the Phoenix`, na.rm = TRUE),
            `6 Half-Blood Prince` = mean(`6 Half-Blood Prince`, na.rm = TRUE),
            `7 Deathly Hallows` = mean(`7 Deathly Hallows`, na.rm = TRUE))
collapse
```


# Collapse
```{r}
predicted_probs <- predict(model_top_knn, pred_scripts, type="prob")
predicted_class <- predict(model_top_knn, pred_scripts, type="raw")

collapse <- cbind(Film = scripts$Film, predicted_probs)
final <- collapse %>%
  group_by(Film) %>%
  summarize(`1 Philosopher's Stone` = mean(`1 Philosopher's Stone`, na.rm = TRUE),
            `2 Chamber of Secrets` = mean(`2 Chamber of Secrets`, na.rm = TRUE),
            `3 Prisoner of Azkaban` = mean(`3 Prisoner of Azkaban`, na.rm = TRUE),
            `4 Goblet of Fire` = mean(`4 Goblet of Fire`, na.rm = TRUE),
            `5 Order of the Phoenix` = mean(`5 Order of the Phoenix`, na.rm = TRUE),
            `6 Half-Blood Prince` = mean(`6 Half-Blood Prince`, na.rm = TRUE),
            `7 Deathly Hallows` = mean(`7 Deathly Hallows`, na.rm = TRUE))
final <- final %>% column_to_rownames("Film")
final_grph <- t(final) * 100 
final_grph <- cbind(Book = rownames(final_grph), as_tibble(final_grph))
rownames(final_grph) <- 1:7

# Graph
grph_pred <- function(df, x, cols) {
  for (i in cols) {
    g <- df %>% 
      as.data.frame() %>%
      ggplot(aes(x = get(x), y = final_grph[ , i],
                 fill = get(x))) +
      geom_bar(stat = "identity") +
      coord_flip() +
      ylim(0, 100) +
      labs(title="Script:",
           subtitle = colnames(df)[i],
           y="Probability (%)",
           x="Book") + 
      scale_fill_manual(values = c("#946B2D", "#0D6217", "#000A90",
                                   "#AAAAAA", "#000000", "#7F0909", "#FFC500")) +
      Grph_theme()
  print(g)
  }
}
# Graph each probability
grph_pred(final_grph, "Book", cols = 2:9)
```


# Save
```{r}
save(pred_full, pred_scripts, predicted_probs, predicted_class, final,
     stm_model, bow_model, bowtfidf_model, we_model, model_top_knn,
     scripts, file = "Script_Predictions.RData")
```







