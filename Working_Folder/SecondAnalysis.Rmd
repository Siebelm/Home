---
title: "Harry Potter NLP"
author: "Michael Siebel"
date: "`r date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    css: "../Rstyles.css" 
    code_folding: hide
    highlight: tango
    includes:
      in_header: "menu.html"
---

<br>

>  Natural Language Processing (NLP) for: <br>
>  Harry Potter and the Philosopher's Stone (1997) <br>
>  Harry Potter and the Chamber of Secrets (1998) <br>
>  Harry Potter and the Prisoner of Azkaban (1999) <br>
>  Harry Potter and the Goblet of Fire (2000) <br>
>  Harry Potter and the Order of the Phoenix (2003) <br>
>  Harry Potter and the Half-Blood Prince (2005) <br>
>  Harry Potter and the Deathly Hallows (2007)

# Bottom Line Up Front

# Setup
```{r setup, results=FALSE, echo=TRUE, message=FALSE, comment=NA, warning=FALSE, tidy=TRUE} 
rm(list=ls())
gc()


library(pacman)
pacman::p_load(knitr, magrittr, dplyr, ggplot2, rvest, xgboost, sentimentSetsR, caret, textTinyR, text2vec, tm, tidytext, stringr, stringi, SnowballC, stopwords, kableExtra, corpus, glue, RColorBrewer, tidyr)
  
knitr::opts_chunk$set(echo=TRUE, message=FALSE, comment=NA, warning=FALSE, tidy=TRUE, results="hold", cache=FALSE, dpi=120)

# Custom functions
## Remove quotation marks
pasteNQ <- function(...) {
  output <- paste(...)
  noquote(output)
}
pasteNQ0 <- function(...) {
  output <- paste0(...)
  noquote(output)
}

## Chart Template
Grph_theme <- function() {
  palette <- brewer.pal("Greys", n=9)
  color.background = palette[2]
  color.grid.major = palette[3]
  color.axis.text = palette[6]
  color.axis.title = palette[7]
  color.title = palette[9]    
  theme_bw(base_size=9) + 
  theme(panel.background=element_rect(fill=color.background, color=color.background)) +
  theme(plot.background=element_rect(fill=color.background, color=color.background)) +
  theme(panel.border=element_rect(color=color.background)) +
  theme(panel.grid.major=element_line(color=color.grid.major,size=.25)) +
  theme(panel.grid.minor=element_blank()) +
  theme(axis.ticks=element_blank()) +
  theme(legend.position="none") +
  theme(legend.title=element_text(size=16,color='black')) +
  theme(legend.background = element_rect(fill=color.background)) +
  theme(legend.text = element_text(size=14,color='black')) +
  theme(strip.text.x = element_text(size=14,color='black',vjust=1)) +
  theme(plot.title=element_text(color=color.title, size=20, vjust=1.25)) +
  theme(axis.text.x=element_text(size=14,color='black')) +
  theme(axis.text.y=element_text(size=14,color='black')) +
  theme(axis.title.x=element_text(size=16,color='black', vjust=0)) +
  theme(axis.title.y=element_text(size=16,color='black', vjust=1.25)) +
  theme(plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), "cm"))
}

## Chart Template Facet Wrap
Grph_theme_facet <- function() {
  palette <- brewer.pal("Greys", n=9)
  color.background = palette[2]
  color.grid.major = palette[3]
  color.axis.text = palette[6]
  color.axis.title = palette[7]
  color.title = palette[9]    
  theme_bw(base_size=9) + 
  theme(panel.background=element_rect(fill=color.background, color=color.background)) +
  theme(plot.background=element_rect(fill=color.background, color=color.background)) +
  theme(panel.border=element_rect(color=color.background)) +
  theme(panel.grid.major=element_line(color=color.grid.major,size=.25)) +
  theme(panel.grid.minor=element_blank()) +
  theme(axis.ticks=element_blank()) +
  theme(legend.position="none") +
  theme(legend.title=element_text(size=11,color='black')) +
  theme(legend.background = element_rect(fill=color.background)) +
  theme(legend.text = element_text(size=9,color='black')) +
  theme(strip.text.x = element_text(size=9,color='black',vjust=1)) +
  theme(plot.title=element_text(color=color.title, size=20, vjust=1.25)) +
  theme(axis.text.x=element_text(size=9,color='black')) +
  theme(axis.text.y=element_text(size=9,color='black')) +
  theme(axis.title.x=element_text(size=10,color='black', vjust=0)) +
  theme(axis.title.y=element_text(size=10,color='black', vjust=1.25)) +
  theme(plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), "cm"))
}

## Clean Corpus
basicclean <- function(rawtext, contractions = TRUE) {
  
  # Set to lowercase
  rawtext <- tolower(rawtext)

  # Fix apostorphe
  rawtext <- gsub("â€™", "'", rawtext)

  # Remove contractions
  fix_contractions <- function(rawtext) {
    rawtext <- gsub("will not", "won't", rawtext)
    rawtext <- gsub("can't", "cannot", rawtext)
    rawtext <- gsub("can not", "cannot", rawtext)
    rawtext <- gsub("shall not", "shant", rawtext)
    rawtext <- gsub("n't", " not", rawtext)
    rawtext <- gsub("'ll", " will", rawtext)
    rawtext <- gsub("'re", " are", rawtext)
    rawtext <- gsub("'ve", " have", rawtext)
    rawtext <- gsub("'m", " am", rawtext)
    rawtext <- gsub("'d", " would", rawtext)
    rawtext <- gsub("'ld", " would", rawtext)
    rawtext <- gsub("'ld", " would", rawtext)
    rawtext <- gsub("'s", "", rawtext)
    return(rawtext)
  }
  if (contractions==TRUE) {
    rawtext <- fix_contractions(rawtext)
  }
  
  # Strip whitespace
  rawtext <- stripWhitespace(rawtext)

  return(rawtext)
}

# Remove stop words
removestopwords <- function(rawtext, remove=NULL, retain=NULL) {

  # Remove stop words
  stopwords_custom <- stopwords::stopwords("en", source = "snowball")
  stopwords_custom <- c(stopwords_custom, remove)
  stopwords_retain <- retain
  stopwords_custom <- stopwords_custom[!stopwords_custom %in% stopwords_retain]
  rawtext <- removeWords(rawtext, stopwords_custom)

  return(rawtext)
}

## Word Stemming
wordstem <- function(rawtext) {
  # Stemming words
  rawtext <- stemDocument(rawtext)

  return(rawtext)
}

## Remove Non-Alpha
removenonalpha <- function(rawtext) {
  # Remove puncutation, numbers, and other none characters
  rawtext <- removePunctuation(rawtext)
  rawtext <- removeNumbers(rawtext)
  rawtext <- gsub("[^[:alnum:]///' ]", "", rawtext)
  rawtext <- gsub("[']", "", rawtext)

  return(rawtext)
}
```


```{r}
setwd("C:/Users/siebe/Documents/07_Books/Harry Potter/")

titles <- c("1 Philosopher's Stone", "2 Chamber of Secrets", "3 Prisoner of Azkaban",
            "4 Goblet of Fire", "5 Order of the Phoenix", "6 Half-Blood Prince",
            "7 Deathly Hallows")
html <- c("Harry_Potter_and_the_Philosophers_Stone.html",
          "Harry_Potter_and_the_Chamber_of_Secrets.html",
          "Harry_Potter_and_the_Prisoner_of_Azkaban.html",
          "Harry_Potter_and_the_Goblet_of_Fire.html",
          "Harry_Potter_and_the_Order_of_the_Phoenix.html",
          "Harry_Potter_and_the_Half-Blood_Prince.html",
          "Harry_Potter_and_the_Deathly_Hallows.html")

books <- tibble(Text=as.character(), Book=as.character())
para3 <- tibble(Text=as.character(), Book=as.character())
para1 <- tibble(Text=as.character(), Book=as.character())

for (i in 1:7) {  
  rawtext <- read_html(html[i])%>%
      html_nodes(xpath = '/html/body/p') %>%
          html_text(trim = TRUE)
  
  wordcount <- sapply(strsplit(rawtext, " "), length)
  paragraph <- rawtext #[wordcount >= 3]

  # Book Level Documents
  books <- rbind(books,
                 tibble(Text = str_c(paragraph, 
                                         collapse = " "),
                            Book = titles[i]
                            )
           )

  # Parapraph
  para1 <- rbind(para1,
                 tibble(Text = paragraph, Book = titles[i]))

  # Paragraph Level Documents
  triplet <- do.call(rbind, 
                     lapply(seq(1, length(paragraph), by = 3),
                          function(x) 
                                  tibble(Text = str_c(paragraph[x:(x+2)], 
                                                          collapse = " "),
                                             Book = titles[i]
                                             )
                          )
                   )
  para3 <- rbind(para3, triplet)
}

# Page Level Documents
pages <- books %>%
  unnest_tokens(Text, Text, 
                token = "regex", pattern = "[[:space:]]",
                to_lower = F) %>%
  group_by(Book, Page = dplyr::row_number() %/% 250) %>%
  dplyr::summarize(Text = stringr::str_c(Text, collapse = " ")) %>%
  mutate(Page = dplyr::row_number()) %>%
  ungroup()
## Wordcount
pages$Wordcount <- sapply(strsplit(pages$Text %>% as.character(), " "), length)
## Word IDs
s <- data.frame()
for(j in titles) {
  t <- pages[pages$Book==j, ]
  t$Word_Start <- NA
  t$Word_Start[1] <- 1
  t$Word_End <- NA
  t$Word_End[1] <- t$Wordcount[1]
  for(i in 2:nrow(t)) {
    t$Word_Start[i] <- t$Word_End[i-1] + 1
    t$Word_End[i] <- t$Word_Start[i] + t$Wordcount[i]
  }
  s <- rbind(s, t)
}
pages <- dplyr::left_join(pages, s)

# Paragraph Level Documents
# Add Page Numbers
page_nums <- function(df) {
  ## Drop missing
  df <- df[!is.na(df$Text), ]
  ## Paragraph ID
  df$Para_ID <- row.names(df) %>% as.numeric()
  ## Wordcount
  df$Wordcount <- sapply(strsplit(df$Text %>% as.character(), " "), length)
  ## Word IDs
  s <- data.frame()
  for(j in titles) {
    t <- df[df$Book==j, ]
    t$Word_Start <- NA
    t$Word_Start[1] <- 1
    t$Word_End <- NA
    t$Word_End[1] <- t$Wordcount[1]
    for(i in 2:nrow(t)) {
      t$Word_Start[i] <- t$Word_End[i-1] + 1
      t$Word_End[i] <- t$Word_Start[i] + t$Wordcount[i]
    }
    s <- rbind(s, t)
  }
  df <- dplyr::left_join(df, s)
  
  # Page ID
  match_page <- function(j) {
    t1 <- df[df$Book==j, c("Para_ID", "Word_Start", "Word_End")]
    t2 <- pages[pages$Book==j, c("Page", "Word_Start", "Word_End")]
    
    t1$Page <- NA
    for(i1 in 1:nrow(t1)) {
        for(i2 in nrow(t2):1) {
        t1$Page[i1] <- ifelse((t1$Word_Start[i1] >= t2$Word_Start[i2]) & 
                              (t1$Word_Start[i1] <  t2$Word_End[i2]) & 
                              (t1$Word_End[i1] <= t2$Word_End[i2]+20),
                               t2$Page[i2],
                               t1$Page[i1])
      }
    }
    return(t1)
  }
  
  # Parallel
  pacman::p_load(future.apply)
  plan(multiprocess)
  s <- future_lapply(titles, match_page)
  s <- do.call("rbind", s)
  df <- dplyr::left_join(df, s)
  
  return(df)
}
para3 <- page_nums(para3)
para1 <- page_nums(para1)

# Remove Word IDs
pages <- pages[ , c("Text", "Book", "Wordcount", "Page")]
para3 <- para3[ , c("Text", "Book", "Wordcount", "Page")]
para1 <- para1[ , c("Text", "Book", "Wordcount", "Page")]

# Place books in chronolgical order
books$Book <- factor(books$Book, levels=titles)
para3$Book <- factor(para3$Book, levels=titles)
para1$Book <- factor(para1$Book, levels=titles)
pages$Book <- factor(pages$Book, levels=titles)
```

# Text at the Paragraph Level

For this sentiment analysis, I want to grab the text around certain characters. Chapters and pages are too much text as they can contain multiple story points, and sentences are too little text as they likely contain little contextual information.  

Instead, I plan to take paragraphs as the documents for my corpus.  However, paragraphs can be single sentences as in the case of two characters switch dialogue.  Therefore, I take paragraph triplets: three paragraphs containing three or more words.

## Word Distributions

I start checking for normal (or "normal-ish") distributions of words per paragraph triplet to make sure there is some consistency in document length.  The word distributions appear to approximate a normal distribution, although with a non-trivial right tail.  In addition, the word distributions are similar across books, making it a comparable level of analysis.

```{r}
# Remove Text
CleanText_p3 <- basicclean(para3$Text)
CleanText_p1 <- basicclean(para1$Text)
CleanText_w250 <- basicclean(pages$Text)

# Summary Statistics
pasteNQ0("Average Amount of Words per Paragraph Triplet")
summary(para3$Wordcount)

# Graph distribution of words all
ggplot(para3, aes(Wordcount, fill=I("#7F0909"))) + 
  geom_histogram() + 
  stat_bin(bins = 100) +
  Grph_theme() +
  ylab('Frequency') + xlab('Count of Words') + 
  ggtitle('Words per Paragraph Triplet')

# Graph with book fill
ggplot(para3, aes(Wordcount, fill=Book)) + 
  geom_area(stat = "bin") + 
  scale_fill_manual(values = c("#946B2D", "#0D6217", "#000A90", 
                                 "#AAAAAA", "#000000", "#7F0909", "#FFC500")) +
  Grph_theme() +
  ylab('Frequency') + xlab('Count of Words') + 
  ggtitle('Words per Paragraph Triplet') + 
  theme(legend.text = element_text(size=8,color='black')) +
  theme(legend.position="bottom")
```

## Page level variation

Paragraph level analysis has the advantage of grouping text by logical beginnings and endings.  Alternatively, page level analysis often groups text by the beginning and ending in half-sentence, mid-paragraph.

However, page level analysis has the advantage of not containing any variation in document length; all documents are a standard 250 words.

Given these advantages and disadvantages, I prioritized paragraph level analysis by using all paragraph triplets in the Series.  I then appended (code later) random samples of page level documents inversely proportionaly to the amount of paragraph triplets in each book.  In other words, I balanced the classes, ensuring each book contained the same number of documents, by adding many page level documents to the shorter books and fewer page level documents to the longer books.

# Sentiment Model

## Progress
```{r}
# Page Level
pages$Progress <- row.names(pages) %>% as.numeric() / nrow(pages) * 100

# Paragraph Level
para3$Progress <- row.names(para3) %>% as.numeric() / nrow(para3) * 100
para1$Progress <- row.names(para1) %>% as.numeric() / nrow(para1) * 100
```


## Sentiment Scores
```{r}
# Sentiment Scores
average <- function(x) {
  pos <- sum(x[x>0], na.rm = T)
  neg <- sum(x[x<0], na.rm = T) %>% abs()
  neu <- length(x[x==0])
  bal <- ( (pos-neg)/(pos+neg) )*100
  y <- ifelse(is.nan(bal),0,bal %>% as.numeric())
  return(y)
}

pages$Sentiment <- sapply(CleanText_w250, 
                          function(x) getSentiment(x, dictionary = "vader", 
                                                    score.type = average))
para3$Sentiment <- sapply(CleanText_p3, 
                          function(x) getSentiment(x, dictionary = "vader", 
                                               score.type = average)) 
para1$Sentiment <- sapply(CleanText_p1, 
                          function(x) getSentiment(x, dictionary = "vader", 
                                               score.type = average)) 
```


## Animated writing
```{r, include=F}
# count exclamation marks
pages$E_Mark <- str_count(pages$Text, "[!]") %>%
                ifelse(is.na(.),0,.)
para3$E_Mark <- str_count(para3$Text, "[!]") %>%
                ifelse(is.na(.),0,.)
para1$E_Mark <- str_count(para1$Text, "[!]") %>%
                ifelse(is.na(.),0,.)

# count question marks
pages$Q_Mark <- str_count(pages$Text, "[?]") %>%
                ifelse(is.na(.),0,.)
para3$Q_Mark <- str_count(para3$Text, "[?]") %>%
                ifelse(is.na(.),0,.)
para1$Q_Mark <- str_count(para1$Text, "[?]") %>%
                ifelse(is.na(.),0,.)

# count caps-lock words
# pages$Caps_Lock <- str_count(pages$Text, "\\b[A-Z]{2,}$\\b") %>%
#                    ifelse(is.na(.),0,.)
# para3$Caps_Lock <- str_count(para3$Text, "\\b[A-Z]{2,}$\\b") %>%
#                    ifelse(is.na(.),0,.)
# para1$Caps_Lock <- str_count(para1$Text, "\\b[A-Z]{2,}$\\b") %>%
#                    ifelse(is.na(.),0,.)
```


## Lead characters
```{r}
heroes <- c("lily","james","hagrid","dumbledore","sirius","lupin","moody","slughorn","dobby","cedric","luna","tonks","mcgonagall","ginny","order of the phoenix","neville")
villians <- c("voldemort","nagini","snape","draco","lucius","umbridge","pettigrew","dementor","dementors","greyback","bellatrix","quirrell","riddle","death eaters","aragog","basilisk","dudley","vernon","petunia")

Sentiment <- c(heroes, villians)

# Page Level
for(string in Sentiment) {
  findstr <- paste0("\\b",string,"\\b")
  assign( string, ifelse(grepl(findstr,CleanText_w250),1,0) )
}
pages <- cbind(pages, sapply( Sentiment, get ) )

# Paragraph Level
for(string in Sentiment) {
  findstr <- paste0("\\b",string,"\\b")
  assign( string, ifelse(grepl(findstr,CleanText_p3),1,0) )
}
para3 <- cbind(para3, sapply( Sentiment, get ) )

for(string in Sentiment) {
  findstr <- paste0("\\b",string,"\\b")
  assign( string, ifelse(grepl(findstr,CleanText_p1),1,0) )
}
para1 <- cbind(para1, sapply( Sentiment, get ) )

```



# Bag of Words

## Document Term Matrices
```{r}
# Function
dtm_bow <- function(train_text, test_text, df) {
  set.seed(20)
  
  train_text_cleaned <- train_text %>%
                        basicclean() %>%
                        removestopwords() %>%
                        wordstem() %>%
                        removenonalpha()
  test_text_cleaned <- test_text %>%
                       basicclean() %>%
                       removestopwords() %>%
                       wordstem() %>%
                       removenonalpha()
  
  #Train
  ## IDs
  ids <- 1:length(train_text)
  
  ## tokenize
  it_train <- itoken(train_text_cleaned, 
  				           ids = ids, 
  				           progressbar = FALSE) 
  
  ## ngrams
  vocab <- create_vocabulary(it_train, c(1,1)) 
  vocab <- prune_vocabulary(vocab, doc_proportion_min = 0.005)
  
  vectorizer <- vocab_vectorizer(vocab)
  
  ## create dtm
  dtm_train <- create_dtm(it_train, vectorizer, type="dgCMatrix")
  
  ## number of term input
  pasteNQ("Training document term matrix specifications:")
  pasteNQ("Cleaned", sum(df$Wordcount), "words into", ncol(dtm_train), "columns")
  pasteNQ("Number of documents:", nrow(dtm_train))
  cat("\n")

  # Test
  ## IDs
  ids <- 1:length(test_text)

  ## tokenize
  it_test <- itoken(test_text_cleaned, 
  				          ids = ids, 
  				          progressbar = FALSE) 
  
  # create dtm
  dtm_test <- create_dtm(it_test, vectorizer, type="dgCMatrix")
  
  # number of term input
  pasteNQ("Test document term matrix specifications:")
  pasteNQ("Cleaned", sum(df$Wordcount), "words into", ncol(dtm_test), "columns")
  pasteNQ("Number of documents:", nrow(dtm_test))
  cat("\n")  
  
  return(list(dtm_train, dtm_test))
}
```


# Word Embeddings
```{r}
# Function
dtm_we <- function(train_text, test_text) {
  set.seed(20)
  
  # Simple clean
  train_text_cleaned <- train_text %>%
                        basicclean(contractions = F) 
  test_text_cleaned <- test_text %>%
                       basicclean(contractions = F)
  
  # Tokenize
  tokens <- word_tokenizer(train_text)
  
  # create dtm
  dtm_train <- textTinyR::Doc2Vec$new(token_list = tokens, 
  							                      word_vector_FILE = "glove.42B.300d.vec",
  							                      copy_data = FALSE) 
  
  dtm_train <- dtm_train$doc2vec_methods(method = "min_max_norm", threads = 4)
  
  pasteNQ("Train: Doc2Vec Word Embeddings")
  pasteNQ("External Word Embeddings: FastText English word vectors released by Facebook")
  pasteNQ("Pretraining on: 1.9 million word vectors trained on Common Crawl (42B tokens)")
  pasteNQ("Dimensionality Reduction to:", 300)	
  pasteNQ("Final Dimensions")
  dim(data.frame(as.matrix(dtm_train)))
  cat("\n")
  
  
  # Tokenize
  tokens <- word_tokenizer(test_text)
  
  # create dtm
  dtm_test <- textTinyR::Doc2Vec$new(token_list = tokens, 
  							                      word_vector_FILE = "glove.42B.300d.vec",
  							                      copy_data = FALSE) 
  
  dtm_test <- dtm_test$doc2vec_methods(method = "min_max_norm", threads = 4)
  
  pasteNQ("Train: Doc2Vec Word Embeddings")
  pasteNQ("External Word Embeddings: FastText English word vectors released by Facebook")
  pasteNQ("Pretraining on: 1.9 million word vectors trained on Common Crawl (42B tokens)")
  pasteNQ("Dimensionality Reduction to:", 300)	
  pasteNQ("Final Dimensions")
  dim(data.frame(as.matrix(dtm_test)))
  cat("\n") 
  
  return(list(dtm_train, dtm_test))
}
```





*Example*
docs upsample_n
3     max(docs) - 3 = 3*2 = 6 + 3 = 9
4     max(docs) - 4 = 2*2 = 4 + 4 = 8
6     max(docs) - 6 = 0*2 = 0 + 6 = 6
2     max(docs) - 2 = 4*2 = 8 + 2 = 10
6     max(docs) - 6 = 0*2 = 0 + 6 = 6

*Definition*
1) largest doc count - number of docs #balances
2) times by 2 #increase upsample for a doubled para3
3) add orignal para3 to double total doc size

*Result*
12 docs for all
doubling docs then upsampling
never repeating same doc twice

# Data Imbalances in Training Data
```{r}
set.seed(20)
train_index <- sample(nrow(pages),nrow(pages)*0.70)

train <- pages[train_index, ]
test  <- pages[-train_index, ]

# Currently balance
pasteNQ("Current Balance")
table(train$Book)

# sort Book freqs by chronological order
docs <- table(train$Book) %>%
        as.data.frame()

# Define upsample scalars
upsample_n <- max(docs$Freq) - docs$Freq
upsample_n <- floor((upsample_n * 2 + docs$Freq) / 3)
upsample_para1 <- upsample_n * 2
upsample_para3 <- upsample_n

# Should all be the same number
cat("\n")
pasteNQ("Target Balance:")
upsample_para1 + upsample_para3 + docs$Freq
```

## Upsampleing
```{r}
upsample <- function(df, upsample_n) {
  # Remove short docs
  df <- df[df$Wordcount > 20, ]
  
  # Separate pages corpus into separate objects by Book title
  for(i in 1:7) {
      assign(paste0("page_", i),
             train[train$Book==titles[i],] %>%
             tidyr::drop_na())
    
      assign(paste0("para_", i),
             df[df$Book==titles[i],] %>%
             tidyr::drop_na())
  }
  
  # Upsample from single paragraphs by taking random sample w/o replacement of   upsample scalars
  upsamples <- data.frame()
  for (i in 1:7) {
    r <- get(paste0("page_", i))
    t <- get(paste0("para_", i))
    upsamples <- rbind(upsamples, (
                       t[sample(nrow(t[t$Page %in% r$Page, ]), 
                                size = upsample_n[i]), ]
                       )
    )
    t <- NULL
  }
  
  df <- upsamples
  
  return(df)
}
para3_df <- upsample(para3, upsample_para3)
para1_df <- upsample(para1, upsample_para1)
train <- rbind(train, para3_df)
train <- rbind(train, para1_df)

cat("\n")
pasteNQ("Final Balance of Training Data")
table(train$Book)
cat("\n")
pasteNQ("Balance of Testing Data")
table(test$Book)
```


## Create Three Datasets
```{r}
set.seed(20)

# Target Variables
target_train <- train[ , "Book"] %>% 
                make.names() 
target_test  <- test[ , "Book"] %>% 
                make.names() 

# Sentiment Model
sent_train <- select(train, -c("Text", "Book", "Page", "Progress"))
sent_test  <- select(test, -c("Text", "Book", "Page", "Progress"))

# Bag of Words Model
bow_dtm <- dtm_bow(train$Text, test$Text, pages)
bow_train <- bow_dtm[[1]]
bow_test  <- bow_dtm[[2]]

# Word Embeddings Model
we_dtm <- dtm_we(train$Text, test$Text)
we_train <- we_dtm[[1]]
we_test  <- we_dtm[[2]]

# Dataframe for results
results_train <- data.frame(Target = target_train)
results_test  <- data.frame(Target = target_test)

save(train, test,
     target_train, target_test, 
     sent_train, sent_test, 
     bow_train, bow_test,
     we_train, we_test,
     results_train, results_test,
     file = "Harry_Potter_and_the_Classification.RData")
```


# Model data
```{r}
load("Harry_Potter_and_the_Classification.RData")

num_train <- as.factor(target_train) %>% as.numeric() - 1
num_test  <- as.factor(target_test) %>% as.numeric - 1

# Sentiment model 
sent_train <- xgb.DMatrix(as.matrix(sent_train),
                          label=num_train,
                          missing=NaN)
sent_test  <- xgb.DMatrix(as.matrix(sent_test),
                          label=num_test,
                          missing=NaN)

# Bag of words TF-IDF model
tfidf <- TfIdf$new()
bowtfidf_train <- fit_transform(bow_train, tfidf)
bowtfidf_test <- transform(bow_test, tfidf)
bowtfidf_train <- xgb.DMatrix(bowtfidf_train,
                              label=num_train,
                              missing=NaN)
bowtfidf_test  <- xgb.DMatrix(bowtfidf_test,
                              label=num_test,
                              missing=NaN)

# Bag of words TF model
bow_train <- xgb.DMatrix(bow_train,
                         label=num_train,
                         missing=NaN)
bow_test  <- xgb.DMatrix(bow_test,
                         label=num_test,
                         missing=NaN)

# Word embeddings model
we_train <- xgb.DMatrix(we_train,
                        label=num_train,
                        missing=NaN)
we_test  <- xgb.DMatrix(we_test,
                        label=num_test,
                        missing=NaN)

# Global parameters
num_class <- 7

# cross validation
cv_tune <- function(data, iterations = 25, num_class = 7, 
                    nrounds = 500, nfold = 4, 
                    eval_metric = "mlogloss", 
                    objective = "multi:softprob") {
  best_param = list()
  best_seednumber = 1234
  best_logloss = Inf
  best_logloss_index = 0
  
  for (i in 1:iterations) {
      param <- list(
            objective = objective,
            num_class = num_class,
            eval_metric = eval_metric,
            eta = runif(1, .01, .8),
            max_depth = sample(4:8, 1),
            subsample = 1
            )
      seed.number <- sample.int(10000, 1)[[1]]
      set.seed(seed.number)          
      cross_val <- xgb.cv(data = data, param = param, 
                          verbose = F, nthread = 8, 
                          early_stopping_rounds = 5, maximize = FALSE,
                          nrounds = nrounds, nfold = nfold)
      
      eval_metrics <- cross_val$evaluation_log[, "test_mlogloss_mean"] %>%
                      as.data.frame()
      min_logloss <- min(eval_metrics)
      min_logloss_index <- which.min(unlist(eval_metrics))
      
      if (min_logloss < best_logloss) {
          best_eval <- eval_metrics
          best_logloss <- min_logloss
          best_nround <- min_logloss_index
          best_seednumber <- seed.number
          best_param <- param
      }
  }
  
  return(list(best_nround, best_param, best_eval))
}

# Results
pred_test <- data.frame(matrix(nrow=nrow(test), ncol=7*4))
pred_train <- data.frame(matrix(nrow=nrow(train), ncol=7*4))
```


## Sentiment Model
```{r}
# cross validation
cv_best <- cv_tune(data = sent_train)
pasteNQ("Best NRound:")
cv_best[[1]] %>% as.numeric()
pasteNQ("Best Params:")
cv_best[[2]]

# best_eval <- cv_best[[3]]
# ggplot(best_eval) + 
#     geom_line(aes(y = test_mlogloss_mean, 
#                   x=row.names(best_eval) %>% as.numeric())) +
#     ggtitle("Test Data Logloss Means") +
#     labs(y = "Logloss", x = "Iteration") +
#     Grph_theme() 

# best model
sent_model <- xgboost(data=sent_train, verbose=F,
                      nrounds=cv_best[[1]], param=cv_best[[2]])

# prediction
cols <- 1:7
pred_test[ , cols] <- predict(sent_model, sent_test) %>% 
                              matrix(ncol=num_class, byrow=TRUE)
results_test[ , "Sent_Label"] <- max.col(pred_test[ , cols])

# error and accuracy measure
results_test[ , "Sent_Label"] <- factor(results_test[ , "Sent_Label"],
                                  labels = unique(results_test$Target))
sent_acc <- confusionMatrix(results_test$Sent_Label, 
                            results_test$Target)
sent_acc

# Graph
ggplot(results_test, 
       aes(Target, Sent_Label, color=Target)) + 
  geom_jitter(size=1) + 
  labs(title="Confusion Matrix",
       subtitle="Predicted vs. Observed",
       y="Predicted",
       x="Observed") + 
  Grph_theme() +
  theme(legend.position='none',
        axis.text.x = element_text(angle = 45, hjust = 1))
```


## Bag of Words TF Model
```{r}
# cross validation
cv_best <- cv_tune(data = bow_train)
pasteNQ("Best NRound:")
cv_best[[1]] %>% as.numeric()
pasteNQ("Best Params:")
cv_best[[2]]

# best model
bow_model <- xgboost(data=bow_train, verbose=F,
                      nrounds=cv_best[[1]], param=cv_best[[2]])

# prediction
cols <- 8:14
pred_test[ , cols] <- predict(bow_model, bow_test) %>% 
                              matrix(ncol=num_class, byrow=TRUE)
results_test[ , "BoW_Label"] <- max.col(pred_test[ , cols])

# error and accuracy measure
results_test[ , "BoW_Label"] <- factor(results_test[ , "BoW_Label"],
                                  labels = unique(results_test$Target))
bow_acc <- confusionMatrix(results_test$BoW_Label, 
                           results_test$Target)
bow_acc

# Graph
ggplot(results_test, 
       aes(Target, BoW_Label, color=Target)) + 
  geom_jitter(size=1) + 
  labs(title="Confusion Matrix",
       subtitle="Predicted vs. Observed",
       y="Predicted",
       x="Observed") + 
  Grph_theme() +
  theme(legend.position='none',
        axis.text.x = element_text(angle = 45, hjust = 1)) 
```


## Bag of Words TF-IDF Model
```{r}
# cross validation
cv_best <- cv_tune(data = bowtfidf_train)
pasteNQ("Best NRound:")
cv_best[[1]] %>% as.numeric()
pasteNQ("Best Params:")
cv_best[[2]]

# best model
bowtfidf_model <- xgboost(data=bowtfidf_train, verbose=F,
                      nrounds=cv_best[[1]], param=cv_best[[2]])

# prediction
cols <- 15:21
pred_test[ , cols] <- predict(bowtfidf_model, bowtfidf_test) %>% 
                              matrix(ncol=num_class, byrow=TRUE)
results_test[ , "BoWtfidf_Label"] <- max.col(pred_test[ , cols])

# error and accuracy measure
results_test[ , "BoWtfidf_Label"] <- factor(results_test[ , "BoWtfidf_Label"],
                                  labels = unique(results_test$Target))
bowtfidf_acc <- confusionMatrix(results_test$BoWtfidf_Label, 
                           results_test$Target)
bowtfidf_acc

# Graph
ggplot(results_test, 
       aes(Target, BoWtfidf_Label, color=Target)) + 
  geom_jitter(size=1) + 
  labs(title="Confusion Matrix",
       subtitle="Predicted vs. Observed",
       y="Predicted",
       x="Observed") + 
  Grph_theme() +
  theme(legend.position='none',
        axis.text.x = element_text(angle = 45, hjust = 1)) 
```



## Word Embeddings Model
```{r}
# cross validation
cv_best <- cv_tune(data = we_train)
pasteNQ("Best NRound:")
cv_best[[1]] %>% as.numeric()
pasteNQ("Best Params:")
cv_best[[2]]

# best model
we_model <- xgboost(data=we_train, verbose=F,
                    nrounds=cv_best[[1]], param=cv_best[[2]])

# prediction
cols <- 22:28
pred_test[ , cols] <- predict(we_model, we_test) %>% 
                     matrix(ncol=num_class, byrow=TRUE)
results_test[ , "WE_Label"] <- max.col(pred_test[ , cols])

# error and accuracy measure
results_test[ , "WE_Label"] <- factor(results_test[ , "WE_Label"],
                                  labels = unique(results_test$Target))
we_acc <- confusionMatrix(results_test$WE_Label, 
                          results_test$Target)
we_acc

# Graph
ggplot(results_test, 
       aes(Target, WE_Label, color=Target)) + 
  geom_jitter(size=1) + 
  labs(title="Confusion Matrix",
       subtitle="Predicted vs. Observed",
       y="Predicted",
       x="Observed") + 
  Grph_theme() +
  theme(legend.position='none',
        axis.text.x = element_text(angle = 45, hjust = 1)) 
```

# Ensembling
```{r}
#Predicting the training probabilities
pred_train[ , 1:7]   <- predict(sent_model, sent_train) %>% 
                                matrix(ncol=num_class, byrow=TRUE) 

pred_train[ , 8:14]  <- predict(bow_model, bow_train) %>% 
                                matrix(ncol=num_class, byrow=TRUE) 

pred_train[ , 15:21] <- predict(bowtfidf_model, bowtfidf_train) %>% 
                                matrix(ncol=num_class, byrow=TRUE) 

pred_train[ , 22:28] <- predict(we_model, we_train) %>% 
                                matrix(ncol=num_class, byrow=TRUE) 

# Name columns
colnames(pred_test) <- c(paste0("Sent_Pred_", unique(results_test$Target)),
                         paste0("BoW_Pred_", unique(results_test$Target)),
                         paste0("BoWtfidf_Pred_", 
                                unique(results_test$Target)),
                         paste0("WE_Pred_", unique(results_test$Target)))
colnames(pred_train) <- c(paste0("Sent_Pred_", unique(results_test$Target)),
                          paste0("BoW_Pred_", unique(results_test$Target)),
                          paste0("BoWtfidf_Pred_", 
                                 unique(results_test$Target)),
                          paste0("WE_Pred_", unique(results_test$Target)))

# PCA
pca <- irlba::prcomp_irlba(pred_train, n = 7, scale. = T)
# library(rsvd)
# pca <- rpca(pred_train, k = 7, scale = T)
pca_train <- pca$x
pca_test <- predict(pca, pred_test)
``` 


## Top Layer
```{r}
# Top layer model
top_train <- xgb.DMatrix(as.matrix(pca_train),
                         label=num_train,
                         missing=NaN)
top_test  <- xgb.DMatrix(as.matrix(pca_test),
                         label=num_test,
                         missing=NaN)

# Results
top_layer_train <- data.frame(matrix(nrow=nrow(train), ncol=7))
top_layer_test <- data.frame(matrix(nrow=nrow(test), ncol=7))

# cross validation
cv_best <- cv_tune(data = top_train)
pasteNQ("Best NRound:")
cv_best[[1]] %>% as.numeric()
pasteNQ("Best Params:")
cv_best[[2]]

# best model
model_top_layer <- xgboost(data=top_train, verbose=F,
                           nrounds=cv_best[[1]], param=cv_best[[2]])

# prediction
top_layer_test[ , 1:7] <- predict(model_top_layer, top_test) %>% 
                                  matrix(ncol=num_class, byrow=TRUE)
top_layer_test[ , "Top_Label"] <- max.col(top_layer_test[ , 1:7])

# error and accuracy measure
results_test[ , "Top_Label"] <- factor(top_layer_test[ , "Top_Label"],
                                  labels = unique(results_test$Target))
top_acc <- confusionMatrix(results_test$Top_Label, 
                           results_test$Target)
top_acc

# Graph
ggplot(results_test, 
       aes(Target, Top_Label, color=Target)) + 
  geom_jitter(size=1) + 
  labs(title="Confusion Matrix",
       subtitle="Predicted vs. Observed",
       y="Predicted",
       x="Observed") + 
  Grph_theme() +
  theme(legend.position='none',
        axis.text.x = element_text(angle = 45, hjust = 1)) 

# save.image("Harry_Potter_and_the_Classification_End.RData")
```



## Top Layer Logistic Regression
```{r}
library(nnet)
glm_df <- pca_train %>% data.frame()
glm_df$target <- results_train$Target
# cross validation
model_top_glm <- train(
  target ~ .,
  data = glm_df,
  method = "multinom",
  trControl = trainControl(method = "cv", number = 4, workers = 4),
  trace = F
)

# prediction
top_glm_test <- data.frame(matrix(nrow=nrow(test), ncol=7))
top_glm_test[ , "Top_GLM_Label"] <- predict(model_top_glm, pca_test) 

# error and accuracy measure
results_test[ , "Top_GLM_Label"] <- factor(top_glm_test[ , "Top_GLM_Label"],
                                  labels = unique(results_test$Target))
top_glm_acc <- confusionMatrix(results_test$Top_GLM_Label, 
                           results_test$Target)
top_glm_acc

# Graph
ggplot(results_test, 
       aes(Target, Top_GLM_Label, color=Target)) + 
  geom_jitter(size=1) + 
  labs(title="Confusion Matrix",
       subtitle="Predicted vs. Observed",
       y="Predicted",
       x="Observed") + 
  Grph_theme() +
  theme(legend.position='none',
        axis.text.x = element_text(angle = 45, hjust = 1)) 
```


# Final Scores
```{r}
pasteNQ("Sentiment Model")
sent_acc$overall["Accuracy"]
cat("\n")

pasteNQ("Bag of Words (TF) Model")
bow_acc$overall["Accuracy"]
cat("\n")

pasteNQ("Bag of Words (TF-IDF) Model")
bowtfidf_acc$overall["Accuracy"]
cat("\n")

pasteNQ("Word Embedding Model")
we_acc$overall["Accuracy"]
cat("\n")

pasteNQ("Top Layer (GBM) Model")
top_acc$overall["Accuracy"] 
cat("\n")

pasteNQ("Top Layer (Logistic) Model")
top_glm_acc$overall["Accuracy"]


# Graph
bal_acc <- cbind(titles, "Sentiment", sent_acc$byClass[ , "Balanced Accuracy"]) %>%
     rbind(cbind(titles, "Bag of Words (TF)", bow_acc$byClass[ , "Balanced Accuracy"])) %>%
     rbind(cbind(titles, "Bag of Words (TF-IDF)", bowtfidf_acc$byClass[ , "Balanced Accuracy"])) %>%
     rbind(cbind(titles, "Word Embedding", we_acc$byClass[ , "Balanced Accuracy"])) %>% 
  as.data.frame()
colnames(bal_acc) <- c("Book", "Model", "Balanced Accuracy")

bal_acc %>% 
  ggplot(aes(x = `Balanced Accuracy` %>% as.numeric(), 
             y = Book,
             fill = Model)) +
  geom_bar(stat = "identity") +
  facet_wrap(~Model) +
  labs(title="Balanced Accuracy",
       y="Book",
       x="Balanced Accuracy (%)") + 
  Grph_theme_facet() 

top_glm_acc$byClass %>% 
  as.data.frame() %>%
  ggplot(aes(x = `Balanced Accuracy` %>% as.numeric()*100, 
             y = titles)) +
  geom_bar(stat = "identity") +
  xlim(0, 100) +
  labs(title="Balanced Accuracy",
       y="Book",
       x="Balanced Accuracy (%)") + 
  Grph_theme_facet() 
```


# Save
```{}
save(train, test,
     target_train, target_test, 
     pred_train, pred_test,
     pca_train, pca_test,
     sent_train, sent_test, 
     bow_train, bow_test,
     bowtfidf_train, bowtfidf_test,
     we_train, we_test,
     results_train, results_test,
     sent_model, sent_train,
     bow_model, bow_train,
     bowtfidf_model, bowtfidf_train,
     we_model, we_train,
     model_top_layer, top_layer_test,
     model_top_glm, top_glm_test,
     sent_acc, bow_acc, bowtfidf_acc, we_acc, top_acc, top_glm_acc,
     file = "Harry_Potter_and_the_Classification_Models.RData")
```


